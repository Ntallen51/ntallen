!pip install boto3 opencv-python pandas
import boto3

# Put your root access keys here
boto3.setup_default_session(
    aws_access_key_id='Access key',
    aws_secret_access_key='secret',
    region_name='us-east-1'
)

# Test connection
s3 = boto3.client('s3')
try:
    response = s3.list_buckets()
    print("Success! Here are your buckets:")
    for bucket in response['Buckets']:
        print(bucket['Name'])
except Exception as e:
    print(f"Error: {str(e)}")
import boto3
import logging
from IPython.display import display
import pandas as pd
import json
from datetime import datetime
import os

# Set up logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger("RekognitionAnalysis")

# Initialize AWS clients
rekognition_client = boto3.client('rekognition', region_name='us-east-1')
s3_client = boto3.client('s3', region_name='us-east-1')

# Configuration
BUCKET_NAME = "instagram-data-noah"
MEDIA_PREFIX = "media_files/"
OUTPUT_CSV = "rekognition_analysis.csv"  # Fixed filename for tracking processed files

def list_media_files(bucket: str, prefix: str = MEDIA_PREFIX) -> list:
    """List all media file keys in the specified S3 bucket under the given prefix."""
    keys = []
    paginator = s3_client.get_paginator('list_objects_v2')
    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):
        if 'Contents' in page:
            for obj in page['Contents']:
                key = obj['Key']
                if key.lower().endswith(('.jpg', '.jpeg', '.png', '.mp4')):
                    keys.append(key)
    return keys

def analyze_image_comprehensive(bucket: str, image_key: str) -> dict:
    """Analyze an image using all available Rekognition features."""
    try:
        logger.info(f"Analyzing image: {image_key}")
        image_obj = {'S3Object': {'Bucket': bucket, 'Name': image_key}}
        
        # Collect all analyses
        analysis = {
            "media_key": image_key,
            "media_type": "image",
            "timestamp": datetime.now().isoformat(),
        }
        
        # 1. Label Detection
        labels_response = rekognition_client.detect_labels(
            Image=image_obj,
            MaxLabels=50,
            MinConfidence=70
        )
        analysis["labels"] = labels_response.get('Labels', [])
        
        # 2. Face Detection
        faces_response = rekognition_client.detect_faces(
            Image=image_obj,
            Attributes=['ALL']
        )
        analysis["faces"] = faces_response.get('FaceDetails', [])
        
        # 3. Text Detection
        text_response = rekognition_client.detect_text(
            Image=image_obj
        )
        analysis["text"] = text_response.get('TextDetections', [])
        
        # 4. Moderation Labels
        moderation_response = rekognition_client.detect_moderation_labels(
            Image=image_obj,
            MinConfidence=70
        )
        analysis["moderation_labels"] = moderation_response.get('ModerationLabels', [])
        
        # 5. Celebrity Recognition
        try:
            celebrity_response = rekognition_client.recognize_celebrities(
                Image=image_obj
            )
            analysis["celebrities"] = celebrity_response.get('CelebrityFaces', [])
        except Exception as e:
            logger.warning(f"Celebrity recognition failed: {str(e)}")
            analysis["celebrities"] = []
            
        # Add summary counts
        analysis.update({
            "num_labels": len(analysis["labels"]),
            "num_faces": len(analysis["faces"]),
            "num_text": len(analysis["text"]),
            "num_moderation_labels": len(analysis["moderation_labels"]),
            "num_celebrities": len(analysis["celebrities"]),
        })
        
        return analysis
        
    except Exception as e:
        logger.error(f"Error analyzing image {image_key}: {str(e)}")
        return None

def load_existing_analyses():
    """Load existing analyses from CSV file."""
    if os.path.exists(OUTPUT_CSV):
        try:
            df = pd.read_csv(OUTPUT_CSV)
            # Convert string representations back to objects
            for col in ['labels', 'faces', 'text', 'moderation_labels', 'celebrities']:
                if col in df.columns:
                    df[col] = df[col].apply(lambda x: json.loads(x) if pd.notnull(x) else [])
            return df
        except Exception as e:
            logger.error(f"Error loading existing analyses: {str(e)}")
    return pd.DataFrame()

def save_analyses(df):
    """Save analyses to CSV file."""
    try:
        # Convert objects to string representations for storage
        df_save = df.copy()
        for col in ['labels', 'faces', 'text', 'moderation_labels', 'celebrities']:
            if col in df_save.columns:
                df_save[col] = df_save[col].apply(json.dumps)
        
        df_save.to_csv(OUTPUT_CSV, index=False)
        logger.info(f"Analysis saved to {OUTPUT_CSV}")
    except Exception as e:
        logger.error(f"Error saving analyses: {str(e)}")

def main():
    # Load existing analyses
    existing_df = load_existing_analyses()
    processed_files = set(existing_df['media_key'].tolist()) if not existing_df.empty else set()
    
    # Get current media files
    media_files = list_media_files(BUCKET_NAME)
    new_files = [f for f in media_files if f not in processed_files]
    
    print(f"Found {len(media_files)} total media files")
    print(f"Previously processed: {len(processed_files)} files")
    print(f"New files to process: {len(new_files)}")
    
    # Process new files
    new_analyses = []
    for file in new_files:
        if file.lower().endswith(('.jpg', '.jpeg', '.png')):
            result = analyze_image_comprehensive(BUCKET_NAME, file)
            if result:
                new_analyses.append(result)
                print(f"\nCompleted analysis for {file}")
                print(f"Found: {result['num_labels']} labels, {result['num_faces']} faces, "
                      f"{result['num_text']} text instances, {result['num_moderation_labels']} moderation labels, "
                      f"{result['num_celebrities']} celebrities")
    
    # Combine new analyses with existing ones
    if new_analyses:
        new_df = pd.DataFrame(new_analyses)
        if existing_df.empty:
            final_df = new_df
        else:
            final_df = pd.concat([existing_df, new_df], ignore_index=True)
        
        # Save updated analyses
        save_analyses(final_df)
        print(f"\nProcessed {len(new_analyses)} new images")
        print("Latest analysis results:")
        display(new_df.head())
    else:
        print("\nNo new images to process")

if __name__ == "__main__":
    main()

import boto3
import logging
from IPython.display import display
import pandas as pd
import json
from datetime import datetime
import io
import os
import time
import numpy as np
from collections import defaultdict

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger("DetailedRekognitionAnalysis")

BUCKET_NAME = "instagram-data-noah"
MEDIA_PREFIX = "media_files/"
REKOGNITION_DETAILED_CSV = "rekognition_detailed_analysis.csv"
FINAL_COMBINED_CSV = "rekognition_metrics_combined_detailed.csv"

rekognition_client = boto3.client('rekognition')
s3_client = boto3.client('s3')


def wait_for_job_completion(job_id, get_results_func, max_attempts=60):
    attempt = 0
    while attempt < max_attempts:
        response = get_results_func(JobId=job_id)
        status = response['JobStatus']
        if status in ['SUCCEEDED', 'FAILED']:
            return response
        attempt += 1
        time.sleep(5)
    raise TimeoutError(f"Job {job_id} did not complete within the allowed time")


def extract_all_face_details(face, prefix=''):
    details = {}
    for key, value in face.get('BoundingBox', {}).items():
        details[f"{prefix}bounding_box_{key.lower()}"] = value

    age_range = face.get('AgeRange', {})
    details[f"{prefix}age_low"] = age_range.get('Low')
    details[f"{prefix}age_high"] = age_range.get('High')

    gender = face.get('Gender', {})
    details[f"{prefix}gender_value"] = gender.get('Value')
    details[f"{prefix}gender_confidence"] = gender.get('Confidence')

    for emotion in face.get('Emotions', []):
        # Fixed typo: changed emotion.on.get to emotion.get
        emotion_type = emotion.get('Type', '').lower()
        details[f"{prefix}emotion_{emotion_type}_confidence"] = emotion.get('Confidence')

    for landmark in face.get('Landmarks', []):
        landmark_type = landmark.get('Type', '').lower()
        details[f"{prefix}landmark_{landmark_type}_x"] = landmark.get('X')
        # Fixed typo: changed landmark_typeype to landmark_type
        details[f"{prefix}landmark_{landmark_type}_y"] = landmark.get('Y')

    for key, value in face.get('Quality', {}).items():
        details[f"{prefix}quality_{key.lower()}"] = value

    for attr in ['Smile', 'Eyeglasses', 'Sunglasses', 'Beard', 'Mustache', 'EyesOpen',
                 'MouthOpen', 'Pose', 'Blur', 'Sharpness', 'Confidence']:
        if attr in face:
            if isinstance(face[attr], dict):
                for key, value in face[attr].items():
                    details[f"{prefix}{attr.lower()}_{key.lower()}"] = value
            else:
                details[f"{prefix}{attr.lower()}"] = face[attr]
    return details


def extract_all_label_details(label, prefix=''):
    details = {}
    details[f"{prefix}name"] = label.get('Name')
    details[f"{prefix}confidence"] = label.get('Confidence')
    for i, parent in enumerate(label.get('Parents', [])):
        details[f"{prefix}parent_{i}_name"] = parent.get('Name')
    for i, category in enumerate(label.get('Categories', [])):
        details[f"{prefix}category_{i}_name"] = category.get('Name')
    for i, instance in enumerate(label.get('Instances', [])):
        instance_prefix = f"{prefix}instance_{i}_"
        # Fixed typo: changed "instantance" to "instance"
        for key, value in instance.get('BoundingBox', {}).items():
            details[f"{instance_prefix}bounding_box_{key.lower()}"] = value
        details[f"{instance_prefix}confidence"] = instance.get('Confidence')
    return details


def extract_all_text_details(text, prefix=''):
    details = {}
    details[f"{prefix}detected_text"] = text.get('DetectedText')
    details[f"{prefix}type"] = text.get('Type')
    details[f"{prefix}confidence"] = text.get('Confidence')
    geometry = text.get('Geometry', {})
    for key, value in geometry.get('BoundingBox', {}).items():
        details[f"{prefix}bounding_box_{key.lower()}"] = value
    for i, point in enumerate(geometry.get('Polygon', [])):
        details[f"{prefix}polygon_point_{i}_x"] = point.get('X')
        details[f"{prefix}polygon_point_{i}_y"] = point.get('Y')
    return details


def analyze_video(bucket: str, key: str) -> dict:
    try:
        video_obj = {'S3Object': {'Bucket': bucket, 'Name': key}}
        all_details = defaultdict(dict)

        # Label Detection
        start_label_response = rekognition_client.start_label_detection(
            Video=video_obj,
            MinConfidence=1
        )
        label_response = wait_for_job_completion(
            start_label_response['JobId'],
            rekognition_client.get_label_detection
        )
        if label_response['JobStatus'] == 'SUCCEEDED':
            for i, label in enumerate(label_response.get('Labels', [])):
                label_details = extract_all_label_details(label['Label'], f"label_{i}_")
                all_details.update(label_details)
                all_details[f"label_{i}_timestamp"] = label.get('Timestamp')

        # Face Detection
        start_face_response = rekognition_client.start_face_detection(
            Video=video_obj,
            FaceAttributes='ALL'
        )
        face_response = wait_for_job_completion(
            start_face_response['JobId'],
            rekognition_client.get_face_detection
        )
        if face_response['JobStatus'] == 'SUCCEEDED':
            for i, face in enumerate(face_response.get('Faces', [])):
                face_details = extract_all_face_details(face['Face'], f"face_{i}_")
                all_details.update(face_details)
                all_details[f"face_{i}_timestamp"] = face.get('Timestamp')

        # Text Detection
        start_text_response = rekognition_client.start_text_detection(
            Video=video_obj
        )
        text_response = wait_for_job_completion(
            start_text_response['JobId'],
            rekognition_client.get_text_detection
        )
        if text_response['JobStatus'] == 'SUCCEEDED':
            for i, text in enumerate(text_response.get('TextDetections', [])):
                text_details = extract_all_text_details(text['TextDetection'], f"text_{i}_")
                all_details.update(text_details)
                all_details[f"text_{i}_timestamp"] = text.get('Timestamp')

        # Content Moderation
        start_moderation_response = rekognition_client.start_content_moderation(
            Video=video_obj,
            MinConfidence=1
        )
        moderation_response = wait_for_job_completion(
            start_moderation_response['JobId'],
            rekognition_client.get_content_moderation
        )
        if moderation_response['JobStatus'] == 'SUCCEEDED':
            for i, moderation in enumerate(moderation_response.get('ModerationLabels', [])):
                prefix = f"moderation_{i}_"
                all_details[f"{prefix}name"] = moderation['ModerationLabel'].get('Name')
                all_details[f"{prefix}confidence"] = moderation['ModerationLabel'].get('Confidence')
                all_details[f"{prefix}parent_name"] = moderation['ModerationLabel'].get('ParentName')
                all_details[f"{prefix}timestamp"] = moderation.get('Timestamp')

        return dict(all_details)
    except Exception as e:
        logger.error(f"Error analyzing video {key}: {str(e)}")
        return None


def analyze_media_detailed(bucket: str, key: str) -> dict:
    try:
        all_details = defaultdict(dict)
        if key.lower().endswith('.mp4'):
            video_details = analyze_video(bucket, key)
            if video_details:
                all_details.update(video_details)
        else:
            image_obj = {'S3Object': {'Bucket': bucket, 'Name': key}}

            # Label Detection
            response = rekognition_client.detect_labels(
                Image=image_obj,
                MaxLabels=1000,
                MinConfidence=1
            )
            for i, label in enumerate(response.get('Labels', [])):
                label_details = extract_all_label_details(label, f"label_{i}_")
                all_details.update(label_details)

            # Face Detection
            response = rekognition_client.detect_faces(
                Image=image_obj,
                Attributes=['ALL']
            )
            for i, face in enumerate(response.get('FaceDetails', [])):
                face_details = extract_all_face_details(face, f"face_{i}_")
                all_details.update(face_details)

            # Text Detection
            response = rekognition_client.detect_text(Image=image_obj)
            for i, text in enumerate(response.get('TextDetections', [])):
                text_details = extract_all_text_details(text, f"text_{i}_")
                all_details.update(text_details)

            # Moderation Labels
            response = rekognition_client.detect_moderation_labels(
                Image=image_obj,
                MinConfidence=1
            )
            for i, label in enumerate(response.get('ModerationLabels', [])):
                prefix = f"moderation_{i}_"
                all_details[f"{prefix}name"] = label.get('Name')
                all_details[f"{prefix}confidence"] = label.get('Confidence')
                all_details[f"{prefix}parent_name"] = label.get('ParentName')

            # Celebrity Recognition
            try:
                response = rekognition_client.recognize_celebrities(Image=image_obj)
                for i, celebrity in enumerate(response.get('CelebrityFaces', [])):
                    prefix = f"celebrity_{i}_"
                    all_details[f"{prefix}name"] = celebrity.get('Name')
                    all_details[f"{prefix}confidence"] = celebrity.get('MatchConfidence')
                    all_details[f"{prefix}id"] = celebrity.get('Id')
                    all_details[f"{prefix}urls"] = json.dumps(celebrity.get('Urls', []))
                    if 'Face' in celebrity:
                        celeb_face_details = extract_all_face_details(celebrity['Face'], f"{prefix}face_")
                        all_details.update(celeb_face_details)
            except Exception as e:
                logger.warning(f"Celebrity recognition failed: {str(e)}")

        all_details['media_key'] = key
        all_details['analysis_timestamp'] = datetime.now().isoformat()
        return dict(all_details)

    except Exception as e:
        logger.error(f"Error analyzing {key}: {str(e)}")
        return None


def process_all_media():
    all_media = []
    paginator = s3_client.get_paginator('list_objects_v2')
    for page in paginator.paginate(Bucket=BUCKET_NAME, Prefix=MEDIA_PREFIX):
        if 'Contents' in page:
            for obj in page['Contents']:
                if obj['Key'].lower().endswith(('.jpg', '.jpeg', '.png', '.mp4')):
                    all_media.append(obj['Key'])
    logger.info(f"Found {len(all_media)} media files to process")

    existing_df = pd.DataFrame()
    if os.path.exists(REKOGNITION_DETAILED_CSV):
        existing_df = pd.read_csv(REKOGNITION_DETAILED_CSV)
        processed_keys = set(existing_df['media_key'].tolist())
    else:
        processed_keys = set()

    new_media = [key for key in all_media if key not in processed_keys]
    logger.info(f"Processing {len(new_media)} new media files")

    results = []
    for key in new_media:
        logger.info(f"Processing {key}")
        result = analyze_media_detailed(BUCKET_NAME, key)
        if result:
            results.append(result)

    if results:
        new_df = pd.DataFrame(results)
        if not existing_df.empty:
            all_columns = set(existing_df.columns) | set(new_df.columns)
            existing_df_new = pd.DataFrame(columns=all_columns)
            new_df_new = pd.DataFrame(columns=all_columns)

            for col in existing_df.columns:
                existing_df_new[col] = existing_df[col]
            for col in new_df.columns:
                new_df_new[col] = new_df[col]

            final_df = pd.concat([existing_df_new, new_df_new], ignore_index=True)
        else:
            final_df = new_df

        final_df.to_csv(REKOGNITION_DETAILED_CSV, index=False)
        logger.info(f"Saved detailed analysis to {REKOGNITION_DETAILED_CSV}")
        return final_df
    else:
        logger.info("No new results to save")
        return existing_df


def combine_with_metrics():
    try:
        rekognition_df = pd.read_csv(REKOGNITION_DETAILED_CSV)

        response = s3_client.get_object(
            Bucket=BUCKET_NAME,
            Key="insights_on_media.csv"
        )
        metrics_df = pd.read_csv(io.BytesIO(response['Body'].read()))

        metrics_df['Media ID'] = metrics_df['Media ID'].astype(str)

        metrics_wide = metrics_df.pivot(
            index='Media ID',
            columns='Metric Name',
            values='Value'
        ).reset_index()

        rekognition_df['Media ID'] = rekognition_df['media_key'].apply(
            lambda x: str(x.split('/')[-1].split('.')[0])
        )

        metrics_wide['Media ID'] = metrics_wide['Media ID'].astype(str)
        rekognition_df['Media ID'] = rekognition_df['Media ID'].astype(str)

        combined_df = pd.merge(
            rekognition_df,
            metrics_wide,
            on='Media ID',
            how='inner'
        )

        combined_df.to_csv(FINAL_COMBINED_CSV, index=False)
        logger.info(f"Saved combined analysis to {FINAL_COMBINED_CSV}")

        return combined_df

    except Exception as e:
        logger.error(f"Error combining data: {str(e)}")
        return None


def main():
    logger.info("Starting detailed Rekognition analysis...")
    rekognition_df = process_all_media()
    if rekognition_df is not None:
        logger.info("Combining with Instagram metrics...")
        combined_df = combine_with_metrics()
        if combined_df is not None:
            print("\nAnalysis complete!")
            print(f"Total records: {len(combined_df)}")
            print("\nColumns in final dataset:")
            for col in combined_df.columns:
                print(f"- {col}")
            print("\nSample of combined data:")
            display(combined_df.head())


if __name__ == "__main__":
    main()
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns
import boto3
import io

# Replace with your actual S3 bucket name
BUCKET_NAME = "your-bucket-name"  

class ContentAnalyzer:
    def __init__(self):
        # Use the comprehensive feature set from your first code block
        self.features = self.create_feature_columns()
        self.models = {}
        self.scalers = {}
        self.feature_importance = {}

    def create_feature_columns(self):
        return {
            'human_presentation': {
                'face_expression': ['smiling', 'laughing', 'serious', 'surprised', 'eye_contact', 'head_angle', 'teeth_visibility', 'eye_squint', 'expression_change_rate'],
                'body_language': ['hand_gestures', 'body_position', 'walking_pattern', 'sitting_position', 'standing_position', 'interaction_position', 'group_fo_formation', 'personal_space', 'movement_flow', 'energy_level'],
                'appearance': ['outfit_colors', 'pattern_types', 'accessory_placement', 'hair_styling', 'makeup_intensity', 'outfit_formality', 'brand_visibility', 'style_consistency', 'outfit_background_contrast'],
                'personal_branding': ['signature_poses', 'facial_expression_consistency', 'personal_style_evolution', 'recurring_items', 'personal_logo_placement', 'filter_consistency'],
                'authenticity': ['behind_scenes', 'candid_moments', 'natural_reactions', 'bloopers', 'real_life_situations', 'emotional_vulnerability', 'personal_space_reveals', 'daily_routine']
            },
            'location_setting': {
                'environment': ['kitchen', 'gym', 'office', 'bedroom', 'beach', 'city', 'park', 'restaurant', 'retail', 'professional_setting', 'natural_setting', 'urban_setting', 'event_venue', 'public_space'],
                'background': ['busyness_level', 'cleanliness', 'natural_elements', 'urban_elements', 'window_presence', 'mirror_presence', 'art_presence', 'plant_presence', 'furniture_visibility'],
                'lighting': ['natural_light', 'artificial_light', 'dim', 'bright', 'backlit', 'sidelit', 'soft_light', 'hard_light', 'color_temperature'],
                'time_of_day': ['morning', 'afternoon', 'evening', 'night', 'golden_hour', 'blue_hour']
            },
            'content_action': {
                'demonstration': ['step_by_step', 'before_after', 'product_application', 'diy_project', 'cooking_prep', 'fitness_move', 'beauty_technique', 'tech_howto', 'home_improvement'],
                'lifestyle': ['daily_routine', 'meal_prep', 'workout', 'shopping', 'travel', 'work_related', 'hobby', 'pet_interaction', 'childcare', 'social_gathering'],
                'entertainment': ['dance_sequence', 'singing', 'lip_syncing', 'comedy_skit', 'storytelling', 'instrument_playing', 'sports_performance', 'acting', 'magic_trick', 'gaming'],
                'educational': ['whiteboard_explanation', 'book_reference', 'scientific_experiment', 'language_teaching', 'math_problem_solving', 'historical_reenactment', 'nature_explanation', 'tech_walkthrough', 'financial_advice', 'health_info'],
                'product_interaction': ['unboxing', 'try_on_haul', 'product_comparison', 'brand_logo_exposure', 'service_walkthrough', 'sponsored_content', 'product_placement', 'brand_integration', 'customer_testimonial', 'affiliate_link']
            },
            'content_structure': {
                'pacing': ['hook_timing', 'section_divisions', 'climax_timing', 'cta_placement', 'b_roll_frequency', 'transition_types', 'timelapse_usage', 'slowmo_usage', 'spliplit_screen', 'text_overlay_timing'],
                'narrative': ['story_arc', 'personal_anecdote', 'cliffhanger', 'plot_twist', 'flashback', 'character_development', 'emotional_journey', 'suspense_building', 'humor_insertion', 'inspirational_message'],
                'engagement_tactics': ['audience_address', 'question_posing', 'chalhallenge_presentation', 'poll_integration', 'interaction_prompt', 'emotional_trigger', 'curiosity_gap', 'controversy_intro', 'collaboration_invitation', 'behind_scenes_reveal']
            },
            'food_content': {
                'cooking_prep': ['chopping_method', 'cooking_method', 'plating_technique', 'ingredient_mixing', 'sauce_application', 'garnish_placement', 'kitchen_tool_usage', 'temperature_indication', 'measurement_demo', 'texture_closeup'],
                'recipe_presentation': ['ingredient_layout', 'recipe_card_placement', 'measurement_display', 'step_sequencing', 'timelapse_vs_realtime', 'final_result_reveal', 'portion_size_demo', 'serving_suggestion', 'leftover_storage', 'meal_prep_org'],
                'food_aesthetics': ['color_combination', 'plating_composition', 'steam_visibility', 'cheese_pull', 'sauce_drip', 'cutting_reveal', 'food_motion', 'texture_closeup', 'cross_section', 'melting_sequence'],
                'dining_content': ['menu_browsing', 'order_arrival_reaction', 'first_bite_reaction', 'sharing_plate_arrangement', 'restaurant_ambiance', 'chef_interaction', 'table_setting', 'food_photography_setup', 'drink_presentation', 'bill_splitting']
            },
            'lifestyle_content': {
                'morning_routine': ['wake_up_sequence', 'bed_making', 'skincare_application', 'breakfast_prep', 'coffee_ritual', 'outfit_selection', 'bag_packing', 'time_management', 'productivity_hack'],
                'home_organization': ['closet_arrangement', 'drawer_organization', 'pantry_stocking', 'cleaning_sequence', 'decluttering_process', 'storage_solution', 'label_making', 'folding_technique', 'space_saving_hack'],
                'shopping_haul': ['store_navigation', 'product_selection', 'price_comparison', 'try_on_sequence', 'haul_arrangement', 'receipt_organization', 'budget_tracking', 'sale_shopping', 'online_vs_instore', 'styling_combination'],
                'home_decor': ['room_arrangement', 'color_scheme_selection', 'furniture_placement', 'art_hanging', 'plant_styling', 'lighting_setup', 'seasonal_decorating', 'diy_project', 'small_space_solution', 'mood_board_creation'],
                'beauty_selfcare': ['skincare_layering', 'makeup_application', 'hair_styling', 'face_mask_application', 'self_care_routine', 'product_organization', 'tool_sanitization', 'nail_care', 'spa_day_setup'],
                'workspace_productivity': ['desk_arrangement', 'planning_method', 'note_taking', 'calendar_organization', 'time_blocking', 'work_break_activity', 'desk_accessory', 'lighting_optimization', 'ergonomic_adjustment', 'focus_technique']
            },
            'restaurant_showcase': {
                'first_impression': ['exterior_shot', 'entry_area', 'lighting_condition', 'table_setup', 'kitchen_visibility', 'ambiance_audio', 'decor_focus', 'seating_arrangement', 'bar_area', 'special_feature'],
                'menu_interaction': ['menu_browsing', 'item_highlighting', 'price_visibility', 'special_callout', 'daily_special', 'chef_recommendation', 'signature_dish', 'dietary_option', 'qr_vs_physical', 'drink_menu'],
                'food_presentation': ['food_arrival_timing', 'plate_angle', 'steam_visibility', 'size_comparison', 'multi_dish_layout', 'garnish_placement', 'sauce_presentation', 'side_dish_arrangement', 'table_space_organization', 'sharing_plate_setup'],
                'food_action': ['cutting_into_dish', 'sauce_pouring', 'cheese_pull', 'noodle_pull', 'egg_yolk_breaking', 'tableside_mixing', 'dipping_action', 'spreading_technique', 'breaking_cracking', 'beverage_stirring'],
                'interactive_moment': ['first_bite_reaction', 'tasting_expression', 'sharing_reaction', 'group_reaction', 'chef_interaction', 'server_presentation', 'tableside_prep', 'food_passing', 'communal_dining', 'split_plate_demo']
            },
            'cuisine_specific': {
                'italian': ['pizza_cheese_pull', 'pasta_twirling', 'sauce_coating', 'fresh_pasta_making', 'parmesan_grating', 'olive_oil_drizzle', 'bruschetta_assembly', 'tiramisu_layering', 'espresso_shot', 'gelato_scooping'],
                'asian': ['ramen_noodle_pull', 'sushi_roll_cutting', 'dumpling_folding', 'wok_tossing', 'bubble_tea_shaking', 'chopstick_usage', 'hot_pot_dipping', 'korean_bbq_grilling', 'bao_bun_steaming', 'matcha_whisking'],
                'mexican': ['taco_assembly', 'guacamole_mashing', 'salsa_mixing', 'tortilla_pressing', 'burrito_wrapping', 'quesadilla_flipping', 'churro_frying', 'margarita_salting', 'fajita_sizzle', 'elote_preparation'],
                'american': ['burger_stacking', 'bbq_smoking', 'pancake_flipping', 'sandwich_cutting', 'milkshake_blending', 'fried_chicken_crisping', 'apple_pie_slicing', 'chili_ladling', 'buffalo_wing_tossing', 'hot_dog_topping']
            }
        }

    def extract_features(self, rekognition_data):
        """Use existing columns from Rekognition data"""
        features = rekognition_data.dropna(axis=1, how='all')
        return features

    def detect_feature(self, row, category, subcategory, feature):
        # This function would contain complex logic to detect each feature
        # from the Rekognition data. For example:
        if category == 'human_presentation' and subcategory == 'face_expression':
            if feature == 'smiling':
                return any('smile' in label.lower() for label in row['labels'])
        # ... many more detection rules for each feature
        return 0  # Default to 0 if feature not detected

    def train_models(self, X, y_dict):
        for metric, y in y_dict.items():
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
            
            self.scalers[metric] = StandardScaler()
            X_train_scaled = self.scalers[metric].fit_transform(X_train)
            X_test_scaled = self.scalers[metric].transform(X_test)
            
            self.models[metric] = RandomForestRegressor(n_estimators=100, random_state=42)
            self.models[metric].fit(X_train_scaled, y_train)
            
            y_pred = self.models[metric].predict(X_test_scaled)
            mse = mean_squared_error(y_test, y_pred)
            r2 = r2_score(y_test, y_pred)
            
            print(f"{metric} - MSE: {mse}, R2: {r2}")
            
            self.feature_importance[metric] = dict(zip(X.columns, self.models[metric].feature_importances_))

    def generate_insights(self):
        insights = []
        for metric, importance in self.feature_importance.items():
            top_features = sorted(importance.items(), key=lambda x: x[1], reverse=True)[:10]
            for feature, impact in top_features:
                insight = self.create_human_readable_insight(feature, impact, metric)
                insights.append(insight)
        return insights

    def create_human_readable_insight(self, feature, impact, metric):
        category, subcategory, specific_feature = feature.split('_', 2)
        return f"Including {specific_feature.replace('_', ' ')} in your {subcategory} increases {metric} by approximately {impact*100:.1f}%"

    def predict_performance(self, new_content):
        predictions = {}
        for metric, model in self.models.items():
            X_scaled = self.scalers[metric].transform(new_content)
            predictions[metric] = model.predict(X_scaled)
        return predictions

    def visualize_feature_importance(self, metric):
        importance = self.feature_importance[metric]
        features = list(importance.keys())
        scores = list(importance.values())
        
        plt.figure(figsize=(10,8))
        plt.bar(range(len(scores)), scores)
        plt.xlabel('Features')
        plt.ylabel('Importance')
        plt.title(f'Feature Importance for {metric}')
        plt.xticks(range(len(scores)), features, rotation='vertical')
        plt.tight_layout()
        plt.show()

    def prepare_target_variables(self, metrics_data, rekognition_data):
        """Prepare engagement metrics as target variables"""
        # First, transform metrics from long to wide format
        metrics_wide = metrics_data.pivot(
            index='Media ID',
            columns='Metric Name',
            values='Value'
        ).reset_index()

        print("Metrics shape before merge:", metrics_wide.shape)

        # Extract media IDs from rekognition data
        rekognition_data['Media ID'] = rekognition_data['media_key'].apply(
            lambda x: str(x.split('/')[-1].split('.')[0])
        )

        # Merge metrics with rekognition data to ensure alignment
        merged_metrics = pd.merge(
            rekognition_data[['Media ID']],
            metrics_wide,
            on='Media ID',
            how='inner'
        )

        print("Merged metrics shape:", merged_metrics.shape)

        # Create dictionary of target variables
        target_variables = {}

        metric_mapping = {
            'likes': ['likes'],
            'comments': ['comments'],
            'shares': ['shares'],
            'saves': ['saved'],
            'reach': ['reach'],
            'total_interactions': ['total_interactions'],
            'plays': ['plays'],
            'watch_time': ['ig_reels_video_view_total_time'],
            'avg_watch_time': ['ig_reels_avg_watch_time'],
            'replays': ['clips_replays_count']
        }

        for target, possible_names in metric_mapping.items():
            found_column = next((col for col in possible_names if col in merged_metrics.columns), None)
            if found_column:
                target_variables[target] = merged_metrics[found_column]
                print(f"Found data for {target}: {len(target_variables[target])} samples")
            else:
                print(f"Warning: Could not find a column for {target}")

        return target_variables

from sklearn.preprocessing import LabelEncoder
from sklearn.impute import SimpleImputer

def main():
    try:
        # Load the already combined data
        combined_data = pd.read_csv('rekognition_metrics_combined_detailed.csv', low_memory=False)
        print("Loaded combined data with shape:", combined_data.shape)

        analyzer = ContentAnalyzer()
        
        # Separate features (Rekognition data) from targets (metrics)
        metric_columns = ['likes', 'comments', 'shares', 'saved', 'reach', 
                         'total_interactions', 'plays', 'ig_reels_video_view_total_time', 
                         'ig_reels_avg_watch_time', 'clips_replays_count']
        
        # Get features (all columns except metrics and ID columns)
        feature_columns = [col for col in combined_data.columns 
                         if col not in metric_columns and col not in ['Media ID', 'media_key']]
        
        X = combined_data[feature_columns]
        
        # Ha Handle different data types
        numeric_features = X.select_dtypes(include=['int64', 'float64']).columns
        boolean_features = X.select_dtypes(include=['bool']).columns
        categorical_features = X.select_dtypes(include=['object']).columns

        # Impute missing values
        numeric_imputer = SimpleImputer(strategy='mean')
        X[numeric_features] = numeric_imputer.fit_transform(X[numeric_features])

        # Encode categorical variables
        le = LabelEncoder()
        for column in categorical_features:
            X[column] = X[column].astype(str)  # Convert to string to handle any non-string objects
            X[column] = le.fit_transform(X[column])

        print("Prepared features with shape:", X.shape)
        print("Numeric features:", len(numeric_features))
        print("Boolean features:", len(boolean_features))
        print("Categorical features (encoded):", len(categorical_features))
        
        # Create target variables dictionary
        y_dict = {}
        for metric in metric_columns:
            if metric in combined_data.columns:
                y_dict[metric] = combined_data[metric]
                print(f"Found target variable: {metric}")
        
        print("Prepared target variables:", list(y_dict.keys()))
        
        if not y_dict:
            print("No target variables found. Cannot proceed with model training.")
            return
        
        if X.empty:
            print("No features found. Cannot proceed with model training.")
            return
        
        analyzer.train_models(X, y_dict)
        insights = analyzer.generate_insights()

    except FileNotFoundError:
        print("Could not find rekognition_metrics_combined_detailed.csv file")
        return
    except Exception as e:
        print(f"Error loading data: {str(e)}")
        print(f"Error occurred at line: {e.__traceback__.tb_lineno}")
        return

if __name__ == "__main__":
    main() #THIS IS GOING TO NEED TO BE CHANGED/DELETED OR SOMETHING. NOAH TO TAKE A LOOK AT AFTER DONE RUNNING UPDATED CODE.
    #DO NOT FORGET ABOUT THIS CODE
    #
    #
    #
    #
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import pytz

class TimeAnalyzer:
    def __init__(self):
        self.time_features = self.create_time_features()

    def create_time_features(self):
        return {
            'basic_time': ['hour', 'day_of_week', 'is_weekend', 'is_holiday', 'season'],
            'meal_times': ['is_breakfast', 'is_lunch', 'is_dinner', 'is_late_night'],
            'activity_patterns': ['work_hours', 'commute_time', 'leisure_time'],
            'platform_usage': ['peak_usage_time', 'low_usage_time'],
            'post_timing': ['time_since_last_post', 'posting_frequency', 'consistent_timing'],
            'event_proximity': ['near_major_event', 'holiday_season'],
            'timezone_considerations': ['local_time', 'global_time_alignment']
        }

    def extract_time_features(self, post_data):
        features = pd.DataFrame()
        for category, feature_list in self.time_features.items():
            for feature in feature_list:
                features[f"time_{category}_{feature}"] = post_data.apply(
                    lambda row: self.calculate_time_feature(row, category, feature), axis=1
                )
        return features

    def calculate_time_feature(self, row, category, feature):
        post_time = pd.to_datetime(row['post_timestamp'])
        if category == 'basic_time':
            if feature == 'hour':
                return post_time.hour
            elif feature == 'day_of_week':
                return post_time.dayofweek
            elif feature == 'is_weekend':
                return int(post_time.dayofweek >= 5)
            elif feature == 'is_holiday':
                return self.is_holiday(post_time)
            elif feature == 'season':
                return self.get_season(post_time)
        elif category == 'meal_times':
            return self.is_meal_time(post_time, feature)
        elif category == 'activity_patterns':
            return self.get_activity_pattern(post_time, feature)
        elif category == 'platform_usage':
            return self.get_platform_usage(post_time, feature)
        elif category == 'post_timing':
            return self.analyze_post_timing(row, feature)
        elif category == 'event_proximity':
            return self.check_event_proximity(post_time, feature)
        elif category == 'timezone_considerations':
            return self.consider_timezone(post_time, feature)
        return 0

    def is_holiday(self, date):
        # Implement holiday checking logic
        return 0

    def get_season(self, date):
        # Implement season determination logic
        return 0

    def is_meal_time(self, time, meal):
        # Implement meal time checking logic
        return 0

    def get_activity_pattern(self, time, activity):
        # Implement activity pattern determination
        return 0

    def get_platform_usage(self, time, usage_type):
        # Implement platform usage pattern logic
        return 0

    def analyze_post_timing(self, row, timing_type):
        # Implement post timing analysis
        return 0

    def check_event_proximity(self, time, event_type):
        # Implement event proximity checking
        return 0

    def consider_timezone(self, time, consideration):
        # Implement timezone consideration logic
        return 0

class ContentAnalyzer:
    def __init__(self):
        # Ensure TimeAnalyzer is created before using it in create_feature_columns
        self.time_analyzer = TimeAnalyzer()
        self.features = self.create_feature_columns()
        self.models = {}
        self.scalers = {}
        self.feature_importance = {}

    def create_feature_columns(self):
        # ... (previous feature columns remain the same)
        # For demonstration, a dummy feature dictionary is used.
        features = {
            'visual_features': {
                'color': ['red', 'green', 'blue']
            }
        }
        # Add time-related features from TimeAnalyzer
        features['time_analysis'] = self.time_analyzer.time_features
        return features

    def extract_features(self, rekognition_data, post_data):
        features = pd.DataFrame()
        for category, subcategories in self.features.items():
            if category != 'time_analysis':
                for subcategory, feature_list in subcategories.items():
                    for feature in feature_list:
                        features[f"{category}_{subcategory}_{feature}"] = rekognition_data.apply(
                            lambda row: self.detect_feature(row, category, subcategory, feature), axis=1
                        )
        # Extract time features
        time_features = self.time_analyzer.extract_time_features(post_data)
        features = pd.concat([features, time_features], axis=1)
        return features

    def detect_feature(self, row, category, subcategory, feature):
        # ... (previous feature detection logic remains the same)
        return 0

    def prepare_target_variables(self, metrics_data):
        return {
            'likes': metrics_data['likes'],
            'comments': metrics_data['comments'],
            'shares': metrics_data['shares'],
            'saves': metrics_data['saves'],
            'reach': metrics_data['reach'],
            'watch_time': metrics_data['watch_time'],
            'completion_rate': metrics_data['completion_rate']
        }

    def train_models(self, X, y_dict):
        for metric, y in y_dict.items():
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
            self.scalers[metric] = StandardScaler()
            X_train_scaled = self.scalers[metric].fit_transform(X_train)
            X_test_scaled = self.scalers[metric].transform(X_test)
            self.models[metric] = RandomForestRegressor(n_estimators=100, random_state=42)
            self.models[metric].fit(X_train_scaled, y_train)
            y_pred = self.models[metric].predict(X_test_scaled)
            mse = mean_squared_error(y_test, y_pred)
            r2 = r2_score(y_test, y_pred)
            print(f"{metric} - MSE: {mse}, R2: {r2}")
            self.feature_importance[metric] = dict(zip(X.columns, self.models[metric].feature_importances_))

    def generate_insights(self):
        insights = []
        for metric, importance in self.feature_importance.items():
            top_features = sorted(importance.items(), key=lambda x: x[1], reverse=True)[:15]
            for feature, impact in top_features:
                insight = self.create_human_readable_insight(feature, impact, metric)
                insights.append(insight)
        return insights

    def create_human_readable_insight(self, feature, impact, metric):
        if feature.startswith('time_'):
            return self.create_time_based_insight(feature, impact, metric)
        else:
            category, subcategory, specific_feature = feature.split('_', 2)
            return f"Including {specific_feature.replace('_', ' ')} in your {subcategory} increases {metric} by approximately {impact*100:.1f}%"

    def create_time_based_insight(self, feature, impact, metric):
        _, category, specific_feature = feature.split('_', 2)
        if category == 'basic_time':
            if specific_feature == 'hour':
                return f"Posting during the {self.get_hour_range(impact)} tends to increase {metric} by {impact*100:.1f}%"
            elif specific_feature == 'day_of_week':
                return f"Posting on {self.get_day_of_week(impact)} tends to increase {metric} by {impact*100:.1f}%"
        elif category == 'meal_times':
            return f"Posting during {specific_feature.replace('is_', '')} time increases {metric} by {impact*100:.1f}%"
        # Add more specific time-based insight generations as needed...
        return f"The timing feature '{specific_feature}' impacts {metric} by {impact*100:.1f}%"

    def get_hour_range(self, impact):
        hour = int(impact * 24)
        return f"{hour}:00 - {(hour + 1) % 24}:00"

    def get_day_of_week(self, impact):
        days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
        return days[int(impact * 7)]

    def predict_performance(self, new_content):
        predictions = {}
        for metric, model in self.models.items():
            X_scaled = self.scalers[metric].transform(new_content)
            predictions[metric] = model.predict(X_scaled)
        return predictions

    def visualize_feature_importance(self, metric):
        importance = self.feature_importance[metric]
        features = list(importance.keys())
        scores = list(importance.values())
        plt.figure(figsize=(10,8))
        plt.bar(range(len(scores)), scores)
        plt.xlabel('Features')
        plt.ylabel('Importance')
        plt.title(f'Feature Importance for {metric}')
        plt.xticks(range(len(scores)), features, rotation='vertical')
        plt.tight_layout()
        plt.show()

    def recommend_posting_time(self, content_type):
        # Implement logic to recommend the best posting time based on content type
        pass

    def optimize_posting_frequency(self):
        # Implement logic to suggest optimal posting frequency
        pass

    def analyze_audience_activity(self):
        # Implement logic to analyze audience activity
        pass

def main():
    # Load data
    rekognition_data = pd.read_csv('rekognition_analysis.csv')
    metrics_data = pd.read_csv('engagement_metrics.csv')
    post_data = pd.read_csv('post_data.csv')  # New file with posting timestamps and related info
    
    analyzer = ContentAnalyzer()
    
    # Extract features and prepare target variables
    X = analyzer.extract_features(rekognition_data, post_data)
    y_dict = analyzer.prepare_target_variables(metrics_data)
    
    # Train models
    analyzer.train_models(X, y_dict)
    
    # Generate insights
    insights = analyzer.generate_insights()
    for insight in insights:
        print(insight)
    
    # Example prediction using the first row as an example
    new_content = X.iloc[0].to_frame().T  
    predictions = analyzer.predict_performance(new_content)
    print("\nPredictions for new content:")
    for metric, value in predictions.items():
        print(f"{metric}: {value[0]:.2f}")
    
    analyzer.visualize_feature_importance('likes')
    
    # Time-based recommendations
    print("\nRecommended posting time for food content:")
    analyzer.recommend_posting_time('food')
    
    print("\nOptimal posting frequency:")
    analyzer.optimize_posting_frequency()
    
    print("\nAudience activity analysis:")
    analyzer.analyze_audience_activity()

if __name__ == "__main__":
    main()
class ContentAnalyzer:
    def __init__(self):
        self.models = {}
        self.scalers = {}
        self.feature_importance = {}
        self.features = self.create_detailed_feature_columns()

    def create_detailed_feature_columns(self):
        return {
            'human_presentation': {
                'face_expression': {
                    'smile_characteristics': ['smile_intensity', 'smile_duration', 'smile_genuineness', 'teeth_visibility', 'smile_symmetry'],
                    'laugh_characteristics': ['laugh_intensity', 'laugh_duration', 'laugh_sound_level', 'laugh_body_movement'],
                    'serious_expression': ['brow_furrowing', 'lip_tightness', 'jaw_clenching', 'eye_intensity'],
                    'surprise_elements': ['eyebrow_raise_speed', 'eye_widening_degree', 'mouth_opening_shape', 'surprise_duration'],
                    'eye_contact': ['direct_eye_contact_duration', 'eye_contact_frequency', 'eye_movement_patterns', 'pupil_dilation'],
                    'head_angle': ['head_tilt_degree', 'head_turn_angle', 'head_nod_frequency', 'head_stability'],
                    'micro_expressions': ['micro_expression_type', 'micro_expression_duration', 'micro_expression_intensity'],
                    'expression_transitions': ['transition_speed', 'transition_smoothness', 'expression_holding_time', 'expression_cycle_patterns']
                },
                'body_language': {
                    'hand_gestures': ['gesture_type', 'gesture_speed', 'gesture_amplitude', 'gesture_frequency', 'finger_positioning'],
                    'body_positioning': ['posture_type', 'posture_changes', 'weight_distribution', 'body_openness', 'body_mirroring'],
                    'movement_patterns': ['movement_speed', 'movement_fluidity', 'movement_range', 'movement_rhythm', 'stillness_periods'],
                    'sitting_characteristics': ['sitting_posture', 'leg_positioning', 'seat_edge_proximity', 'fidgeting_frequency', 'sitting_duration'],
                    'standing_characteristics': ['standing_posture', 'weight_shifting', 'foot_positioning', 'arm_placement', 'standing_duration'],
                    'interaction_positioning': ['interpersonal_distance', 'body_angling', 'eye_level_matching', 'touch_frequency', 'personal_space_management'],
                    'group_formation': ['formation_shape', 'spacing_consistency', 'formation_changes', 'central_figure_positioning', 'peripheral_positioning'],
                    'energy_level_indicators': ['movement_frequency', 'gesture_enthusiasm', 'vocal_energy', 'facial_animation', 'overall_dynamism']
                },
                'appearance': {
                    'outfit_analysis': {
                        'color_scheme': ['primary_color', 'secondary_color', 'color_contrast', 'color_coordination', 'color_psychology'],
                        'pattern_types': ['pattern_style', 'pattern_complexity', 'pattern_size', 'pattern_contrast', 'pattern_distribution'],
                        'outfit_components': ['top_style', 'bottom_style', 'outerwear_type', 'footwear_style', 'accessory_count'],
                        'fit_characteristics': ['looseness', 'tightness', 'draping', 'tailoring_quality', 'proportion_balance'],
                        'style_categorization': ['casual', 'formal', 'athletic', 'bohemian', 'preppy', 'edgy', 'vintage', 'minimalist', 'maximalist']
                    },
                    'accessory_details': {
                        'jewelry': ['necklace_style', 'earring_type', 'bracelet_placement', 'ring_count', 'watch_style'],
                        'bags': ['bag_type', 'bag_size', 'bag_material', 'bag_brand_visibility', 'bag_functionality'],
                        'eyewear': ['glasses_style', 'sunglasses_shape', 'lens_tint', 'frame_material', 'eyewear_brand'],
                        'headwear': ['hat_type', 'hat_positioning', 'hair_accessory_style', 'headwear_functionality']
                    },
                    'hair_styling': {
                        'hair_length': ['short', 'medium', 'long', 'varied_length'],
                        'hair_color': ['natural_color', 'dyed_color', 'highlights', 'ombre', 'balayage'],
                        'hair_texture': ['straight', 'wavy', 'curly', 'coily', 'mixed_texture'],
                        'hairstyle': ['up_do', 'down_style', 'braided', 'ponytail', 'bun', 'side_part', 'middle_part', 'bangs'],
                        'hair_volume': ['flat', 'medium_volume', 'high_volume', 'teased'],
                        'hair_shine': ['matte', 'natural_shine', 'high_shine', 'wet_look']
                    },
                    'makeup_analysis': {
                        'eye_makeup': ['eyeshadow_color', 'eyeliner_style', 'mascara_intensity', 'eyebrow_styling', 'false_lashes'],
                        'lip_makeup': ['lipstick_color', 'lip_gloss', 'lip_liner', 'lip_volume'],
                        'face_makeup': ['foundation_coverage', 'contouring_intensity', 'blush_application', 'highlighter_placement', 'overall_finish'],
                        'makeup_style': ['natural', 'glamorous', 'editorial', 'vintage', 'avant-garde']
                    },
                    'brand_visibility': ['logo_placement', 'brand_name_visibility', 'signature_design_elements', 'brand_color_usage', 'brand_associated_style'],
                    'style_consistency': ['outfit_theme_adherence', 'color_palette_consistency', 'accessory_coordination', 'style_evolution_over_time', 'personal_style_signature'],
                    'outfit_background_interaction': ['color_complement_to_background', 'style_match_with_setting', 'outfit_prominence_in_frame', 'background_element_coordination']
                },
                'personal_branding': {
                    'signature_poses': ['recurring_pose_types', 'pose_confidence', 'pose_uniqueness', 'pose_complexity', 'pose_brand_alignment'],
                    'facial_expression_consistency': ['signature_smile', 'expression_range_consistency', 'emotion_portrayal_style', 'micro-expression_patterns'],
                    'personal_style_evolution': ['style_change_frequency', 'trend_adoption_speed', 'style_experimentation_level', 'core_style_elements_retention', 'style_influence_on_audience'],
                    'recurring_items': ['signature_accessory', 'favorite_clothing_item', 'repeated_background_elements', 'consistent_color_choices', 'repeated_product_placements'],
                    'personal_logo_usage': ['logo_placement_consistency', 'logo_size_variations', 'logo_color_adaptations', 'logo_integration_subtlety', 'logo_screen_time'],
                    'filter_and_editing_consistency': ['color_grading_style', 'filter_type_consistency', 'editing_intensity_level', 'visual_effect_signatures', 'post-processing_techniques']
                },
                'authenticity': {
                    'behind_scenes_content': ['production_setup_visibility', 'candid_crew_interactions', 'blooper_inclusion_frequency', 'unpolished_content_ratio', 'real-time_vs_scripted_balance'],
                    'candid_moments': ['spontaneous_reaction_capture', 'unplanned_event_inclusion', 'natural_interaction_portrayal', 'genuine_emotion_display_frequency', 'guard_down_moments'],
                    'real_life_situations': ['everyday_challenge_portrayal', 'relatable_problem_solving', 'mundane_task_inclusion', 'imperfection_showcase', 'real-time_problem_handling'],
                    'emotional_vulnerability': ['personal_struggle_sharing', 'emotion_processing_on_camera', 'vulnerability_level_in_content', 'audience_connection_through_openness', 'recovery_and_growth_narratives'],
                    'personal_space_reveals': ['home_environment_sharing', 'private_space_access_level', 'personal_item_storytelling', 'space_personalization_showcase', 'comfort_zone_visibility'],
                    'daily_routine_authenticity': ['morning_routine_realness', 'work_process_transparency', 'unfiltered_day-in-life_content', 'habit_and_quirk_showcase', 'routine_deviation_honesty']
                }
            },
            'location_setting': {
                'environment_type': {
                    'indoor_settings': {
                        'kitchen': ['kitchen_size', 'appliance_modernity', 'counter_space', 'kitchen_organization', 'cooking_activity_level'],
                        'living_room': ['seating_arrangement', 'decor_style', 'natural_light_level', 'tv_presence', 'living_room_size'],
                        'bedroom': ['bed_size', 'bedroom_organization', 'personal_touches', 'sleeping_area_vs_workspace', 'bedroom_color_scheme'],
                        'bathroom': ['bathroom_size', 'fixture_quality', 'bathroom_cleanliness', 'storage_solutions', 'lighting_quality'],
                        'home_office': ['desk_setup', 'office_organization', 'tech_presence', 'productivity_tools_visibility', 'office_personalization'],
                        'gym': ['equipment_variety', 'gym_spaciousness', 'cleanliness_level', 'gym_occupancy', 'gym_lighting'],
                        'restaurant': ['seating_capacity', 'ambiance_type', 'table_arrangement', 'restaurant_theme_consistency', 'kitchen_visibility'],
                        'retail_space': ['store_layout', 'product_display_method', 'checkout_area_visibility', 'store_branding_presence', 'customer_density'],
                        'professional_setting': ['office_type', 'workspace_modernity', 'collaboration_spaces', 'professional_environment_formality', 'brand_presence_in_workspace']
                    },
                    'outdoor_settings': {
                        'beach': ['sand_quality', 'water_visibility', 'beach_occupancy', 'natural_features', 'beach_activities_present'],
                        'park': ['greenery_level', 'park_facilities', 'path_types', 'park_size_indication', 'park_usage_diversity'],
                        'city_street': ['building_density', 'street_busyness', 'urban_feature_types', 'street_cleanliness', 'city_skyline_visibility'],
                        'rural_area': ['landscape_type', 'vegetation_density', 'man-made_structure_presence', 'animal_presence', 'rural_activity_indicators'],
                        'mountain': ['mountain_range_visibility', 'elevation_indication', 'terrain_type', 'mountain_activity_presence', 'weather_conditions'],
                        'forest': ['tree_density', 'forest_type', 'undergrowth_level', 'forest_light_filtering', 'wildlife_indicators'],
                        'suburban_area': ['house_types', 'yard_presence', 'street_layout', 'community_features', 'suburban_activity_level']
                    },
                    'transitional_spaces': {
                        'patio_balcony': ['size', 'furniture_type', 'view_quality', 'plant_presence', 'indoor-outdoor_flow'],
                        'garden': ['garden_size', 'plant_variety', 'garden_features', 'maintenance_level', 'garden_purpose'],
                        'pool_area': ['pool_size', 'pool_type', 'surrounding_amenities', 'pool_cleanliness', 'pool_usage']
                    }
                },
                'background_elements': {
                    'busyness_level': ['background_object_count', 'movement_in_background', 'visual_complexity', 'background_noise_level', 'distraction_potential'],
                    'cleanliness': ['visible_organization', 'surface_cleanliness', 'clutter_presence', 'cleaning_activity_visibility', 'overall_hygiene_impression'],
                    'natural_elements': ['plant_presence', 'water_features', 'natural_light_quality', 'visible_sky', 'natural_material_usage'],
                    'urban_elements': ['building_visibility', 'street_feature_presence', 'urban_infrastructure_visibility', 'population_density_indicators', 'urban_style_elements'],
                    'decor_and_design': {
                        'art_presence': ['art_type', 'art_size', 'art_style', 'art_placement', 'art_color_scheme'],
                        'furniture': ['furniture_style', 'furniture_arrangement', 'furniture_color_scheme', 'furniture_functionality', 'furniture_quality'],
                        'color_scheme': ['wall_color', 'accent_colors', 'color_coordination', 'color_psychology_application', 'color_balance'],
                        'texture_elements': ['fabric_textures', 'wall_textures', 'floor_textures', 'textural_contrast', 'texture_layering']
                    }
                },
                'lighting_characteristics': {
                    'natural_light': ['sunlight_intensity', 'sunlight_direction', 'shadow_patterns', 'natural_light_color_temperature', 'time_of_day_indication'],
                    'artificial_light': ['light_fixture_types', 'light_temperature', 'light_intensity', 'light_distribution', 'mood_lighting'],
                    'lighting_quality': ['soft_vs_hard_light', 'light_evenness', 'shadow_presence', 'light_flicker', 'color_rendering_accuracy'],
                    'lighting_techniques': ['backlighting', 'side_lighting', 'uplighting', 'spotlight_usage', 'ambient_lighting'],
                    'lighting_for_video': ['key_light_setup', 'fill_light_usage', 'rim_light_presence', 'lighting_for_skin_tones', 'lighting_consistency_across_shots']
                },
                'time_and_season_indicators': {
                    'time_of_day': ['morning_light_quality', 'midday_shadow_characteristics', 'afternoon_golden_hour', 'evening_light_transition', 'night_lighting_conditions'],
                    'seasonal_cues': ['spring_bloom_visibility', 'summer_light_intensity', 'fall_color_changes', 'winter_environment_indicators'],
                    'weather_conditions': ['clear_sky_appearance', 'cloud_cover_type', 'rain_or_snow_presence', 'wind_effect_visibility', 'weather_appropriate_activities']
                }
            },
            'content_action': {
                'demonstration_techniques': {
                    'step_by_step_process': {
                        'step_organization': ['step_numbering_visibility', 'step_transition_clarity', 'step_duration_consistency', 'step_complexity_progression', 'step_prerequisite_clarity'],
                        'visual_clarity': ['camera_angle_optimization', 'focus_point_accuracy', 'lighting_per_step', 'background_distraction_level', 'detail_visibility'],
                        'instruction_delivery': ['vocal_clarity', 'instruction_pace', 'terminology_usage', 'emphasis_on_key_points', 'common_mistake_warnings'],
                        'timing_elements': ['step_duration_appropriateness', 'pause_length_between_steps', 'real_time_vs_timelapse', 'rushing_indicator', 'viewer_processing_time']
                    },
                    'before_after_presentation': {
                        'before_state_documentation': ['initial_condition_clarity', 'problem_area_highlighting', 'starting_point_context', 'before_shot_duration', 'multiple_angle_coverage'],
                        'process_visibility': ['key_change_moments', 'progress_indicators', 'transformation_timeline', 'effort_level_visibility', 'setback_transparency'],
                        'after_reveal': ['dramatic_effect_usage', 'result_highlighting', 'multiple_view_angles', 'detail_focus_shots', 'reaction_capture'],
                        'comparison_techniques': ['split_screen_usage', 'sliding_comparison_tool', 'side_by_side_placement', 'lighting_consistency', 'angle_matching']
                    }
                },
                'lifestyle_activities': {
                    'daily_routine_capture': {
                        'morning_sequence': {
                            'wake_up_moment': ['natural_vs_alarm', 'light_condition', 'wake_up_reaction', 'bed_exit_method', 'first_action_timing'],
                            'grooming_routine': ['bathroom_sequence_order', 'product_usage_detail', 'technique_demonstration', 'time_management_visibility', 'routine_efficiency'],
                            'breakfast_preparation': ['meal_type_selection', 'preparation_method', 'multitasking_evidence', 'nutrition_consideration', 'time_constraint_handling']
                        },
                        'work_preparation': {
                            'outfit_selection': ['selection_process', 'try_on_moments', 'accessory_coordination', 'weather_consideration', 'occasion_appropriateness'],
                            'bag_packing': ['essential_item_check', 'organization_method', 'last_minute_additions', 'forgotten_item_handling', 'efficiency_level'],
                            'commute_preparation': ['time_check_frequency', 'weather_checking', 'transport_mode_selection', 'backup_plan_evidence', 'stress_level_indicators']
                        }
                    },
                    'fitness_activities': {
                        'workout_preparation': {
                            'gear_selection': ['appropriate_attire', 'equipment_gathering', 'pre_workout_setup', 'safety_gear_check', 'accessory_preparation'],
                            'warm_up_sequence': ['warm_up_duration', 'movement_types', 'intensity_progression', 'form_attention', 'breathing_pattern'],
                            'equipment_setup': ['weight_selection', 'machine_adjustment', 'space_organization', 'safety_check', 'cleanliness_maintenance']
                        },
                        'exercise_execution': {
                            'form_demonstration': ['movement_precision', 'alignment_check', 'tempo_control', 'breathing_coordination', 'muscle_engagement_cues'],
                            'progression_tracking': ['weight_increment', 'rep_counting', 'set_completion', 'rest_period_timing', 'fatigue_management'],
                            'variation_showcase': ['exercise_modifications', 'difficulty_levels', 'alternative_movements', 'equipment_substitutions', 'adaptation_explanations']
                        }
                    },
                    'social_interactions': {
                        'group_dynamics': {
                            'conversation_flow': ['topic_transitions', 'engagement_levels', 'turn_taking', 'reaction_authenticity', 'group_energy'],
                            'physical_positioning': ['group_formation', 'personal_space', 'body_language', 'eye_contact_patterns', 'movement_synchronization'],
                            'activity_coordination': ['decision_making', 'task_distribution', 'cooperation_level', 'conflict_resolution', 'group_consensus']
                        },
                        'event_participation': {
                            'arrival_behavior': ['timing_appropriateness', 'greeting_style', 'initial_interaction', 'space_navigation', 'social_integration'],
                            'engagement_level': ['conversation_initiation', 'activity_participation', 'attention_distribution', 'energy_contribution', 'departure_timing'],
                            'social_documentation': ['photo_taking', 'moment_sharing', 'memory_creation', 'group_inclusion', 'storytelling_elements']
                        }
                    }
                },
                'entertainment_performance': {
                    'dance_execution': {
                        'movement_quality': {
                            'rhythm_alignment': ['beat_matching', 'tempo_consistency', 'musical_interpretation', 'rhythm_complexity', 'timing_precision'],
                            'body_control': ['balance_maintenance', 'transition_smoothness', 'isolation_clarity', 'movement_fluidity', 'energy_control'],
                            'spatial_awareness': ['space_utilization', 'direction_changes', 'level_variations', 'formation_awareness', 'pathway_clarity']
                        },
                        'performance_elements': {
                            'facial_expression': ['emotion_conveyance', 'audience_connection', 'character_maintenance', 'expression_timing', 'authenticity_level'],
                            'costume_interaction': ['costume_management', 'prop_integration', 'wearing_technique', 'movement_adaptation', 'visual_effect'],
                            'camera_interaction': ['angle_awareness', 'frame_positioning', 'movement_adaptation', 'focal_point_emphasis', 'spatial_composition']
                        }
                    }
                },
                'entertainment_performance': {
                    'music_performance': {
                        'vocal_delivery': {
                            'pitch_control': ['note_accuracy', 'pitch_stability', 'key_transitions', 'vocal_range_utilization', 'pitch_correction_visibility'],
                            'breath_control': ['phrase_length_management', 'breathing_technique', 'power_distribution', 'sustained_note_control', 'breath_support_evidence'],
                            'tone_quality': ['voice_texture', 'tone_consistency', 'vocal_color', 'resonance_placement', 'timbre_control'],
                            'expression': ['emotional_conveyance', 'dynamic_variation', 'stylistic_choices', 'interpretative_elements', 'authentic_delivery']
                        },
                        'instrumental_performance': {
                            'technique_execution': ['finger_placement', 'hand_position', 'posture_maintenance', 'instrument_handling', 'technical_difficulty_level'],
                            'rhythm_precision': ['tempo_maintenance', 'rhythmic_accuracy', 'groove_consistency', 'syncopation_handling', 'meter_changes'],
                            'sound_quality': ['tone_production', 'volume_control', 'articulation_clarity', 'sound_balance', 'acoustic_environment_interaction']
                        },
                        'performance_presentation': {
                            'stage_presence': ['confidence_level', 'audience_engagement', 'movement_appropriateness', 'space_utilization', 'performance_energy'],
                            'visual_elements': ['lighting_interaction', 'camera_angle_utilization', 'performance_setting', 'outfit_effectiveness', 'background_elements']
                        }
                    },
                    'comedy_performance': {
                        'timing_elements': {
                            'joke_delivery': ['setup_pacing', 'punchline_timing', 'pause_utilization', 'audience_reaction_time', 'recovery_speed'],
                            'physical_comedy': ['movement_timing', 'gesture_synchronization', 'facial_expression_timing', 'prop_interaction_timing', 'slapstick_execution']
                        },
                        'content_structure': {
                            'bit_construction': ['premise_establishment', 'buildup_progression', 'payoff_delivery', 'callback_integration', 'theme_connection'],
                            'narrative_flow': ['story_arc', 'tension_building', 'resolution_timing', 'subplot_weaving', 'perspective_shifts']
                        },
                        'audience_interaction': {
                            'engagement_tactics': ['crowd_work_timing', 'response_handling', 'energy_management', 'attention_direction', 'participation_encouragement'],
                            'reaction_management': ['laughter_timing', 'silence_handling', 'unexpected_response_adaptation', 'momentum_maintenance', 'energy_recovery']
                        }
                    },
                    'dramatic_performance': {
                        'character_embodiment': {
                            'emotional_portrayal': ['emotion_authenticity', 'emotional_progression', 'subtle_expression', 'emotional_contrast', 'emotional_layering'],
                            'physical_characterization': ['posture_adaptation', 'movement_style', 'gestural_choices', 'character_mannerisms', 'physical_transformation'],
                            'vocal_characterization': ['accent_consistency', 'vocal_placement', 'speech_pattern', 'volume_modulation', 'vocal_age_portrayal']
                        },
                        'scene_execution': {
                            'partner_interaction': ['eye_contact_usage', 'physical_spacing', 'emotional_connection', 'reaction_timing', 'energy_exchange'],
                            'environment_interaction': ['space_awareness', 'prop_handling', 'set_integration', 'atmospheric_response', 'blocking_execution'],
                            'emotional_progression': ['buildup_control', 'climax_timing', 'resolution_pacing', 'emotional_arc', 'subtextual_layering']
                        }
                    }
                },
                'educational_content': {
                    'information_delivery': {
                        'concept_introduction': {
                            'opening_hook': ['attention_grabbing_element', 'relevance_establishment', 'curiosity_triggering', 'problem_presentation', 'solution_preview'],
                            'concept_breakdown': ['complexity_staging', 'foundational_element_identification', 'building_block_progression', 'relationship_mapping', 'example_integration'],
                            'visual_support': ['diagram_clarity', 'illustration_relevance', 'animation_effectiveness', 'text_overlay_timing', 'visual_hierarchy']
                        },
                        'explanation_techniques': {
                            'verbal_communication': ['vocabulary_level', 'pace_variation', 'emphasis_placement', 'clarification_timing', 'question_anticipation'],
                            'demonstration_method': ['hands_on_elements', 'step_sequencing', 'mistake_prevention', 'alternative_approaches', 'practice_opportunity'],
                            'engagement_maintenance': ['attention_redirection', 'interest_sustaining', 'complexity_management', 'energy_level', 'participation_encouragement']
                        }
                    },
                    'subject_specific_approaches': {
                        'technical_instruction': {
                            'tool_introduction': ['tool_identification', 'functionality_explanation', 'safety_consideration', 'maintenance_guidance', 'troubleshooting_tips'],
                            'process_demonstration': ['sequence_clarity', 'critical_point_emphasis', 'error_prevention', 'efficiency_tips', 'quality_check_points'],
                            'problem_solving': ['issue_identification', 'solution_approach', 'alternative_methods', 'decision_making_process', 'outcome_verification']
                        },
                        'theoretical_concepts': {
                            'abstract_explanation': ['concept_visualization', 'real_world_connection', 'analogy_usage', 'complexity_breakdown', 'understanding_verification'],
                            'logical_progression': ['premise_establishment', 'reasoning_chain', 'conclusion_development', 'exception_handling', 'application_demonstration']
                        }
                    }
                },
                'product_interaction': {
                    'product_showcase': {
                        'initial_presentation': {
                            'unboxing_sequence': ['package_condition', 'opening_method', 'component_reveal', 'first_impression_capture', 'protection_assessment'],
                            'product_overview': ['size_demonstration', 'color_accuracy', 'texture_visibility', 'feature_highlight', 'comparison_reference'],
                            'quality_assessment': ['material_examination', 'construction_review', 'finish_inspection', 'defect_checking', 'durability_indication']
                        },
                        'feature_demonstration': {
                            'functionality_testing': ['feature_activation', 'performance_measurement', 'user_interface_interaction', 'capability_limits', 'reliability_testing'],
                            'use_case_scenarios': ['practical_application', 'situation_simulation', 'problem_solving_demonstration', 'versatility_showcase', 'limitation_acknowledgment']
                        }
                    }
                    # ... continuing with more categories
                },
                'product_interaction': {
                    # ... previous sections ...
                    'comparison_and_review': {
                        'competitive_analysis': {
                            'feature_comparison': ['specification_matching', 'performance_benchmarking', 'user_experience_contrast', 'price_point_evaluation', 'target_audience_alignment'],
                            'visual_comparison': ['side_by_side_presentation', 'size_comparison', 'design_element_contrast', 'color_option_range', 'aesthetic_appeal_judgement'],
                            'brand_positioning': ['market_placement', 'brand_value_proposition', 'competitor_differentiation', 'consumer_perception_analysis', 'trend_alignment']
                        },
                        'long_term_usage': {
                            'durability_assessment': ['wear_and_tear_evidence', 'stress_test_results', 'longevity_projection', 'maintenance_requirement', 'repair_history'],
                            'user_adaptation': ['learning_curve_description', 'habit_formation_observation', 'long_term_satisfaction', 'feature_utilization_over_time', 'user_proficiency_development'],
                            'value_over_time': ['cost_per_use_calculation', 'long_term_benefit_analysis', 'alternative_cost_comparison', 'upgrade_necessity_evaluation', 'resale_value_consideration']
                        }
                    },
                    'sponsored_content_integration': {
                        'disclosure_practices': {
                            'sponsorship_declaration': ['disclosure_timing', 'verbal_vs_written_disclosure', 'disclosure_clarity', 'sponsorship_nature_explanation', 'repeated_disclosure_in_long_content'],
                            'authenticity_maintenance': ['personal_experience_sharing', 'balanced_review_approach', 'criticism_inclusion', 'genuine_enthusiasm_indicators', 'alignment_with_personal_brand']
                        },
                        'brand_message_delivery': {
                            'key_point_emphasis': ['brand_slogan_integration', 'unique_selling_point_highlight', 'target_audience_consideration', 'benefit_articulation', 'feature_prioritization'],
                            'call_to_action': ['cta_placement', 'incentive_clarity', 'urgency_creation', 'multiple_cta_varieties', 'viewer_journerney_consideration']
                        }
                    }
                },
                'content_structure': {
                    'pacing_and_rhythm': {
                        'opening_sequence': {
                            'hook_effectiveness': ['attention_grab_speed', 'curiosity_trigger_strength', 'relevance_establishment', 'tone_setting', 'expectation_management'],
                            'introduction_pacing': ['information_density', 'personality_emergence_rate', 'context_provision_speed', 'viewer_orientation_efficiency', 'initial_el_engagement_sustainability']
                        },
                        'content_flow': {
                            'segment_transitions': ['transition_smoothness', 'thematic_connection_clarity', 'pace_variation', 'attention_retention_techniques', 'logical_progression'],
                            'energy_modulation': ['high_energy_segment_placement', 'calm_moment_integration', 'emotional_pacing', 'viewer_fatigue_consideration', 'climax_build_up'],
                            'information_density': ['complex_info_chunk_size', 'simplification_techniques', 'repetition_strategy', 'key_point_emphasis', 'information_hierarchy']
                        },
                        'conclusion_and_call_to_action': {
                            'wrap_up_efficiency': ['key_point_recapitulation', 'emotional_resolution', 'future_content_teasing', 'viewer_reflection_prompts', 'satisfaction_delivery'],
                            'cta_integration': ['cta_natural_placement', 'value_proposition_clarity', 'action_step_simplicity', 'motivation_alignment', 'cta_variety']
                        }
                    },
                    'visual_storytelling': {
                        'shot_composition': {
                            'framing_techniques': ['rule_of_thirds_application', 'leading_lines_usage', 'depth_creation', 'symmetry_vs_asymmetry', 'negative_space_utilization'],
                            'subject_positioning': ['eye_line_consideration', 'power_dynamics_through_position', 'movement_space_allowance', 'background_interaction', 'multi_subject_balance'],
                            'color_composition': ['color_scheme_coherence', 'emotional_color_usage', 'contrast_implementation', 'brand_color_integration', 'color_symbolism']
                        },
                        'camera_movement': {
                            'static_shots': ['tripod_stability', 'framing_precision', 'duration_appropriateness', 'subject_movement_within_frame', 'intentional_stillness'],
                            'dynamic_movement': ['pan_smoothness', 'tilt_purpose', 'dolly_movement_impact', 'handheld_shake_intentionality', 'gimbal_fluidity'],
                            'transition_techniques': ['cut_timing', 'dissolve_usage', 'wipe_creativity', 'match_cut_effectiveness', 'transition_relevance_to_content']
                        }
                    },
                    'audio_design': {
                        'speech_and_narration': {
                            'vocal_clarity': ['enunciation_precision', 'volume_consistency', 'microphone_quality', 'background_noise_management', 'vocal_processing'],
                            'speech_pacing': ['word_speed', 'pause_utilization', 'emphasis_through_pacing', 'rhythmic_ic_speech_patterns', 'breath_control'],
                            'tonal_variation': ['emotional_conveyance', 'sarcasm_indication', 'excitement_levels', 'seriousness_vs_levity', 'audience_directed_tone']
                        },
                        'music_integration': {
                            'background_music': ['volume_balance', 'mood_matching', 'genre_appropriateness', 'lyric_vs_instrumental_choice', 'music_fade_techniques'],
                            'musical_accents': ['sync_with_visual_elements', 'emotional_punctuation', 'transition_enhancement', 'branding_through_music', 'silence_utilization']
                        },
                        'sound_effects': {
                            'ambient_sounds': ['environmental_authenticity', 'atmospheric_enhancement', 'subtle_mood_setting', 'spatialization', 'continuity_maintenance'],
                            'action_emphasis': ['movement_sound_sync', 'impact_sound_design', 'characteristic_sound_signatures', 'sound_metaphors', 'audio_hyperbole_for_effect']
                        }
                    },
                    'narrative_techniques': {
                        'storytelling_arc': {
                            'setup': ['character_introduction', 'world_building', 'initial_situation_establishment', 'viewer_connection_formation', 'curiosity_cultivation'],
                            'conflict_development': ['tension_introduction', 'stake_raising', 'obstacle_presentation', 'character_challenge', 'pacing_of_complications'],
                            'climax_construction': ['tension_peak', 'critical_moment_framing', 'emotional_intensity', 'payoff_delivery', 'viewer_anticipation_fulfillment'],
                            'resolution_and_conclusion': ['conflict_resolution_method', 'character_arc_completion', 'lesson_or_moral_presentation', 'emotional_satisfaction_delivery', 'open_vs_closed_ending']
                        },
                        'engagement_techniques': {
                            'suspense_building': ['information_withholding', 'foreshadowing', 'time_pressure_creation', 'stakes_elevation', 'mystery_element_introduction'],
                            'emotional_manipulation': ['empathy_generation', 'contrast_for_emotional_impact', 'music_for_mood', 'visual_symbolism', 'character_vulnerability_exposure'],
                            'humor_integration': ['comedy_timing', 'expectation_subversion', 'recurring_jokes', 'situational_humor', 'self_deprecation_balance']
                        }
                    }
                },
                'food_content': {
                    'cooking_preparation': {
                        'ingredient_showcase': {
                            'visual_presentation': ['ingredient_freshness_display', 'color_variety', 'texture_visibility', 'size_comparison', 'quantity_representation'],
                            'origin_and_quality': ['sourcing_information', 'organic_vs_conventional', 'brand_showcase', 'quality_grade_indication', 'local_vs_imported'],
                            'alternative_options': ['substitution_suggestions', 'dietary_restriction_options', 'seasonal_variations', 'budget_friendly_alternatives', 'ethnic_ingredient_explanations']
                        },
                        'technique_demonstration': {
                            'knife_skills': ['grip_demonstration', 'cutting_speed', 'consistency_of_cuts', 'finger_safety', 'knife_type_selection'],
                            'heat_control': ['temperature_setting', 'heat_distribution_management', 'cooking_vessel_choice', 'flame_control', 'oven_management'],
                            'mixing_and_blending': ['ingredient_incorporation', 'tool_selection', 'texture_achievement', 'over_mixing_prevention', 'emulsion_creation']
                        }
                    },
                    'plating_and_presentation': {
                        'composition': {
                            'color_balance': ['complementary_color_usage', 'monochromatic_themes', 'color_contrast', 'garnish_color_selection', 'sauce_color_impact'],
                            'texture_variety': ['crunchy_element_placement', 'soft_texture_balance', 'sauce_application_method', 'textural_garnishes', 'layering_techniques'],
                            'height_and_dimension': ['vertical_element_incorporation', 'layering_strategy', 'negative_space_usage', 'portion_mounding', '3D_presentation_techniques']
                        },
                        'plating_techniques': {
                            'sauce_application': ['drizzle_patterns', 'sauce_smear_techniques', 'dots_and_spots', 'sauce_pooling', 'hidden_sauce_placement'],
                            'portion_control': ['protein_portioning', 'starch_serving_size', 'vegetable_arrangement', 'sauce_quantity', 'garnish_proportion'],
                            'garnish_placement': ['edible_garnish_selection', 'garnish_relevance', 'micro_green_usage', 'herb_sprinkling_technique', 'geometric_garnish_patterns']
                        }
                    },
                    'eating_experience': {
                        'first_bite': {
                            'anticipation_building': ['steam_visibility', 'aroma_description', 'utensil_selection', 'bite_size_consideration', 'texture_expectation_setting'],
                            'reaction_capture': ['facial_expression', 'verbal_response', 'chewing_analysis', 'swallowing_observation', 'immediate_after-taste_reaction'],
                            'sensory_description': ['flavor_profile_articulation', 'texture_experience', 'temperature_sensation', 'aftertaste_description', 'mouthfeel_analysis']
                        },
                        'multi_course_progression': {
                            'palate_preparation': ['amuse_bouche_presentation', 'palate_cleanser_usage', 'appetizer_to_entree_transition', 'beverage_pairing_introduction'],
                            'course_flow': ['portion_size_progression', 'flavor_intensity_development', 'texture_variety_throughout_meal', 'temperature_contrasts', 'cuisine_style_coherence'],
                            'dining_pacing': ['inter_course_timing', 'rest_period_allowance', 'conversation_breaks', 'anticipation_building_between_courses', 'meal_duration_management']
                        }
                    }
                },
                'lifestyle_content': {
                    'daily_routines': {
                        'morning_rituals': {
                            'wake_up_sequence': ['alarm_interaction', 'bed_exit_method', 'immediate_environment_interaction', 'first_action_post_waking', 'energy_level_indicators'],
                            'grooming_routine': ['bathroom_sequence', 'product_application_order', 'time_management', 'multitasking_behaviors', 'self_care_emphasis'],
                            'breakfast_habits': ['meal_choice_reasoning', 'preparation_speed', 'nutrition_consideration', 'eating_environment', 'morning_beverage_ritual']
                        },
                        'work_preparation': {
                            'outfit_selection': ['closet_interaction', 'decision_making_process', 'weather_consideration', 'style_vs_comfort_balance', 'accessorizing_steps'],
                            'commute_rituals': ['transportation_mode', 'time_optimization', 'commute_activity', 'stress_management', 'arrival_preparation']
                        }
                    },
                    'home_organization': {
                        'decluttering_process': {
                            'item_sorting': ['keep_donate_discard_system', 'emotional_attachment_handling', 'usefulness_evaluation', 'quantity_reduction_strategies', 'storage_capacity_consideration'],
                            'space_optimization': ['vertical_space_utilization', 'multi_functional_furniture', 'hidden_storage_solutions', 'room_zoning', 'minimalism_application']
                        },
                        'cleaning_routines': {
                            'daily_maintenance': ['quick_pick_up_habits', 'surface_wiping_frequency', 'laundry_management', 'dish_washing_routine', 'bed_making_ritual'],
                            'deep_cleaning_sessions': ['scheduled_vs_spontaneous', 'task_prioritization', 'tool_and_product_selection', 'hard_to_reach_area_attention', 'post_cleaning_reward_system']
                        }
                    },
                    'wellness_practices': {
                        'fitness_routines': {
                            'workout_planning': ['goal_setting', 'schedule_integration', 'exercise_variety', 'progression_tracking', 'rest_day_planning'],
                            'execution_and_form': ['warm_up_routine', 'proper_technique_emphasis', 'intensity_management', 'cool_down_practices', 'post_workout_recovery']
                        },
                        'mental_health_management': {
                            'stress_reduction_techniques': ['meditation_practices', 'breathing_exercises', 'nature_interaction', 'creative_outlet_utilization', 'social_connection_nurturing'],
                            'productivity_and_focus': ['task_prioritization', 'distraction_management', 'work_break_balance', 'environment_optimization', 'goal_visualization']
                        }
                    },
                    'relationship_dynamics': {
                        'family_interactions': {
                            'quality_time_creation': ['shared_activity_planning', 'device_free_periods', 'meaningful_conversation_initiation', 'tradition_establishment', 'individual_attention_distribution'],
                            'conflict_resolution': ['communication_style', 'empathy_demonstration', 'compromise_negotiation', 'forgiveness_practices', 'boundary_setting']
                        },
                        'social_life_balance': {
                            'friendship_maintenance': ['regular_check_ins', 'shared_experience_creation', 'support_system_utilization', 'long_distance_friendship_strategies', 'group_dynamic_navigation'],
                            'networking_practices': ['professional_relationship_cultivation', 'social_media_engagement', 'industry_event_participation', 'follow_up_habits', 'value_exchange_focus']
                        }
                    }
                },
                'restaurant_showcase': {
                    'ambiance_and_setting': {
                        'exterior_presentation': {
                            'facade_appeal': ['architectural_style', 'signage_visibility', 'lighting_design', 'outdoor_seating_arrangement', 'entrance_accessibility'],
                            'surrounding_area': ['neighborhood_character', 'parking_availability', 'pedestrian_traffic', 'complementary_businesses', 'scenic_views']
                        },
                        'interior_design': {
                            'seating_arrangement': ['table_spacing', 'booth_vs_table_ratio', 'bar_seating_design', 'private_dining_options', 'communal_table_presence'],
                            'lighting_scheme': ['ambient_lighting_level', 'accent_lighting_usage', 'natural_light_integration', 'mood_setting_through_light', 'table_lighting'],
                            'decor_elements': ['color_scheme', 'artwork_selection', 'plant_life_integration', 'thematic_consistency', 'local_culture_representation']
                        }
                    },
                    'food_interaction': {
                        'consumption_technique': {
                            'bite_mechanics': {
                                'first_bite_approach': {
                                    'bite_size_selection': ['small_testing_bite', 'confident_full_bite', 'careful_edge_bite', 'center_targeted_bite', 'structured_layered_bite'],
                                    'bite_angle': ['direct_front_approach', 'side_angle_bite', '45_degree_approach', 'top_down_bite', 'bottom_up_technique'],
                                    'pre_bite_preparation': ['food_item_rotation', 'optimal_angle_finding', 'component_adjustment', 'sauce_distribution_check', 'structural_integrity_assessment']
                                },
                                'bite_execuecution': {
                                    'pressure_control': ['gentle_initial_pressure', 'firm_decisive_bite', 'progressive_pressure_increase', 'texture_adapted_force', 'bite_depth_control'],
                                    'cleanliness_management': ['drip_prevention_technique', 'sauce_control_method', 'filling_containment', 'neat_separation', 'mess_minimization_strategy'],
                                    'multi_component_handling': ['layer_compression_technique', 'filling_retention_method', 'structural_support_maintenance', 'component_balance_preseeservation', 'ingredient_distribution_control']
                                }
                            },
                            'utensil_usage': {
                                'fork_techniques': {
                                    'grip_style': ['formal_dining_grip', 'casual_hold', 'specialized_technique_grip', 'adaptable_position', 'cultural_specific_method'],
                                    'food_capture': ['precision_spearing', 'scooping_technique', 'twirling_method', 'gathering_approach', 'separating_motion'],
                                    'manipulation': ['food_turning', 'portion_control', 'ingrediedient_combining', 'sauce_collection', 'plate_cleaning']
                                },
                                'chopstick_mastery': {
                                    'grip_fundamentals': ['finger_position', 'pressure_control', 'stability_maintenance', 'flexibility_allowance', 'comfort_optimization'],
                                    'picking_techniques': ['precision_grasp', 'delicate_item_handling', 'larger_item_management', 'slippery_food_control', 'texture_specific_approach'],
                                    'advanced_maneuvers': ['ingredient_sorting', 'liquid_content_handling', 'noodle_gathering', 'rice_compression', 'sauce_application']
                                },
                                'spoon_utilization': {
                                    'soup_consumption': ['surface_skimming', 'temperature_testing', 'ingredient_selection', 'broth_balance', 'spill_prevention'],
                                    'dessert_enjoyment': ['portion_sizing', 'texture_preservation', 'layered_sampling', 'sauce_incorporation', 'melting_management'],
                                    'shared_dish_service': ['serving_portion_control', 'liquid_transfer', 'solid_separation', 'sauce_distribution', 'plating_assistance']
                                }
                            }
                        },
                        'multi_sensory_experience': {
                            'visual_appreciation': {
                                'color_analysis': ['color_vibrancy_observation', 'contrast_appreciation', 'color_combination_assessment', 'garnish_color_impact', 'visual_temperature_cues'],
                                'plating_evaluation': ['composition_observation', 'negative_space_appreciation', 'height_variation_notice', 'symmetry_analysis', 'portion_size_visual'],
                                'movement_observation': ['steam_rising_patterns', 'sauce_flow_watching', 'melting_progression', 'structural_changes', 'temperature_indicators']
                            },
                            'aroma_experience': {
                                'initial_scent': ['first_st_impression_aroma', 'intensity_level', 'complexity_recognition', 'familiarity_identification', 'emotional_response'],
                                'layered_fragrances': ['primary_aroma_identification', 'secondary_note_detection', 'spice_recognition', 'herb_fragrance_isolation', 'cooking_method_indicdicators'],
                                'aroma_progression': ['temperature_impact_on_smell', 'time_based_changes', 'mixing_effect_on_aroma', 'environmental_influence', 'distance_perception']
                            },
                            'texture_engagement': {
                                'surface_texture': ['initial_touch_sensation', 'surface_pattern_recognition', 'temperature_contrast', 'moisture_level_assessment', 'textural_complexity'],
                                'internal_texture': ['bite_resistance_level', 'internal_structure_feel', 'moisture_content_experience', 'texture_transitions', 'consistency_evaluation'],
                                'mouthfeel_analysis': ['initial_contact_sensation', 'dissolution_pattern', 'coating_characteristics', 'particulate_presence', 'texture_longevity']
                            },
                            'flavor_analysis': {
                                'initial_taste': ['first_flavor_impact', 'immediate_intensity', 'flavor_location_on_tongue', 'taste_bud_activation', 'initial_complexity'],
                                'flavor_development': ['taste_evolution', 'flavor_layering', 'spice_development', 'umami_recognition', 'aftertaste_progression'],
                                'temperature_impact': ['hot_food_perception', 'cold_element_contrast', 'temperature_flavor_relationship', 'optimal_temperature_recognition', 'cooling_effect_on_taste']
                            }
                        },
                        'social_dining_dynamics': {
                            'sharing_behaviors': {
                                'portion_distribution': ['serving_size_determination', 'equal_division_technique', 'preference_consideration', 'sharing_tool_usage', 'portion_negotiation'],
                                'communal_plate_navigation': ['reach_and_access_patterns', 'turn_taking_protocol', 'shared_space_management', 'utensil_coordination', 'food_territory_respect'],
                                'conversation_integration': ['bite_timing_with_dialogue', 'pause_patterns', 'discussion_topic_flow', 'group_rhythm_synchronization', 'multi_tasking_efficiency']
                            },
                            'dining_etiquette': {
                                'cultural_awareness': ['chopstick_protocol', 'hand_usage_customs', 'shared_dish_etiquette', 'eating_order_observation', 'cultural_specific_gestures'],
                                'pace_matching': ['group_eating_speed_alignment', 'course_timing_coordination', 'wait_period_observation', 'finish_time_management', 'break_taking_synchronization'],
                                'social_cues': ['readiness_signaling', 'satisfaction_indication', 'sharing_invitation_gestures', 'completion_communication', 'preference_expression']
                            }
                        },
                        'documentation_behavior': {
                            'photo_technique': {
                                'angle_selection': ['overhead_composition', 'side_profile_capture', '45_degree_perspective', 'detail_close_up', 'context_inclusion'],
                                'lighting_optimization': ['natural_light_utilization', 'shadow_management', 'reflection_control', 'highlight_preservation', 'color_accuracy_maintenance'],
                                'timing_consideration': ['steam_capture', 'action_shot_timing', 'melting_moment_documentation', 'pouring_sequence_recording', 'reaction_shot_coordination']
                            },
                            'video_documentation': {
                                'movement_capture': ['smooth_panning', 'focus_pulling', 'reveal_sequence', 'action_tracking', 'stability_maintenance'],
                                'sound_recording': ['reaction_audio_capture', 'ambient_sound_balance', 'commentary_clarity', 'background_noise_management', 'eating_sound_control'],
                                'narrative_elements': ['story_progression', 'detail_emphasis', 'reaction_inclusion', 'context_establishment', 'conclusion_satisfaction']
                            }
                        }
                    }
                }
            }
        }

# End of create_detailed_feature_columns dictionary
import pandas as pd
import numpy as np
from typing import Dict, List, Any
import boto3
from collections import defaultdict

class FeatureExtractor:
    def __init__(self):
        self.confidence_threshold = 80  # Minimum confidence for Rekognition detections

    def extract_food_interaction_features(self, rekognition_data: dict) -> Dict[str, Any]:
        """
        Extract detailed food interaction features from Rekognition data
        """
        features = defaultdict(dict)
        
        # Consumption Technique Analysis
        features['consumption_technique'] = {
            'bite_mechanics': self._analyze_bite_mechanics(rekognition_data),
            'utensil_usage': self._analyze_utensil_usage(rekognition_data),
            'hand_interaction': self._analyze_hand_interaction(rekognition_data)
        }
        
        # Multi-sensory Experience Indicators
        features['visual_indicators'] = self._analyze_visual_elements(rekognition_data)
        
        # Social Dining Elements
        features['social_dining'] = self._analyze_social_dining(rekognition_data)
        
        return dict(features)

    def _analyze_bite_mechanics(self, data: dict) -> dict:
        """Analyze bite mechanics from Rekognition data"""
        bite_features = defaultdict(float)
        
        # First Bite Detection
        labels = data.get('Labels', [])
        faces = data.get('FaceDetails', [])
        
        # Analyze bite size and approach
        bite_features.update({
            'small_testing_bite': self._detect_bite_size(faces, 'small'),
            'confident_full_bite': self._detect_bite_size(faces, 'large'),
            'bite_angle_front': self._detect_bite_angle(faces, 'front'),
            'bite_angle_side': self._detect_bite_angle(faces, 'side'),
            'bite_angle_45': self._detect_bite_angle(faces, '45_degree')
        })
        
        # Analyze pre-bite preparation
        bite_features.update(self._analyze_pre_bite_behavior(data))
        
        return dict(bite_features)

    def _detect_bite_size(self, faces: List[dict], size_type: str) -> float:
        """
        Detect bite size based on mouth opening and other facial features
        Returns confidence score (0-100)
        """
        if not faces:
            return 0.0
            
        for face in faces:
            landmarks = face.get('Landmarks', [])
            mouth_points = [point for point in landmarks if 'Mouth' in point.get('Type', '')]
            
            if mouth_points:
                mouth_height = self._calculate_mouth_height(mouth_points)
                
                if size_type == 'small':
                    return 100.0 if mouth_height < 0.15 else 0.0
                elif size_type == 'large':
                    return 100.0 if mouth_height > 0.25 else 0.0
                
        return 0.0

    def _detect_bite_angle(self, faces: List[dict], angle_type: str) -> float:
        """
        Detect bite angle based on face pose
        Returns confidence score (0-100)
        """
        if not faces:
            return 0.0
            
        for face in faces:
            pose = face.get('Pose', {})
            yaw = pose.get('Yaw', 0)
            
            if angle_type == 'front':
                return 100.0 if abs(yaw) < 15 else 0.0
            elif angle_type == 'side':
                return 100.0 if abs(yaw) > 45 else 0.0
            elif angle_type == '45_degree':
                return 100.0 if 30 < abs(yaw) < 60 else 0.0
                
        return 0.0

    def _analyze_pre_bite_behavior(self, data: dict) -> dict:
        """Analyze behavior just before taking a bite"""
        features = {}
        
        # Check for food examination
        labels = data.get('Labels', [])
        faces = data.get('FaceDetails', [])
        
        food_focus = any(
            label['Name'].lower() in ['food', 'meal', 'dish'] 
            and label['Confidence'] > self.confidence_threshold
            for label in labels
        )
        
        close_up_shot = self._detect_close_up_composition(data)
        
        features['food_examination'] = 100.0 if food_focus and close_up_shot else 0.0
        
        return features

    def _analyze_visual_elements(self, data: dict) -> dict:
        """
        Analyze visual presentation elements from Rekognition data
        """
        visual_features = defaultdict(float)
        
        # Analyze lighting
        visual_features.update(self._analyze_lighting_conditions(data))
        
        # Analyze color and composition
        visual_features.update(self._analyze_color_composition(data))
        
        # Analyze steam and movement
        visual_features.update(self._analyze_dynamic_elements(data))
        
        return dict(visual_features)

    def _analyze_lighting_conditions(self, data: dict) -> dict:
        """Analyze lighting conditions in the scene"""
        features = {}
        
        # Check brightness levels
        brightness = self._calculate_brightness(data)
        features.update({
            'natural_light': self._detect_natural_light(data),
            'artificial_light': self._detect_artificial_light(data),
            'optimal_food_lighting': self._evaluate_food_lighting(data),
            'mood_lighting': self._detect_mood_lighting(data)
        })
        
        return features

class EngagementAnalyzer:
    def __init__(self):
        self.feature_importance = {}
        self.pattern_detection = {}

    def analyze_feature_engagement_correlation(self, 
                                            features_df: pd.DataFrame, 
                                            engagement_df: pd.DataFrame) -> Dict[str, Any]:
        """
        Analyze correlation between features and engagement metrics
        """
        correlations = {}
        
        for metric in ['likes', 'comments', 'saves', 'shares', 'watch_time']:
            if metric in engagement_df.columns:
                feature_correlations = {}
                
                for feature in features_df.columns:
                    correlation = self._calculate_correlation(
                        features_df[feature],
                        engagement_df[metric]
                    )
                    feature_correlations[feature] = correlation
                
                correlations[metric] = feature_correlations
                
        return correlations

    def identify_high_performing_combinations(self, 
                                           features_df: pd.DataFrame, 
                                           engagement_df: pd.DataFrame,
                                           threshold: float = 0.75) -> List[Dict[str, Any]]:
        """
        Identify feature combinations that lead to high engagement
        """
        high_performing_patterns = []
        
        # Analyze feature combinations
        for metric in ['likes', 'comments', 'saves', 'shares', 'watch_time']:
            if metric in engagement_df.columns:
                patterns = self._find_feature_patterns(
                    features_df,
                    engagement_df[metric],
                    threshold
                )
                high_performing_patterns.extend(patterns)
                
        return high_performing_patterns

class DetailedFeatureExtractor:
    def __init__(self):
        self.confidence_threshold = 80
        self.temporal_window = 30  # frames for temporal analysis
        self.face_expression_cache = {}  # for tracking expression changes

    def extract_all_features(self, rekognition_data: dict) -> Dict[str, Any]:
        """Master function to extract all features"""
        all_features = {
            'human_presentation': self.extract_human_presentation_features(rekognition_data),
            'location_setting': self.extract_location_setting_features(rekognition_data),
            'content_action': self.extract_content_action_features(rekognition_data),
            'content_structure': self.extract_content_structure_features(rekognition_data),
            'food_content': self.extract_food_content_features(rekognition_data),
            'lifestyle_content': self.extract_lifestyle_content_features(rekognition_data),
            'restaurant_showcase': self.extract_restaurant_showcase_features(rekognition_data)
        }
        return all_features

    def extract_human_presentation_features(self, data: dict) -> Dict[str, Any]:
        """Extract all human presentation features"""
        face_features = self.extract_face_expression_features(data)
        body_features = self.extract_body_language_features(data)
        appearance_features = self.extract_appearance_features(data)
        branding_features = self.extract_personal_branding_features(data)
        authenticity_features = self.extract_authenticity_features(data)
        
        return {
            'face_expression': face_features,
            'body_language': body_features,
            'appearance': appearance_features,
            'personal_branding': branding_features,
            'authenticity': authenticity_features
        }

    def extract_face_expression_features(self, data: dict) -> Dict[str, float]:
        """Extract detailed facial expression features"""
        features = {}
        faces = data.get('FaceDetails', [])
        
        for face in faces:
            # Basic expressions
            features.update(self._analyze_smile(face))
            features.update(self._analyze_laugh_indicators(face))
            features.update(self._analyze_serious_expression(face))
            features.update(self._analyze_surprise(face))
            
            # Eye analysis
            features.update(self._analyze_eye_contact(face))
            features.update(self._analyze_eye_squint(face))
            
            # Head positioning
            features.update(self._analyze_head_angle(face))
            
            # Expression changes
            features.update(self._analyze_expression_changes(face))
            
            # Micro-expressions
            features.update(self._analyze_micro_expressions(face))

        return features

    def _analyze_smile(self, face: dict) -> Dict[str, float]:
        """Detailed smile analysis"""
        smile_features = {}
        
        if 'Smile' in face:
            smile = face['Smile']
            confidence = smile.get('Confidence', 0)
            is_smiling = smile.get('Value', False)
            
            if is_smiling and confidence > self.confidence_threshold:
                # Analyze smile intensity
                smile_features['smile_intensity'] = self._calculate_smile_intensity(face)
                
                # Analyze smile authenticity
                smile_features['smile_genuineness'] = self._calculate_smile_authenticity(face)
                
                # Analyze teeth visibility
                smile_features['teeth_visibility'] = self._analyze_teeth_visibility(face)
                
                # Analyze smile symmetry
                smile_features['smile_symmetry'] = self._calculate_smile_symmetry(face)

        return smile_features

    def _analyze_expression_changes(self, face: dict) -> Dict[str, float]:
        """Analyze expression changes over time"""
        features = {}
        face_id = face.get('FaceId', '')
        
        if face_id in self.face_expression_cache:
            prev_expression = self.face_expression_cache[face_id]
            
            # Calculate expression change rate
            features['expression_change_rate'] = self._calculate_expression_change_rate(
                prev_expression,
                face
            )
            
            # Analyze transition smoothness
            features['expression_transition_smoothness'] = self._calculate_transition_smoothness(
                prev_expression,
                face
            )
            
            # Track emotional progression
            features['emotional_progression'] = self._analyze_emotional_progression(
                prev_expression,
                face
            )

        self.face_expression_cache[face_id] = face
        return features

    def extract_body_language_features(self, data: dict) -> Dict[str, float]:
        """Extract detailed body language features"""
        features = {}
        
        # Hand gesture analysis
        features.update(self._analyze_hand_gestures(data))
        
        # Body position analysis
        features.update(self._analyze_body_position(data))
        
        # Movement pattern analysis
        features.update(self._analyze_movement_patterns(data))
        
        # Group formation analysis
        features.update(self._analyze_group_formation(data))
        
        # Personal space analysis
        features.update(self._analyze_personal_space(data))
        
        return features

    def _analyze_hand_gestures(self, data: dict) -> Dict[str, float]:
        """Detailed hand gesture analysis"""
        features = {}
        
        # Get pose landmarks
        pose_data = data.get('PoseDetails', {})
        landmarks = pose_data.get('Landmarks', [])
        
        # Analyze hand positions and movements
        features.update(self._detect_hand_position_type(landmarks))
        features.update(self._analyze_hand_movement_patterns(landmarks))
        features.update(self._detect_specific_gestures(landmarks))
        
        # Analyze gesture characteristics
        features.update({
            'gesture_speed': self._calculate_gesture_speed(landmarks),
            'gesture_amplitude': self._calculate_gesture_amplitude(landmarks),
            'gesture_precision': self._calculate_gesture_precision(landmarks),
            'gesture_fluidity': self._calculate_gesture_fluidity(landmarks),
            'gesture_confidence': self._calculate_gesture_confidence(landmarks)
        })
        
        return features

    def extract_appearance_features(self, data: dict) -> Dict[str, Dict[str, float]]:
        """Extract detailed appearance features"""
        return {
            'outfit_analysis': self._analyze_outfit(data),
            'accessory_details': self._analyze_accessories(data),
            'hair_styling': self._analyze_hair(data),
            'makeup_analysis': self._analyze_makeup(data)
        }

    def _analyze_outfit(self, data: dict) -> Dict[str, float]:
        """Detailed outfit analysis"""
        outfit_features = {}
        
        # Color analysis
        outfit_features.update(self._analyze_outfit_colors(data))
        
        # Pattern analysis
        outfit_features.update(self._analyze_outfit_patterns(data))
        
        # Style categorization
        outfit_features.update(self._categorize_outfit_style(data))
        
        # Fit analysis
        outfit_features.update(self._analyze_outfit_fit(data))
        
        return outfit_features

    def extract_location_setting_features(self, data: dict) -> Dict[str, Any]:
        """Extract all location and setting features"""
        return {
            'environment': self._analyze_environment(data),
            'background': self._analyze_background(data),
            'lighting': self._analyze_lighting(data),
            'time_of_day': self._analyze_time_indicators(data)
        }

    def _analyze_environment(self, data: dict) -> Dict[str, float]:
        """Detailed environment analysis"""
        env_features = {}
        labels = data.get('Labels', [])
        
        # Detect specific environments
        environments = {
            'kitchen': ['kitchen', 'cooking', 'appliance'],
            'restaurant': ['restaurant', 'dining', 'cafe'],
            'outdoor': ['outdoor', 'nature', 'street'],
            'gym': ['gym', 'fitness', 'exercise'],
            'office': ['office', 'workplace', 'desk'],
            'home': ['home', 'living room', 'bedroom']
        }
        
        for env_type, keywords in environments.items():
            confidence = self._calculate_environment_confidence(labels, keywords)
            env_features[f"{env_type}_confidence"] = confidence
            
            if confidence > self.confidence_threshold:
                env_features.update(self._analyze_specific_environment(data, env_type))
        
        return env_features

    def _analyze_lighting(self, data: dict) -> Dict[str, float]:
        """Detailed lighting analysis"""
        lighting_features = {}
        
        # Natural light detection
        natural_light = self._detect_natural_light(data)
        if natural_light > self.confidence_threshold:
            lighting_features.update({
                'natural_light_intensity': self._calculate_light_intensity(data),
                'natural_light_direction': self._detect_light_direction(data),
                'natural_light_quality': self._analyze_light_quality(data),
                'shadow_patterns': self._analyze_shadow_patterns(data),
                'window_light_interaction': self._analyze_window_light(data)
            })
        
        # Artificial light analysis
        artificial_light = self._detect_artificial_light(data)
        if artificial_light > self.confidence_threshold:
            lighting_features.update({
                'artificial_light_type': self._categorize_light_source(data),
                'light_temperature': self._analyze_light_temperature(data),
                'light_distribution': self._analyze_light_distribution(data),
                'lighting_evenness': self._calculate_lighting_evenness(data),
                'light_fixture_visibility': self._detect_light_fixtures(data)
            })
        
        return lighting_features

    def extract_content_action_features(self, data: dict) -> Dict[str, Any]:
        """Extract all content action features"""
        return {
            'demonstration': self._analyze_demonstration_features(data),
            'lifestyle': self._analyze_lifestyle_features(data),
            'entertainment': self._analyze_entertainment_features(data),
            'educational': self._analyze_educational_features(data),
            'product_interaction': self._analyze_product_interaction(data)
        }

def _analyze_demonstration_features(self, data: dict) -> Dict[str, Any]:
    """Analyze demonstration-specific features"""
    demo_features = {}
    
    # Step-by-step analysis
    demo_features['step_sequence'] = {
        'step_clarity': self._analyze_step_clarity(data),
        'step_ordering': self._analyze_step_ordering(data),
        'transition_quality': self._analyze_step_transitions(data),
        'instruction_clarity': self._analyze_instruction_clarity(data),
        'visual_guidance': {
            'hand_position_clarity': self._analyze_hand_visibility(data),
            'focus_point_clarity': self._analyze_focus_points(data),
            'demonstration_angle': self._analyze_demonstration_angle(data),
            'object_visibility': self._analyze_object_visibility(data),
            'lighting_effectiveness': self._analyze_demonstration_lighting(data)
        }
    }
    
    # Before/After elements
    demo_features['transformation'] = {
        'before_state': self._analyze_before_state(data),
        'after_state': self._analyze_after_state(data),
        'progress_visibility': self._analyze_progress_indicators(data),
        'comparison_clarity': self._analyze_comparison_method(data)
    }
    
    return demo_features


    def extract_food_content_features(self, data: dict) -> Dict[str, Any]:
        """Extract all food-related features"""
        return {
            'food_preparation': self._analyze_food_preparation(data),
            'plating': self._analyze_plating(data),
            'food_interaction': self._analyze_food_interaction(data),
            'dining_environment': self._analyze_dining_environmenment(data),
            'cuisine_specific': self._analyze_cuisine_specific(data)
        }

    def _analyze_food_preparation(self, data: dict) -> Dict[str, Any]:
        """Detailed analysis of food preparation"""
        prep_features = {}
        
        # Technique analysis
        prep_features['cooking_techniques'] = {
            'cutting': self._analyze_cutting_technique(data),
            'heating': self._analyze_heating_method(data),
            'mixing': self._analyze_mixing_technique(data),
            'seasoning': self._analyze_seasoning_application(data),
            'tool_usage': {
                'knife_technique': self._analyze_knife_handling(data),
                'utensil_usage': self._analyze_utensil_usage(data),
                'appliance_operation': self._analyze_appliance_usage(data),
                'tool_appropriateness': self._analyze_tool_selection(data)
            }
        }
        
        # Ingredient handling
        prep_features['ingredient_handling'] = {
            'ingredient_prep': self._analyze_ingredient_preparation(data),
            'measurement_precision': self._analyze_measurement_accuracy(data),
            'freshness_indicators': self._analyze_ingredient_freshness(data),
            'storage_methods': self._analyze_ingredient_storage(data)
        }
        
        return prep_features

    def _analyze_plating(self, data: dict) -> Dict[str, Any]:
        """Detailed plating analysis"""
        plating_features = {
            'composition': {
                'height_variation': self._analyze_plate_height(data),
                'color_balance': self._analyze_color_composition(data),
                'texture_contrast': self._analyze_texture_variety(data),
                'portion_sizing': self._analyze_portion_size(data),
                'negative_space': self._analyze_plate_spacing(data)
            },
            'techniques': {
                'sauce_application': self._analyze_sauce_techniques(data),
                'garnish_placement': self._analyze_garnish_positioning(data),
                'component_arrangement': self._analyze_component_layout(data),
                'stacking_method': self._analyze_stacking_technique(data),
                'temperature_maintenance': self._analyze_temperature_indicators(data)
            }
        }
        return plating_features

    def extract_content_structure_features(self, data: dict) -> Dict[str, Any]:
        """Extract content structure features"""
        return {
            'pacing': self._analyze_pacing(data),
            'shot_composition': self._analyze_shot_composition(data),
            'transitions': self._analyze_transitions(data),
            'narrative_elements': self._analyze_narrative_structure(data),
            'audio_elements': self._analyze_audio_components(data)
        }

    def _analyze_shot_composition(self, data: dict) -> Dict[str, Any]:
        """Analyze shot composition details"""
        composition_features = {
            'framing': {
                'rule_of_thirds': self._analyze_thirds_alignment(data),
                'subject_placement': self._analyze_subject_position(data),
                'frame_balance': self._analyze_visual_balance(data),
                'depth_perception': self._analyze_depth_cues(data),
                'leading_lines': self._analyze_leading_lines(data)
            },
            'camera_work': {
                'angle_choice': self._analyze_camera_angle(data),
                'movement_stability': self._analyze_camera_stability(data),
                'focus_quality': self._analyze_focus_characteristics(data),
                'zoom_level': self._analyze_zoom_usage(data),
                'motion_tracking': self._analyze_subject_tracking(data)
            }
        }
        return composition_features

    def extract_lifestyle_content_features(self, data: dict) -> Dict[str, Any]:
        """Extract lifestyle content features"""
        return {
            'daily_routine': self._analyze_daily_routine(data),
            'home_organization': self._analyze_organization(data),
            'wellness': self._analyze_wellness_activities(data),
            'productivity': self._analyze_productivity_setup(data),
            'social_interaction': self._analyze_social_elements(data)
        }

    def _analyze_daily_routine(self, data: dict) -> Dict[str, Any]:
        """Analyze daily routine elements"""
        routine_features = {
            'morning_sequence': {
                'wake_up_indicators': self._analyze_wake_up_scene(data),
                'grooming_routine': self._analyze_grooming_activities(data),
                'breakfast_preparation': self._analyze_breakfast_scene(data),
                'outfit_selection': self._analyze_clothing_choice(data),
                'time_management': self._analyze_time_indicators(data)
            },
            'productivity_period': {
                'workspace_setup': self._analyze_workspace(data),
                'task_focus': self._analyze_task_engagement(data),
                'break_patterns': self._analyze_break_activities(data),
                'environment_optimization': self._analyze_work_environment(data)
            }
        }
        return routine_features
      def extract_restaurant_showcase_features(self, data: dict) -> Dict[str, Any]:
        """Extract restaurant showcase features"""
        return {
            'ambiance': self._analyze_restaurant_ambiance(data),
            'menu_presentation': self._analyze_menu_presentation(data),
            'dish_showcase': self._analyze_dish_presentation(data),
            'staff_interaction': self._analyze_staff_interactionion(data),
            'dining_experience': self._analyze_dining_experience(data)
        }

    def _analyze_restaurant_ambiance(self, data: dict) -> Dict[str, Any]:
        """Analyze restaurant ambiance details"""
        ambiance_features = {
               'interior_design': {
                'style_coherence': self._analyze_design_consistency(data),
                'lighting_atmosphere': self._analyze_ambient_lighting(data),
                'seating_arrangement': self._analyze_seating_layout(data),
                'decor_elements': self._analyze_decorative_features(data),
                'color_scheme': self._analyze_color_palette(data)
            },
            'atmosphere': {
                'noise_level': self._estimate_noise_level(data),
                'crowd_density': self._analyze_crowd_presence(data),
                'privacy_level': self._assess_privacy_options(data),
                'comfort_factors': self._analyze_comfort_elements(data)
            },
            'unique_features': {
                'signature_elements': self._identify_unique_features(data),
                'thematic_consistency': self._analyze_theme_adherence(data),
                'brand_representation': self._assess_brand_visibility(data)
            }
        }
        return ambiance_features

    def extract_product_review_features(self, data: dict) -> Dict[str, Any]:
        """Extract product review and unboxing features"""
        return {
            'unboxing_sequence': self._analyze_unboxing_proceocess(data),
            'product_showcase': self._analyze_product_display(data),
            'feature_demonstration': self._analyze_feature_demo(data),
            'comparison_elements': self._analyze_product_comparison(data),
            'reviewer_interaction': self._analyze_reviewer_engagement(data)
        }

def _analyze_unboxing_process(self, data: dict) -> Dict[str, Any]:
    """Analyze unboxing sequence details"""
    unboxing_features = {
        'packaging_analysis': {
            'box_condition': self._assess_package_condition(data),
            'branding_visibility': self._analyze_package_branding(data),
            'protective_elements': self._identify_protective_packaging(data),
            'unboxing_ease': self._assess_unboxing_difficulty(data)
        },
        'reveal_sequence': {
            'anticipation_building': self._analyze_reveal_pacing(data),
            'first_impression_capture': self._analyze_initial_reaction(data),
            'component_organization': self._assess_component_layout(data),
            'accessory_inclusion': self._identify_included_accessories(data)
        },
        'presentation_style': {
            'camera_angles': self._analyze_unboxing_angles(data),
            'lighting_quality': self._assess_unboxing_lighting(data),
            'narration_engagement': self._analyze_verbal_description(data),
            'detail_focus': self._assess_detail_emphasis(data)
        }
    }
    return unboxing_features




    def extract_tutorial_features(self, data: dict) -> Dict[str, Any]:
        """Extract tutorial and how-to content features"""
        return {
            'instruction_clarity': self._analyze_instruction_delivery(data),
            'visual_aids': self._analyze_visual_su_support(data),
            'difficulty_progression': self._analyze_complexity_management(data),
            'viewer_engagement': self._analyze_engagement_techniques(data),
            'outcome_demonstration': self._analyze_result_showcase(data)
        }

    def _analyze_instruction_delivery(self, data: dict) -> -> Dict[str, Any]:
        """Analyze instruction delivery in tutorials"""
        instruction_features = {
            'verbal_clarity': {
                'speech_pace': self._analyze_speech_rate(data),
                'articulation_quality': self._assess_pronunciation_clarity(data),
                'vocabulary_appropriateness': self._analyze_terminology_usage(data),
                'explanation_coherence': self._assess_logical_flow(data)
            },
            'step_sequencing': {
                'step_delineation': self._analyze_step_separation(data),
                'progression_logic': self._assess_sequence_logic(data),
                'time_allocation': self._analyze_step_duration(data),
                'transition_smoothness': self._assess_inter_step_transitions(data)
            },
            'demonstration_technique': {
                'hand_placement': self._analyze_hand_visibility(data),
                'tool_handling': self._assess_tool_usage_clarity(data),
                'action_repetition': self._analyze_technique_repetition(data),
                'perspective_changes': self._assess_viewing_angle_variations(data)
            }
        }
        return instruction_features

    def extract_travel_content_features(self, data: dict) -> Dict[str, Any]:
        """Extract travel content features"""
        return {
            'destination_showcase': self._analyze_location_presentation(data),
            'cultural_elements': self._analyze_cultural_representation(data),
            'activity_highlights': self._analyze_travel_activities(data),
            'accommodation_features': self._analyze_lodging_presentation(data),
            'transportation_aspects': self._analyze_travel_methods(data)
        }

    def _analyze_location_presentation(self, data: dict) -> Dict[str, Any]:
        """Analyze how travel destinations are presented"""
        location_features = {
            'scenery_capture': {
                'landscape_variety': self._assess_landscape_diversity(data),
                'natural_features': self._identify_natural_landmarks(data),
                'urban_elements': self._analyze_urban_landscape(data),
                'weather_conditions': self._assess_weather_representation(data)
            },
            'point_of_interest_focus': {
                'landmark_prominence': self._analyze_landmark_emphasis(data),
                'historical_site_presentation': self._assess_historical_context(data),
                'local_attraction_coverage': self._analyze_attraction_showcase(data)
            },
            'atmosphere_conveyance': {
                'time_of_day_representation': self._analyze_lighting_cues(data),
                'crowd_presence': self._assess_location_busyness(data),
                'local_life_depiction': self._analyze_daily_life_elements(data),
                'mood_setting': self._assess_emotional_tone(data)
            }
        }
        return location_features

    def extract_fitness_content_features(self, data: dict) -> Dict[str, Any]:
        """Extract fitness and workout content features"""
        return {
            'exercise_demonstration': self._analyze_exercise_execution(data),
            'workout_environment': self._analyze_fitness_setting(data),
            'equipment_usage': self._analyze_fitness_equipment(data),
            'form_guidance': self._analyze_form_instruction(data),
            'intensity_indicators': self._analyze_workout_intensity(data)
        }

    def _analyze_exercise_execution(self, data: dict) -> Dict[str, Any]:
        """Analyze exercise demonstration details"""
        exercise_features = {
            'movement_quality': {
                'range_of_motion': self._assess_movement_range(data),
                'form_correctness': self._analyze_proper_form(data),
                'movement_fluidity': self._assess_motion_smoothness(data),
                'tempo_control': self._analyze_exercise_pacing(data)
            },
            'body_positioning': {
                'alignment': self._analyze_body_alignment(data),
                'posture': self._assess_postural_elements(data),
                'balance': self._analyze_stability_cues(data),
                'muscle_engagement': self._assess_visible_muscle_activation(data)
            },
            'demonstration_clarity': {
                'starting_position': self._analyze_exercise_setup(data),
                'key_phase_emphasis': self._assess_critical_point_focus(data),
                'repetition_consistency': self._analyze_rep_uniformity(data),
                'modification_options': self._identify_exercise_variations(data)
            }
        }
        return exercise_features

    def extract_beauty_content_features(self, data: dict) -> Dict[str, Any]:
        """Extract beauty and makeup content features"""
        return {
            'product_application': self._analyze_makeup_application(data),
            'technique_demonstration': self._analyze_beauty_techniques(data),
            'before_after_comparison': self._analyze_transformation(data),
            'skincare_routines': self._analyze_skincare_steps(data),
            'hair_styling': self._analyze_hair_techniques(data)
        }

    def _analyze_makeup_application(self, data: dict) -> Dict[str, Any]:
        """Analyze makeup application details"""
        makeup_features = {
            'product_usage': {
                'tool_selection': self._analyze_makeup_tools(data),
                'product_layering': self._assess_product_order(data),
                'color_selection': self._analyze_shade_choices(data),
                'product_texture': self._assess_makeup_consistency(data)
            },
            'application_technique': {
                'blending_quality': self._analyze_blending_skill(data),
                'precision': self._assess_application_accuracy(data),
                'coverage_level': self._analyze_makeup_opacity(data),
                'symmetry': self._assess_facial_symmetry(data)
            },
            'look_characteristics': {
                'style_categorization': self._categorize_makeup_style(data),
                'intensity_level': self._assess_makeup_boldness(data),
                'color_harmony': self._analyze_color_coordination(data),
                'finish_type': self._assess_makeup_finish(data)
            }
        }
        return makeup_features

    # ... Continue with more specific analyzers for each content type ...

 def extract_technical_video_features(self, data: dict) -> Dict[str, Any]:
        """Extract detailed technical aspects of video content"""
        return {
            'video_quality': self._analyze_video_quality(data),
            'frame_characteristics': self._analyze_frame_characteristics(data),
            'color_processing': self._analyze_color_processing(data),
            'motion_elements': self._analyze_motion_characteristics(data),
            'post_processing': self._analyze_post_processing(data)
        }

    def _analyze_video_quality(self, data: dict) -> Dict[str, Any]:
        """Analyze technical video quality aspects"""
        quality_features = {
            'resolution_characteristics': {
                'pixel_density': self._calculate_pixel_density(data),
                'detail_preservation': self._assess_detail_retention(data),
                'edge_sharpness': self._analyze_edge_definition(data),
                'texture_clarity': self._assess_texture_resolution(data),
                'fine_detail_rendering': self._analyze_fine_details(data)
            },
            'compression_artifacts': {
                'blockiness_detection': self._detect_compression_blocks(data),
                'color_banding': self._analyze_color_gradients(data),
                'mosquito_noise': self._detect_compression_noise(data),
                'artifact_severity': self._assess_artifact_impact(data)
            },
               'noise_characteristics': {
                'luminance_noise': self._analyze_brightness_noise(data),
                'chrominance_noise': self._analyze_color_noise(data),
                'noise_pattern': self._assess_noise_distribution(data),
                'grain_characteristics': self._analyze_grain_structure(data)
            }
        }
        return quality_features

    def _analyze_frame_characteristics(self, data: dict) -> Dict[str, Any]:
        """Analyze individvidual frame characteristics"""
        frame_features = {
            'frame_rate_analysis': {
                'motion_smoothness': self._analyze_motion_fluidity(data),
                'frame_timing': self._assess_frame_intervals(data),
                'motion_blur': self._analyze_motion_blur(data),
                'frame_interpolation': self._detect_frame_interpolation(data),
                'temporal_consististency': self._assess_temporal_stability(data)
            },
            'frame_stability': {
                'camera_shake': self._analyze_camera_stability(data),
                'rolling_shutter': self._detect_rolling_shutter(data),
                'stabilization_quality': self._assess_stabilization(data),
                'horizon_alignment': self._analyze_horizon_level(data)
            }
        }
        return frame_features

    def extract_content_structure_detailed(self, data: dict) -> Dict[str, Any]:
        """Extract detailed content structure features"""
        return {
            'pacing_analysis': self._analyze_detailed_pacing(data),
            'audio_structure': self._analyze_audio_structure(data),
            'transition_analysis': self._analyze_transitions_detailed(data),
            'narrative_structure': self._analyze_narrative_elements(data),
            'engagement_mechanics': self._analyze_engagement_structure(data)
        }

    def _analyze_detailed_pacing(self, data: dict) -> Dict[str, Any]:
        """Analyze content pacing in detail"""
        pacing_features = {
            'scene_timing': {
                'shot_duration': {
                    'opening_shot_length': self._measure_opening_duration(data),
                    'content_shot_lengths': self._analyze_shot_durations(data),
                    'closing_shot_length': self._measure_closing_duration(data),
                    'shot_length_variation': self._analyze_duratration_patterns(data),
                    'pacing_rhythm': self._detect_timing_patterns(data)
                },
                'content_density': {
                    'information_rate': self._analyze_information_density(data),
                    'action_density': self._measure_action_frequency(data),
                    'dialogue_pacing': self._analyze_speech_timing(data),
                    'visual_complexity_time': self._analyze_visual_density(data)
                }
            },
            'energy_flow': {
                'intensity_mapping': {
                    'high_energy_segments': self._identify_high_energy_periods(data),
                    'low_energy_segments': self._identify_low_energy_periods(data),
                    'energy_transitions': self._analyze_energy_shifts(data),
                    'peak_moments': self._identify_climax_points(data)
                },
                'attention_management': {
                    'attention_hooks': self._analyze_attention_grabbers(data),
                    'retention_elements': self._identify_retention_tactics(data),
                    'engagement_cycles': self._analyze_engagement_patterns(data),
                    'viewer_recovery_points': self._identify_rest_periods(data)
                }
            }
        }
        return pacing_features

    def _analyze_audio_structure(self, data: dict) -> Dict[str, Any]:
        """Analyze detailed audio structure"""
        audio_features = {
            'voice_characteristics': {
                'speech_patterns': {
                    'speaking_pace': self._analyze_speech_rate(data),
                    'vocal_variety': self._analyze_pitch_variation(data),
                    'emphasis_points': self._detect_vocal_emphasis(data),
                    'speech_clarity': self._assess_articulation(data),
                    'tone_modulation': self._analyze_tone_changes(data)
                },
                'vocal_quality': {
                    'voice_texture': self._analyze_voice_characteristics(data),
                    'emotional_conveyance': self._assess_emotional_tone(data),
                    'authenticity_markers': self._detect_natural_speech(data),
                    'energy_level': self._analyze_vocal_energy(data)
                }
            },
            'music_elements': {
                'background_music': {
                    'genre_selection': self._identify_music_genre(data),
                    'volume_balance': self._analyze_music_levels(data),
                    'mood_matching': self._assess_music_mood(data),
                    'timing_alignment': self._analyze_music_timing(data)
                },
                'sound_design': {
                    'effect_usage': self._analyze_sound_effects(data),
                    'ambient_sound': self._analyze_background_audio(data),
                    'audio_transitions': self._analyze_audio_transitions(data),
                    'spatial_audio': self._analyze_audio_positioning(data)
                }
            }
        }
        return audio_features

    def extract_cuisine_specific_detailed(self, data: dict) -> Dict[str, Any]:
        """Extract detailed cuisine-specific features"""
        return {
            'cuisine_identification': self._analyze_cuisine_type(data),
            'authenticity_markers': self._analyze_authenticity(data),
            'preparation_techniques': self._analyze_cooking_methods(data),
            'cultural_elements': self._analyze_cultural_context(data),
            'regional_variations': self._analyze_regional_specifics(data)
        }

    def _analyze_cuisine_type(self, data: dict) -> Dict[str, Any]:
        """Analyze specific cuisine characteristics"""
        cuisine_features = {
            'ingredient_analysis': {
                'key_ingredients': self._identify_signature_ingredients(data),
                'ingredient_combinations': self._analyze_ingredient_pairings(data),
                'ingredient_preparation': self._analyze_prep_methods(data),
                'ingredient_authenticity': self._assess_ingredient_genuineness(data)
            },
            'cooking_techniques': {
                'traditional_methods': self._identify_traditional_techniques(data),
                'modern_adaptations': self._analyze_technique_modifications(data),
                'equipment_usage': self._analyze_cooking_equipment(data),
                'timing_patterns': self._analyze_cooking_timing(data)
            },
            'presentation_style': {
                'plating_traditions': self._analyze_traditional_plating(data),
                'garnish_choices': self._analyze_cultural_garnishes(data),
                'serving_vessels': self._analyze_traditional_vessels(data),
                'color_patterns': self._analyze_cultural_color_schemes(data)
            }
        }
        return cuisine_features

    def extract_social_interaction_detailed(self, data: dict) -> Dict[str, Any]:
        """Extract detailed social interaction features"""
        return {
            'group_dynamics': self._analyze_group_behavior(data),
            'interpersonal_communication': self._analyze_communication(data),
            'collaborative_elements': self._analyze_collaboration(data),
            'audience_interaction': self._analyze_audience_engagement(data),
            'social_cues': self._analyze_social_signals(data)
        }

    def _analyze_group_behavior(self, data: dict) -> Dict[str, Any]:
        """Analyze detailed group behavior patterns"""
        group_features = {
            'spatial_relationships': {
                'positioning': {
                    'group_formation': self._analyze_group_formation(data),
                    'interpersonal_distance': self._analyze_spacing(data),
                    'hierarchy_indicators': self._analyze_spatial_hierarchy(data),
                    'movement_patterns': self._analyze_group_movement(data)
                },
                'interaction_zones': {
                    'conversation_clusters': self._identify_conversation_groups(data),
                    'activity_spaces': self._analyze_activity_areas(data),
                    'social_boundaries': self._analyze_personal_space(data)
                }
            },
            'behavioral_synchronization': {
                'movement_coordination': self._analyze_coordinated_movement(data),
                'emotional_alignment': self._analyze_emotional_synchrony(data),
                'attention_focus': self._analyze_group_attention(data),
                'response_patterns': self._analyze_group_responses(data)
            }
        }
        return group_features
import boto3
from typing import Dict, Any, List

class HumanPresentationDetector:
    def __init__(self):
        self.rekognition_client = boto3.client('rekognition')
        self.confidence_threshold = 70.0
        self.frame_history = {}  # For tracking changes across frames

    def analyze_face_expression(self, video_path: str) -> Dict[str, Any]:
        """Analyze all facial expression features"""
        try:
            with open(video_path, 'rb') as video:
                response = self.rekognition_client.start_face_detection(
                    Video={'Bytes': video.read()},
                    FaceAttributes=['ALL']
                )
            
            job_id = response['JobId']
            result = self.rekognition_client.get_face_detection(JobId=job_id)
            
            return {
                'smile_characteristics': self._analyze_smile_characteristics(result),
                'laugh_characteristics': self._analyze_laugh_characteristics(result),
                'serious_expression': self._analyze_serious_expression(result),
                'surprise_elements': self._analyze_surprise_elements(result),
                'eye_contact': self._analyze_eye_contact(result),
                'head_angle': self._analyze_head_angle(result),
                'micro_expressions': self._analyze_micro_expressions(result),
                'expression_transitions': self._analyze_expression_transitions(result)
            }
        except Exception as e:
            print(f"Error in face expression analysis: {str(e)}")
            return {}

    def _analyze_smile_characteristics(self, face_detection_result: Dict) -> Dict[str, float]:
        """Analyze detailed smile characteristics"""
        smile_features = {
            'smile_intensity': 0.0,
            'smile_duration': 0.0,
            'smile_genuineness': 0.0,
            'teeth_visibility': 0.0,
            'smile_symmetry': 0.0
        }

        faces_by_frame = face_detection_result['Faces']
        total_frames = len(faces_by_frame)
        smile_frames = 0
        
        for frame in faces_by_frame:
            for face in frame['Face']:
                # Analyze smile intensity
                if face.get('Smile', {}).get('Value'):
                    confidence = face['Smile'].get('Confidence', 0)
                    smile_features['smile_intensity'] = max(
                        smile_features['smile_intensity'],
                        confidence
                    )
                    smile_frames += 1

                # Analyze teeth visibility
                mouth_landmarks = [
                    point for point in face.get('Landmarks', [])
                    if 'Mouth' in point['Type']
                ]
                if mouth_landmarks:
                    smile_features['teeth_visibiliility'] = self._calculate_teeth_visibility(
                        face, mouth_landmarks
                    )

                # Analyze smile symmetry
                if mouth_landmarks:
                    smile_features['smile_symmetry'] = self._calculate_smile_symmetry(
                        mouth_landmarks
                    )

        # Calculate smile duration as percentage of total frames
        smile_features['smile_duration'] = (smile_frames / total_frames) * 100 if total_frames > 0 else 0
        
        # Calculate smile genuineness based on multiple factors
        smile_features['smile_genuineness'] = self._calculate_smile_genuineness(
            smile_features['smile_intensity'],
            smile_features['smile_symmetry'],
            face_detection_result
        )

        return smile_features

    def _calculate_te_teeth_visibility(self, face: Dict, mouth_landmarks: List) -> float:
        """Calculate teeth visibility score"""
        try:
            # Calculate mouth opening
            upper_lip = next(point for point in mouth_landmarks if point['Type'] == 'upperLip')
            lower_lip = next(point for point in mouth_landmarks if point['Type'] == 'lowerLip')
            
            mouth_height = abs(upper_lip['Y'] - lower_lip['Y'])
            face_height = face.get('BoundingBox', {}).get('Height', 1)
            
            # Normalize mouth height relative to face height
            relative_mouth_height = mouth_height / face_height
            
            # Convert to percentage with threshold
            visibility_score = min(100, (relative_mouth_height * 500))  # Adjust multiplier as needed
            
            return visibility_score
        except Exception:
            return 0.0

    def _calculate_smile_symmetry(self, mouth_landmarks: List) -> float:
        """Calculate smile symmetry score"""
        try:
            # Get left and right mouth corners
            left_corner = next(point for point in mouth_landmarks if point['Type'] == 'mouthLeft')
            right_corner = next(point for point in mouth_landmarks if point['Type'] == 'mouthRight')
            center_upper = next(point for point in mouth_landmarks if point['Type'] == 'upperLip')
            
            # Calculate distances from corners to center
            left_distance = ((left_corner['X'] - center_upper['X'])**2 + 
                           (left_corner['Y'] - center_upper['Y'])**2)**0.5
            right_distance = ((right_corner['X'] - center_upper['X'])**2 + 
                            (right_corner['Y'] - center_upper['Y'])**2)**0.5
            
            # Calculate symmetry score (100 = perfect symmetry)
            if max(left_distance, right_distance) > 0:
                symmetry = 100 * (1 - abs(left_distance - right_distance) / max(left_distance, right_distance))
            else:
                symmetry = 0
                
            return symmetry
        except Exception:
            return 0.0

    def _calculate_smile_genuineness(self, intensity: float, symmetry: float, 
                                   face_detection_result: Dict) -> float:
        """Calculate smile genuineness score"""
        # Factors that contribute to genuine smiles:
        # 1. Intensity
        # 2. Symmetry
        # 3. Eye involvement (Duchenne smile)
        # 4. Natural progression
        
        factors = {
            'intensity': intensity * 0.25,  # 25% weight
            'symmetry': symmetry * 0.25,    # 25% weight
            'eye_involvement': self._analyze_eye_involvement(face_detection_result) * 0.3,  # 30% weight
            'natural_progression': self._analyze_smile_progression(face_detection_result) * 0.2  # 20% weight
        }
        
        return sum(factors.values())

    def _analyze_eye_involvement(self, face_detection_result: Dict) -> float:
        """Analyze eye involvement in smile (Duchenne smile detection)"""
        try:
            max_eye_smile_correlation = 0.0
            
            for frame in face_detection_result['Faces']:
                for face in frame['Face']:
                    if face.get('Smile', {}).get('Value'):
                        # Check for eye crinkling (orbicularis oculi activation)
                        eye_landmarks = [
                            point for point in face.get('Landmarks', [])
                            if 'Eye' in point['Type']
                        ]
                        
                        if eye_landmarks:
                            eye_closure = self._calculate_eye_closure(eye_landmarks)
                            smile_intensity = face['Smile'].get('Confidence', 0)
                            
                            # Calculate correlation between smile and eye closure
                            correlation = self._calculate_eye_smile_correlation(
                                eye_closure, smile_intensity
                            )
                            max_eye_smile_correlation = max(max_eye_smile_correlation, correlation)
            
            return max_eye_smile_correlation
        except Exception:
            return 0.0

def _analyze_laugh_characteristics(self, face_detection_result: Dict) -> Dict[str, float]:
    """Analyze laugh-specific characteristics"""
    laugh_features = {
        'laugh_intensity': 0.0,
        'laugh_duration': 0.0,
        'laugh_sound_level': 0.0,  # Would need audio analysis integration
        'laugh_body_movement': 0.0
    }

    frames = face_detection_result['Faces']
    total_frames = len(frames)
    laugh_frames = 0
    
    for frame in frames:
        for face in frame['Face']:
            # Detect intense smile with open mouth as laugh indicator
            smile_value = face.get('Smile', {}).get('Value', False)
            smile_confidence = face.get('Smile', {}).get('Confidence', 0)
            
            # Get mouth landmarks for openness analysis
            mouth_landmarks = [
                point for point in face.get('Landmarks', [])
                if 'Mouth' in point['Type']
            ]
            
            # Calculate mouth openness
            mouth_openness = self._calculate_mouth_openness(mouth_landmarks)
            
            # Combine smile and mouth openness for laugh detection
            if smile_value and smile_confidence > 90 and mouth_openness > 0.7:
                laugh_frames += 1
                laugh_features['laugh_intensity'] = max(
                    laugh_features['laugh_intensity'],
                    self._calculate_laugh_intensity(face, mouth_openness)
                )
                
                # Analyze body movement during laugh
                body_movement = self._analyze_laugh_body_movement(frame)
                laugh_features['laugh_body_movement'] = max(
                    laugh_features['laugh_body_movement'],
                    body_movement
                )

    # Calculate laugh duration as percentage of total frames
    laugh_features['laugh_duration'] = (laugh_frames / total_frames) * 100 if total_frames > 0 else 0

    return laugh_features

def _analyze_serious_expression(self, face_detection_result: Dict) -> Dict[str, float]:
    """Analyze serious expression characteristics"""
    serious_features = {
        'brow_furrowing': 0.0,
        'lip_tightness': 0.0,
        'jaw_clenching': 0.0,
        'eye_intensity': 0.0
    }
    
    for frame in face_detection_result['Faces']:
        for face in frame['Face']:
            # Analyze brow furrowing
            brow_features = self._analyze_brow_position(face)
            serious_features['brow_furrowing'] = max(
                serious_features['brow_furrowing'],
                brow_features
            )
            
            # Analyze lip compression
            lip_features = self._analyze_lip_compression(face)
            serious_features['lip_tightness'] = max(
                serious_features['lip_tightness'],
                lip_features
            )
            
            # Analyze jaw tension
            jaw_features = self._analyze_jaw_tension(face)
            serious_features['jaw_clenching'] = max(
                serious_features['jaw_clenching'],
                jaw_features
            )
            
            # Analyze eye intensity
            eye_features = self._analyze_eye_intensity(face)
            serious_features['eye_intensity'] = max(
                serious_features['eye_intensity'],
                eye_features
            )

    return serious_features

def _analyze_surprise_elements(self, face_detection_result: Dict) -> Dict[str, float]:
    """Analyze surprise expression characteristics"""
    surprise_features = {
        'eyebrow_raise_speed': 0.0,
        'eye_widening_degree': 0.0,
        'mouth_opening_shape': 0.0,
        'surprise_duration': 0.0
    }
    
    frames = face_detection_result['Faces']
    total_frames = len(frames)
    surprise_frames = 0
    previous_eyebrow_position = None
    
    for frame_idx, frame in enumerate(frames):
        for face in frame['Face']:
            # Get facial landmarks
            landmarks = face.get('Landmarks', [])
            
            # Analyze eyebrow position and movement
            current_eyebrow_position = self._get_eyebrow_position(landmarks)
            if previous_eyebrow_position is not None:
                eyebrow_speed = self._calculate_eyebrow_movement_speed(
                    previous_eyebrow_position,
                    current_eyebrow_position,
                    frame_idx
                )
                surprise_features['eyebrow_raise_speed'] = max(
                    surprise_features['eyebrow_raise_speed'],
                    eyebrow_speed
                )
            previous_eyebrow_position = current_eyebrow_position
            
            # Analyze eye widening
            eye_widening = self._calculate_eye_widening(landmarks)
            surprise_features['eye_widening_degree'] = max(
                surprise_features['eye_widening_degree'],
                eye_widening
            )
            
            # Analyze mouth shape
            mouth_shape = self._analyze_mouth_shape(landmarks)
            surprise_features['mouth_opening_shape'] = max(
                surprise_features['mouth_opening_shape'],
                mouth_shape
            )
            
            # Count frames showing surprise
            if self._is_surprise_expression(eye_widening, mouth_shape, current_eyebrow_position):
                surprise_frames += 1

    # Calculate surprise duration
    surprise_features['surprise_duration'] = (surprise_frames / total_frames) * 100 if total_frames > 0 else 0

    return surprise_features

def _analyze_eye_contact(self, face_detection_result: Dict) -> Dict[str, float]:
    """Analyze eye contact characteristics"""
    eye_contact_features = {
        'direct_eye_contact_duration': 0.0,
        'eye_contact_frequency': 0.0,
        'eye_movement_patterns': 0.0,
        'pupil_dilation': 0.0
    }
    
    frames = face_detection_result['Faces']
    total_frames = len(frames)
    eye_contact_frames = 0
    eye_contact_sequences = []
    current_sequence = 0
    
    for frame in frames:
        for face in frame['Face']:
            # Get eye landmarks and pose
            landmarks = face.get('Landmarks', [])
            pose = face.get('Pose', {})
            
            # Check if eyes are looking at camera
            is_eye_contact = self._check_eye_contact(landmarks, pose)
            
            if is_eye_contact:
                eye_contact_frames += 1
                current_sequence += 1
            elif current_sequence > 0:
                eye_contact_sequences.append(current_sequence)
                current_sequence = 0
            
            # Analyze eye movement patterns
            movement_pattern = self._analyze_eye_movement_pattern(landmarks)
            eye_contact_features['eye_movement_patterns'] = max(
                eye_contact_features['eye_movement_patterns'],
                movement_pattern
            )
            
            # Analyze pupil dilation (if available in high enough resolution)
            pupil_size = self._analyze_pupil_size(landmarks)
            eye_contact_features['pupil_dilation'] = max(
                eye_contact_features['pupil_dilation'],
                pupil_size
            )

    # Calculate eye contact metrics
    if total_frames > 0:
        eye_contact_features['direct_eye_contact_duration'] = (eye_contact_frames / total_frames) * 100
        eye_contact_features['eye_contact_frequency'] = len(eye_contact_sequences)

    return eye_contact_features
class HumanPresentationDetector:
    def __init__(self):
        self.rekognition_client = boto3.client('rekognition')
        self.confidence_threshold = 70.0
        self.frame_history = {}  # For tracking changes across frames

    def analyze_face_expression(self, video_path: str) -> Dict[str, Any]:
        """Analyze all facial expression features"""
        try:
            with open(video_path, 'rb') as video:
                response = self.rekognition_client.start_face_detection(
                    Video={'Bytes': video.read()},
                    FaceAttributes=['ALL']
                )
            
            job_id = response['JobId']
            result = self.rekognition_client.get_face_detection(JobId=job_id)
            
            return {
                'smile_characteristics': self._analyze_smile_characteristics(result),
                'laugh_characteristics': self._analyze_laugh_characteristics(result),
                'serious_expression': self._analyze_serious_expression(result),
                'surprise_elements': self._analyze_surprise_elements(result),
                'eye_contact': self._analyze_eye_contact(result),
                'head_angle': self._analyze_head_angle(result),
                'micro_expressions': self._analyze_micro_expressions(result),
                'expression_transitions': self._analyze_expression_transitions(result)
            }
        except Exception as e:
            print(f"Error in face expression analysis: {str(e)}")
            return {}

    def _analyze_smile_characteristics(self, face_detection_result: Dict) -> Dict[str, float]:
        """Analyze detailed smile characteristics"""
        smile_features = {
            'smile_intensity': 0.0,
            'smile_duration': 0.0,
            'smile_genuineness': 0.0,
            'teeth_visibility': 0.0,
            'smile_symmetry': 0.0
        }

        faces_by_frame = face_detection_result['Faces']
        total_frames = len(faces_by_frame)
        smile_frames = 0
        
        for frame in faces_by_frame:
            for face in frame['Face']:
                # Analyze smile intensity
                if face.get('Smile', {}).get('Value'):
                    confidence = face['Smile'].get('Confidence', 0)
                    smile_features['smile_intensity'] = max(
                        smile_features['smile_intensity'],
                        confidence
                    )
                    smile_frames += 1

                # Analyze teeth visibility
                mouth_landmarks = [
                    point for point in face.get('Landmarks', [])
                    if 'Mouth' in point['Type']
                ]
                if mouth_landmarks:
                    smile_features['teeth_visibiliility'] = self._calculate_teeth_visibility(
                        face, mouth_landmarks
                    )

                # Analyze smile symmetry
                if mouth_landmarks:
                    smile_features['smile_symmetry'] = self._calculate_smile_symmetry(
                        mouth_landmarks
                    )

        # Calculate smile duration as percentage of total frames
        smile_features['smile_duration'] = (smile_frames / total_frames) * 100 if total_frames > 0 else 0
        
        # Calculate smile genuineness based on multiple factors
        smile_features['smile_genuineness'] = self._calculate_smile_genuineness(
            smile_features['smile_intensity'],
            smile_features['smile_symmetry'],
            face_detection_result
        )

        return smile_features

    def _calculate_te_teeth_visibility(self, face: Dict, mouth_landmarks: List) -> float:
        """Calculate teeth visibility score"""
        try:
            # Calculate mouth opening
            upper_lip = next(point for point in mouth_landmarks if point['Type'] == 'upperLip')
            lower_lip = next(point for point in mouth_landmarks if point['Type'] == 'lowerLip')
            
            mouth_height = abs(upper_lip['Y'] - lower_lip['Y'])
            face_height = face.get('BoundingBox', {}).get('Height', 1)
            
            # Normalize mouth height relative to face height
            relative_mouth_height = mouth_height / face_height
            
            # Convert to percentage with threshold
            visibility_score = min(100, (relative_mouth_height * 500))  # Adjust multiplier as needed
            
            return visibility_score
        except Exception:
            return 0.0

    def _calculate_smile_symmetry(self, mouth_landmarks: List) -> float:
        """Calculate smile symmetry score"""
        try:
            # Get left and right mouth corners
            left_corner = next(point for point in mouth_landmarks if point['Type'] == 'mouthLeft')
            right_corner = next(point for point in mouth_landmarks if point['Type'] == 'mouthRight')
            center_upper = next(point for point in mouth_landmarks if point['Type'] == 'upperLip')
            
            # Calculate distances from corners to center
            left_distance = ((left_corner['X'] - center_upper['X'])**2 + 
                           (left_corner['Y'] - center_upper['Y'])**2)**0.5
            right_distance = ((right_corner['X'] - center_upper['X'])**2 + 
                            (right_corner['Y'] - center_upper['Y'])**2)**0.5
            
            # Calculate symmetry score (100 = perfect symmetry)
            if max(left_distance, right_distance) > 0:
                symmetry = 100 * (1 - abs(left_distance - right_distance) / max(left_distance, right_distance))
            else:
                symmetry = 0
                
            return symmetry
        except Exception:
            return 0.0

    def _calculate_smile_genuineness(self, intensity: float, symmetry: float, 
                                   face_detection_result: Dict) -> float:
        """Calculate smile genuineness score"""
        # Factors that contribute to genuine smiles:
        # 1. Intensity
        # 2. Symmetry
        # 3. Eye involvement (Duchenne smile)
        # 4. Natural progression
        
        factors = {
            'intensity': intensity * 0.25,  # 25% weight
            'symmetry': symmetry * 0.25,    # 25% weight
            'eye_involvement': self._analyze_eye_involvement(face_detection_result) * 0.3,  # 30% weight
            'natural_progression': self._analyze_smile_progression(face_detection_result) * 0.2  # 20% weight
        }
        
        return sum(factors.values())

    def _analyze_eye_involvement(self, face_detection_result: Dict) -> float:
        """Analyze eye involvement in smile (Duchenne smile detection)"""
        try:
            max_eye_smile_correlation = 0.0
            
            for frame in face_detection_result['Faces']:
                for face in frame['Face']:
                    if face.get('Smile', {}).get('Value'):
                        # Check for eye crinkling (orbicularis oculi activation)
                        eye_landmarks = [
                            point for point in face.get('Landmarks', [])
                            if 'Eye' in point['Type']
                        ]
                        
                        if eye_landmarks:
                            eye_closure = self._calculate_eye_closure(eye_landmarks)
                            smile_intensity = face['Smile'].get('Confidence', 0)
                            
                            # Calculate correlation between smile and eye closure
                            correlation = self._calculate_eye_smile_correlation(
                                eye_closure, smile_intensity
                            )
                            max_eye_smile_correlation = max(max_eye_smile_correlation, correlation)
            
            return max_eye_smile_correlation
        except Exception:
            return 0.0

    def _calculate_head_stability(self, current_pose: Dict, previous_pose: Dict) -> float:
        """Calculate head stability score"""
        try:
            # Calculate changes in all angles
            pitch_change = abs(current_pose.get('Pitch', 0) - previous_pose.get('Pitch', 0))
            roll_change = abs(current_pose.get('Roll', 0) - previous_pose.get('Roll', 0))
            yaw_change = abs(current_pose.get('Yaw', 0) - previous_pose.get('Yaw', 0))
            
            # Total movement
            total_movement = pitch_change + roll_change + yaw_change
            
            # Convert to stability score (100 = perfectly stable, 0 = very unstable)
            stability = max(0, 100 - (total_movement * 2))
            
            return stability
        except Exception:
            return 0.0

    def _calculate_eye_closure(self, eye_landmarks: List) -> float:
        """Placeholder for calculating eye closure percentage"""
        # Implement actual calculation logic here
        return 0.0

    def _calculate_eye_smile_correlation(self, eye_closure: float, smile_intensity: float) -> float:
        """Placeholder for calculating correlation between eye closure and smile intensity"""
        # Implement actual correlation logic here
        return 0.0

    def _analyze_smile_progression(self, face_detection_result: Dict) -> float:
        """Placeholder for analyzing natural smile progression"""
        # Implement actual progression analysis logic here
        return 0.0

    def _analyze_laugh_characteristics(self, face_detection_result: Dict) -> Dict[str, float]:
        # Implement laugh characteristics analysis (not shown in error)
        return {}

    def _analyze_serious_expression(self, face_detection_result: Dict) -> Dict[str, float]:
        # Implement serious expression analysis (not shown in error)
        return {}

    def _analyze_surprise_elements(self, face_detection_result: Dict) -> Dict[str, float]:
        # Implement surprise elements analysis (not shown in error)
        return {}

    def _analyze_eye_contact(self, face_detection_result: Dict) -> Dict[str, float]:
        # Implement eye contact analysis (not shown in error)
        return {}

    def _analyze_head_angle(self, face_detection_result: Dict) -> Dict[str, float]:
        # Implement head angle analysis (not shown in error)
        return {}

    def _analyze_micro_expressions(self, face_detection_result: Dict) -> Dict[str, Any]:
        # Implement micro expressions analysis (not shown in error)
        return {}

    def _analyze_expression_transitions(self, face_detection_result: Dict) -> Dict[str, float]:
        # Implement expression transitions analysis (not shown in error)
        return {}

def _analyze_expression_transitions(self, face_detection_result: Dict) -> Dict[str, Any]:
    """Analyze characteristics of expression transitions"""
    transition_features = {
        'transition_speed': [],
        'transition_smoothness': [],
        'expression_holding_time': [],
        'expression_cycle_patterns': []
    }
    
    frames = face_detection_result['Faces']
    expression_timeline = []
    current_expression = None
    expression_start_time = 0
    
    for frame_idx, frame in enumerate(frames):
        for face in frame['Face']:
            # Get current dominant expression
            emotions = face.get('Emotions', [])
            current_dominant_emotion = self._get_dominant_emotion(emotions)
            
            # Track expression changes
            if current_dominant_emotion != current_expression:
                if current_expression is not None:
                    # Calculate holding time for previous expression
                    holding_time = frame_idx - expression_start_time
                    transition_features['expression_holding_time'].append(holding_time)
                    
                    # Calculate transition characteristics
                    transition_metrics = self._calculate_transition_metrics(
                        frames[expression_start_time:frame_idx + 1]
                    )
                    
                    transition_features['transition_speed'].append(
                        transition_metrics['speed']
                    )
                    transition_features['transition_smoothness'].append(
                        transition_metrics['smoothness']
                    )
                
                current_expression = current_dominant_emotion
                expression_start_time = frame_idx
            
            expression_timeline.append(current_dominant_emotion)
    
    # Analyze expression cycle patterns
    transition_features['expression_cycle_patterns'] = self._analyze_expression_cycles(
        expression_timeline
    )
    
    return transition_features

def _get_dominant_emotion(self, emotions: List) -> str:
    """Get the dominant emotion from a list of emotions"""
    if not emotions:
        return 'NEUTRAL'
    
    return max(emotions, key=lambda x: x['Confidence'])['Type']

def _calculate_transition_metrics(self, transition_frames: List) -> Dict[str, float]:
    """Calculate metrics for an expression transition"""
    metrics = {
        'speed': 0.0,
        'smoothness': 0.0
    }
    
    try:
        emotion_changes = []
        for i in range(1, len(transition_frames)):
            prev_frame = transition_frames[i-1]
            curr_frame = transition_frames[i]
            
            for face in curr_frame['Face']:
                prev_emotions = prev_frame['Face'][0].get('Emotions', [])
                curr_emotions = face.get('Emotions', [])
                
                # Calculate frame-to-frame emotion changes
                change_magnitude = self._calculate_emotion_change_magnitude(
                    prev_emotions, curr_emotions
                )
                emotion_changes.append(change_magnitude)
        
        if emotion_changes:
            # Speed is the average magnitude of change
            metrics['speed'] = sum(emotion_changes) / len(emotion_changes)
            
            # Smoothness is inverse of variance in change magnitude
            variance = np.var(emotion_changes) if len(emotion_changes) > 1 else 0
            metrics['smoothness'] = 100 * (1 - min(1, variance / 50))
    
    except Exception as e:
        print(f"Error calculating transition metrics: {str(e)}")
    
    return metrics

def _analyze_expression_cycles(self, expression_timeline: List[str]) -> List[Dict]:
    """Analyze patterns in expression cycles"""
    cycles = []
    current_cycle = []
    
    for expression in expression_timeline:
        current_cycle.append(expression)
        
        # Check for completed cycles (return to neutral or repeat)
        if len(current_cycle) > 1:
            if expression == 'NEUTRAL' or expression == current_cycle[0]:
                cycles.append({
                    'pattern': current_cycle.copy(),
                    'length': len(current_cycle),
                    'start_expression': current_cycle[0],
                    'end_expression': expression
                })
                current_cycle = [expression]
    
    return cycles

# Now moving on to body language analysis

class BodyLanguageAnalyzer:
    def __init__(self):
        self.rekognition_client = boto3.client('rekognition')
        self.confidence_threshold = 70.0

    def analyze_body_language(self, video_path: str) -> Dict[str, Any]:
        """Analyze all body language features"""
        try:
            with open(video_path, 'rb') as video:
                response = self.rekognition_client.start_person_tracking(
                    Video={'Bytes': video.read()}
                )
            
            job_id = response['JobId']
            result = self.rekognition_client.get_person_tracking(JobId=job_id)
            
            return {
                'hand_gestures': self._analyze_hand_gestures(result),
                'body_positioning': self._analyze_body_positioning(result),
                'movement_patterns': self._analyze_movement_patterns(result),
                'sitting_characteristics': self._analyze_sitting_characteristics(result),
                'standing_characteristics': self._analyze_standing_characteristics(result),
                'interaction_positioning': self._analyze_interaction_positioning(result),
                'group_formation': self._analyze_group_formation(result),
                'energy_level_indicators': self._analyze_energy_levels(result)
            }
        except Exception as e:
            print(f"Error in body language analysis: {str(e)}")
            return {}

    def _analyze_hand_gestures(self, person_tracking_result: Dict) -> Dict[str, Any]:
        """Analyze hand gesture characteristics"""
        gesture_features = {
            'gesture_type': [],
            'gesture_speed': [],
            'gesture_amplitude': [],
            'gesture_frequency': [],
            'finger_positioning': []
        }
        
        persons = person_tracking_result['Persons']
        
        for person in persons:
            body_points = person.get('KeyPoints', [])
            
            # Extract hand movements
            hand_movements = self._extract_hand_movements(body_points)
            
            # Classify gesture types
            if hand_movements:
                gesture_type = self._classify_gesture_type(hand_movements)
                gesture_features['gesture_type'].append(gesture_type)
                
                # Calculate gesture metrics
                metrics = self._calculate_gesture_metrics(hand_movements)
                gesture_features['gesture_speed'].append(metrics['speed'])
                gesture_features['gesture_amplitude'].append(metrics['amplitude'])
                gesture_features['gesture_frequency'].append(metrics['frequency'])
                
                # Analyze finger positioning if available
                if 'HandLandmarks' in person:
                    finger_position = self._analyze_finger_positioning(
                        person['HandLandmarks']
                    )
                    gesture_features['finger_positioning'].append(finger_position)
        
        return gesture_features

    def _extract_hand_movements(self, body_points: List) -> List[Dict]:
        """Extract hand movement trajectories from body keypoints"""
        hand_movements = []
        
        # Get left and right hand points
        left_hand_points = [
            point for point in body_points 
            if point['BodyPart'] == 'leftHand'
        ]
        right_hand_points = [
            point for point in body_points 
            if point['BodyPart'] == 'rightHand'
        ]
        
        # Track movement patterns
        for hand_points in [left_hand_points, right_hand_points]:
            if len(hand_points) > 1:
                movements = []
                for i in range(1, len(hand_points)):
                    movement = {
                        'dx': hand_points[i]['X'] - hand_points[i-1]['X'],
                        'dy': hand_points[i]['Y'] - hand_points[i-1]['Y'],
                        'confidence': min(
                            hand_points[i]['Confidence'],
                            hand_points[i-1]['Confidence']
                        ),
                        'timestamp': hand_points[i]['Timestamp']
                    }
                    movements.append(movement)
                hand_movements.append(movements)
        
        return hand_movements
def _analyze_finger_positioning(self, hand_landmarks: List) -> Dict[str, float]:
    """Analyze finger positioning and configurations"""
    finger_features = {
        'finger_spread': 0.0,
        'finger_curl': 0.0,
        'thumb_position': 0.0,
        'precision_grip': 0.0
    }
    
    # Calculate finger spread
    finger_tips = [point for point in hand_landmarks if 'tip' in point['Type'].lower()]
    if len(finger_tips) >= 2:
        spreads = []
        for i in range(len(finger_tips)-1):
            spread = ((finger_tips[i+1]['X'] - finger_tips[i]['X'])**2 +
                     (finger_tips[i+1]['Y'] - finger_tips[i]['Y'])**2)**0.5
            spreads.append(spread)
        finger_features['finger_spread'] = sum(spreads) / len(spreads)
    
    # Calculate finger curl
    for finger in ['index', 'middle', 'ring', 'pinky']:
        curl = self._calculate_finger_curl(hand_landmarks, finger)
        finger_features['finger_curl'] = max(finger_features['finger_curl'], curl)
    
    # Analyze thumb position
    thumb_features = self._analyze_thumb_position(hand_landmarks)
    finger_features['thumb_position'] = thumb_features
    
    # Detect precision grip
    finger_features['precision_grip'] = self._detect_precision_grip(hand_landmarks)
    
    return finger_features

def _classify_gesture_type(self, hand_movements: List[List[Dict]]) -> str:
    """Classify the type of gesture based on movement patterns"""
    gesture_types = {
        'pointing': 0.0,
        'waving': 0.0,
        'descriptive': 0.0,
        'emphatic': 0.0,
        'rhythmic': 0.0
    }
    
    for hand_trajectory in hand_movements:
        # Analyze movement patterns
        vertical_movement = sum(mov['dy'] for mov in hand_trajectory)
        horizontal_movement = sum(mov['dx'] for mov in hand_trajectory)
        
        # Calculate movement characteristics
        movement_speed = self._calculate_movement_speed(hand_trajectory)
        movement_direction_changes = self._count_direction_changes(hand_trajectory)
        movement_repetition = self._detect_movement_repetition(hand_trajectory)
        
        # Classify gestures based on characteristics
        if abs(horizontal_movement) > abs(vertical_movement) * 3:
            if movement_repetition > 0.7:
                gesture_types['waving'] += 1
            else:
                gesture_types['pointing'] += 1
                
        elif movement_direction_changes > 5:
            gesture_types['descriptive'] += 1
            
        elif movement_speed > 0.7:
            gesture_types['emphatic'] += 1
            
        elif movement_repetition > 0.5:
            gesture_types['rhythmic'] += 1
    
    # Return most prevalent gesture type
    return max(gesture_types.items(), key=lambda x: x[1])[0]

def _calculate_gesture_metrics(self, hand_movements: List[List[Dict]]) -> Dict[str, float]:
    """Calculate detailed metrics for gestures"""
    metrics = {
        'speed': 0.0,
        'amplitude': 0.0,
        'frequency': 0.0
    }
    
    for hand_trajectory in hand_movements:
        if not hand_trajectory:
            continue
            
        # Calculate speed
        time_diff = hand_trajectory[-1]['timestamp'] - hand_trajectory[0]['timestamp']
        if time_diff > 0:
            total_distance = sum(
                (mov['dx']**2 + mov['dy']**2)**0.5 
                for mov in hand_trajectory
            )
            metrics['speed'] = total_distance / time_diff
        
        # Calculate amplitude
        x_coords = [mov['dx'] for mov in hand_trajectory]
        y_coords = [mov['dy'] for mov in hand_trajectory]
        metrics['amplitude'] = max(
            max(x_coords) - min(x_coords),
            max(y_coords) - min(y_coords)
        )
        
        # Calculate frequency
        direction_changes = self._count_direction_changes(hand_trajectory)
        if time_diff > 0:
            metrics['frequency'] = direction_changes / time_diff
    
    return metrics

def _calculate_head_stability(self, current_pose: Dict, previous_pose: Dict) -> float:
    """Calculate head stability score"""
    try:
        # Calculate changes in all angles
        pitch_change = abs(current_pose.get('Pitch', 0) - previous_pose.get('Pitch', 0))
        roll_change = abs(current_pose.get('Roll', 0) - previous_pose.get('Roll', 0))
        yaw_change = abs(current_pose.get('Yaw', 0) - previous_pose.get('Yaw', 0))
        
        # Total movement
        total_movement = pitch_change + roll_change + yaw_change
        
        # Convert to stability score (100 = perfectly stable, 0 = very unstable)
        stability = max(0, 100 - (total_movement * 2))
        
        return stability
    except Exception:
        return 0.0

def _calculate_mouth_openness(self, mouth_landmarks: List) -> float:
    """Calculate degree of mouth openness"""
    try:
        upper_lip = next(point for point in mouth_landmarks if point['Type'] == 'upperLip')
        lower_lip = next(point for point in mouth_landmarks if point['Type'] == 'lowerLip')
        
        # Calculate vertical distance between lips
        vertical_gap = abs(upper_lip['Y'] - lower_lip['Y'])
        
        # Calculate horizontal width of mouth
        left_corner = next(point for point in mouth_landmarks if point['Type'] == 'mouthLeft')
        right_corner = next(point for point in mouth_landmarks if point['Type'] == 'mouthRight')
        mouth_width = abs(right_corner['X'] - left_corner['X'])
        
        # Calculate openness ratio
        if mouth_width > 0:
            openness = vertical_gap / mouth_width
            return min(1.0, openness * 3)  # Normalize to 0-1 range
        return 0.0
    except Exception:
        return 0.0

def _calculate_laugh_intensity(self, face: Dict, mouth_openness: float) -> float:
    """Calculate laugh intensity based on multiple factors"""
    try:
        # Combine multiple factors for laugh intensity
        smile_intensity = face.get('Smile', {}).get('Confidence', 0)
        
        # Get emotions if available
        emotions = face.get('Emotions', [])
        joy_score = next(
            (e['Confidence'] for e in emotions if e['Type'] == 'HAPPY'),
            0
        )
        
        # Weighted combination of factors
        intensity = (
            smile_intensity * 0.3 +  # 30% weight for smile
            mouth_openness * 50 * 0.4 +  # 40% weight for mouth openness
            joy_score * 0.3  # 30% weight for joy emotion
        )
        
        return min(100, intensity)
    except Exception:
        return 0.0

def _analyze_laugh_body_movement(self, frame: Dict) -> float:
    """Analyze body movement during laugh"""
    try:
        # Get pose information if available
        pose = frame.get('Pose', {})
        
        # Calculate movement score based on pose changes
        pitch_change = abs(pose.get('Pitch', 0))
        roll_change = abs(pose.get('Roll', 0))
        yaw_change = abs(pose.get('Yaw', 0))
        
        # Combine movement scores
        movement_score = (pitch_change + roll_change + yaw_change) / 3
        
        return min(100, movement_score)
    except Exception:
        return 0.0

def _analyze_head_angle(self, face_detection_result: Dict) -> Dict[str, float]:
    """Analyze head angle characteristics"""
    head_angle_features = {
        'head_tilt_degree': 0.0,
        'head_turn_angle': 0.0,
        'head_nod_frequency': 0.0,
        'head_stability': 0.0
    }
    
    frames = face_detection_result['Faces']
    total_frames = len(frames)
    previous_pose = None
    nod_count = 0
    stability_scores = []
    
    for frame in frames:
        for face in frame['Face']:
            pose = face.get('Pose', {})
            
            # Analyze head tilt (Roll)
            head_angle_features['head_tilt_degree'] = max(
                head_angle_features['head_tilt_degree'],
                abs(pose.get('Roll', 0))
            )
            
            # Analyze head turn (Yaw)
            head_angle_features['head_turn_angle'] = max(
                head_angle_features['head_turn_angle'],
                abs(pose.get('Yaw', 0))
            )
            
            # Detect head nods (Pitch changes)
            if previous_pose is not None:
                pitch_change = abs(pose.get('Pitch', 0) - previous_pose.get('Pitch', 0))
                if pitch_change > 10:  # Threshold for nod detection
                    nod_count += 1
            
            # Calculate head stability
            if previous_pose is not None:
                stability = self._calculate_head_stability(pose, previous_pose)
                stability_scores.append(stability)
            
            previous_pose = pose
    
    # Calculate final metrics
    if total_frames > 0:
        head_angle_features['head_nod_frequency'] = (nod_count / total_frames) * 100
        head_angle_features['head_stability'] = (
            sum(stability_scores) / len(stability_scores)
            if stability_scores else 0.0
        )
    
    return head_angle_features
def analyze_body_language(self, video_path: str) -> Dict[str, Any]:
    """Analyze all body language features"""
    return {
        'hand_gestures': self._analyze_hand_gestures(),
        'body_positioning': self._analyze_body_positioning(),
        'movement_patterns': self._analyze_movement_patterns(),
        'sitting_characteristics': self._analyze_sitting_characteristics(),
        'standing_characteristics': self._analyze_standing_characteristics(),
        'interaction_positioning': self._analyze_interaction_positioning(),
        'group_formation': self._analyze_group_formation(),
        'energy_level_indicators': self._analyze_energy_level_indicators()
    }

def _analyze_hand_gestures(self, video_frames: List[Dict]) -> Dict[str, Any]:
    """Analyze hand gesture characteristics"""
    return {
        'gesture_type': self._detect_gesture_types(video_frames),
        'gesture_speed': self._calculate_gesture_speed(video_frames),
        'gesture_amplitude': self._calculate_gesture_amplitude(video_frames),
        'gesture_frequency': self._calculate_ge_gesture_frequency(video_frames),
        'finger_positioning': self._analyze_finger_positioning(video_frames)
    }

def _analyze_body_positioning(self, video_frames: List[Dict]) -> Dict[str, Any]:
    """Analyze body positioning characteristics"""
    return {
        'posture_type': self._detect_posture_type(video_frames),
        'posture_changes': self._analyze_posture_changes(video_frames),
        'weight_distribution': self._analyze_weight_distribution(video_frames),
        'body_openness': self._analyze_body_openness(video_frames),
        'body_mirroring': self._analyze_body_mirroring(video_frames)
    }

def _analyze_movement_patterns(self, video_frames: List[Dict]) -> Dict[str, Any]:
    """Analyze movement patterns"""
    return {
        'movement_speed': self._calculate_movement_speed(video_frames),
        'movement_fluidity': self._analyze_movement_fluidity(video_frames),
        'movement_range': self._calculate_movement_range(video_frames),
        'movement_rhythm': self._analyze_movement_rhythm(video_frames),
        'stillness_periods': self._detect_stillness_periods(video_frames)
    }

def _analyze_sitting_characteristics(self, video_frames: List[Dict]) -> Dict[str, Any]:
    """Analyze sitting characteristics"""
    return {
        'sitting_posture': self._analyze_sitting_posture(video_frames),
        'leg_positioning': self._analyze_leg_positioning(video_frames),
        'seat_edge_proximity': self._calculate_seat_edge_proximity(video_frames),
        'fidgeting_frequency': self._analyze_fidgeting(video_frames),
        'sitting_duration': self._calculate_sitting_duration(video_frames)
    }

def _analyze_standing_characteristics(self, video_frames: List[Dict]) -> Dict[str, Any]:
    """Analyze standing characteristics"""
    return {
        'standing_posture': self._analyze_standing_posture(video_frames),
        'weight_shifting': self._analyze_weight_shifting(video_frames),
        'foot_positioning': self._analyze_foot_positioning(video_frames),
        'arm_placement': self._analyze_arm_placement(video_frames),
        'standing_duration': self._calculate_standing_duration(video_frames)
    }

def _analyze_interaction_positioning(self, video_frames: List[Dict]) -> Dict[str, Any]:
    """Analyze interaction positioning"""
    return {
        'interpersonal_distance': self._calculate_interpersonal_distance(video_frames),
        'body_angling': self._analyze_body_angling(video_frames),
        'eye_level_matching': self._analyze_eye_level_matching(video_frames),
        'touch_frequency': self._analyze_touch_frequency(video_frames),
        'personal_space_management': self._analyze_personal_space(video_frames)
    }

def _analyze_group_formation(self, video_frames: List[Dict]) -> Dict[str, Any]:
    """Analyze group formation characteristics"""
    return {
        'formation_shape': self._analyze_formation_shape(video_frames),
        'spacing_consistency': self._analyze_spacing_consistency(video_frames),
        'formation_changes': self._analyze_formation_changes(video_frames),
        'central_figure_positioning': self._analyze_central_figures(video_frames),
        'peripheral_positioning': self._analyze_peripheral_positions(video_frames)
    }

def _analyze_energy_level_indicators(self, video_frames: List[Dict]) -> Dict[str, Any]:
    """Analyze energy level indicators"""
    return {
        'movement_frequency': self._calculate_movement_frequency(video_frames),
        'gesture_enthusiasm': self._analyze_gesture_enthusiasm(video_frames),
        'vocal_energy': self._analyze_vocal_energy(video_frames),
        'facial_animation': self._analyze_facial_animation(video_frames),
        'overall_dynamism': self._calculate_overall_dynamism(video_frames)
    }

def _detect_gesture_types(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Detect and classify types of hand gestures"""
    gesture_types = {
        'pointing': 0.0,
        'waving': 0.0,
        'descriptive': 0.0,
        'emphatic': 0.0,
        'illustrative': 0.0
    }
    
    hand_trajectories = self._extract_hand_trajectories(video_frames)
    
    for trajectory in hand_trajectories:
        # Analyze movement patterns to identify gesture types
        if self._is_pointing_gesture(trajectory):
            gesture_types['pointing'] += 1
        elif self._is_waving_gesture(trajectory):
            gesture_types['waving'] += 1
        elif self._is_descriptive_gesture(trajectory):
            gesture_types['descriptive'] += 1
        elif self._is_emphatic_gesture(trajectory):
            gesture_types['emphatic'] += 1
        elif self._is_illustrative_gesture(trajectory):
            gesture_types['illustrative'] += 1
    
    # Normalize scores
    total_gestures = sum(gesture_types.values())
    if total_gestures > 0:
        for gesture_type in gesture_types:
            gesture_types[gesture_type] = (gesture_types[gesture_type] / total_gestures) * 100
    
    return gesture_types

def _calculate_gesture_speed(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Calculate the speed characteristics of gestures"""
    speed_metrics = {
        'average_speed': 0.0,
        'peak_speed': 0.0,
        'speed_variation': 0.0,
        'acceleration_patterns': []
    }
    
    hand_trajectories = self._extract_hand_trajectories(video_frames)
    speeds = []
    
    for trajectory in hand_trajectories:
        for i in range(1, len(trajectory)):
            # Calculate frame-to-frame speed
            dx = trajectory[i]['x'] - trajectory[i-1]['x']
            dy = trajectory[i]['y'] - trajectory[i-1]['y']
            dt = trajectory[i]['timestamp'] - trajectory[i-1]['timestamp']
            
            if dt > 0:
                speed = ((dx ** 2 + dy ** 2) ** 0.5) / dt
                speeds.append(speed)
                
                # Update peak speed
                speed_metrics['peak_speed'] = max(speed_metrics['peak_speed'], speed)
    
    if speeds:
        # Calculate average speed
        speed_metrics['average_speed'] = sum(speeds) / len(speeds)
        
        # Calculate speed variation
        speed_metrics['speed_variation'] = np.std(speeds) if len(speeds) > 1 else 0
        
        # Analyze acceleration patterns
        speed_metrics['acceleration_patterns'] = self._analyze_acceleration_patterns(speeds)
    
    return speed_metrics

def _calculate_gesture_amplitude(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Calculate the amplitude characteristics of gestures"""
    amplitude_metrics = {
        'vertical_range': 0.0,
        'horizontal_range': 0.0,
        'total_displacement': 0.0,
        'gesture_scale': 0.0,
        'movement_bounds': {'min_x': 0.0, 'max_x': 0.0, 'min_y': 0.0, 'max_y': 0.0}
    }
    
    hand_trajectories = self._extract_hand_trajectories(video_frames)
    
    for trajectory in hand_trajectories:
        if trajectory:
            # Extract x and y coordinates
            x_coords = [point['x'] for point in trajectory]
            y_coords = [point['y'] for point in trajectory]
            
            # Calculate ranges
            amplitude_metrics['horizontal_range'] = max(x_coords) - min(x_coords)
            amplitude_metrics['vertical_range'] = max(y_coords) - min(y_coords)
            
            # Update movement bounds
            amplitude_metrics['movement_bounds']['min_x'] = min(x_coords)
            amplitude_metrics['movement_bounds']['max_x'] = max(x_coords)
            amplitude_metrics['movement_bounds']['min_y'] = min(y_coords)
            amplitude_metrics['movement_bounds']['max_y'] = max(y_coords)
            
            # Calculate total displacement
            total_displacement = 0
            for i in range(1, len(trajectory)):
                dx = trajectory[i]['x'] - trajectory[i-1]['x']
                dy = trajectory[i]['y'] - trajectory[i-1]['y']
                total_displacement += (dx ** 2 + dy ** 2) ** 0.5
            
            amplitude_metrics['total_displacement'] = total_displacement
            
            # Calculate gesture scale relative to body size
            body_height = self._get_body_height(video_frames)
            if body_height:
                amplitude_metrics['gesture_scale'] = max(
                    amplitude_metrics['vertical_range'],
                    amplitude_metrics['horizontal_range']
                ) / body_height
    
    return amplitude_metrics

def _calculate_gesture_frequency(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Calculate the frequency characteristics of gestures"""
    frequency_metrics = {
        'gesture_rate': 0.0,
        'repetition_frequency': 0.0,
        'rhythm_patterns': [],
        'pause_duration': 0.0,
        'gesture_timing': []
    }
    
    gesture_timestamps = self._detect_gesture_timestamps(video_frames)
    total_duration = video_frames[-1]['timestamp'] - video_frames[0]['timestamp']
    
    if gesture_timestamps and total_duration > 0:
        # Calculate overall gesture rate
        frequency_metrics['gesture_rate'] = len(gesture_timestamps) / total_duration
        
        # Analyze intervals between gestures
        intervals = [
            gesture_timestamps[i] - gesture_timestamps[i-1] 
            for i in range(1, len(gesture_timestamps))
        ]
        
        if intervals:
            # Calculate repetition frequency
            frequency_metrics['repetition_frequency'] = 1 / (sum(intervals) / len(intervals))
            
            # Analyze rhythm patterns
            frequency_metrics['rhythm_patterns'] = self._analyze_rhythm_patterns(intervals)
            
            # Calculate average pause duration
            frequency_metrics['pause_duration'] = sum(intervals) / len(intervals)
            
            # Record gesture timing patterns
            frequency_metrics['gesture_timing'] = self._analyze_gesture_timing(
                gesture_timestamps, total_duration
            )
    
    return frequency_metrics

def _analyze_finger_positioning(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze finger positioning and configurations"""
    finger_metrics = {
        'finger_spread': 0.0,
        'finger_curl': 0.0,
        'finger_articulation': 0.0,
        'grip_patterns': [],
        'precision_control': 0.0
    }
    
    hand_landmarks = self._extract_hand_landmarks(video_frames)
    
    for frame_landmarks in hand_landmarks:
        # Calculate finger spread
        spread_score = self._calculate_finger_spread(frame_landmarks)
        finger_metrics['finger_spread'] = max(
            finger_metrics['finger_spread'],
            spread_score
        )
        
        # Analyze finger curl
        curl_score = self._calculate_finger_curl(frame_landmarks)
        finger_metrics['finger_curl'] = max(
            finger_metrics['finger_curl'],
            curl_score
        )
        
        # Measure finger articulation
        articulation_score = self._calculate_finger_articulation(frame_landmarks)
        finger_metrics['finger_articulation'] = max(
            finger_metrics['finger_articulation'],
            articulation_score
        )
        
        # Detect grip patterns
        grip_pattern = self._detect_grip_pattern(frame_landmarks)
        if grip_pattern:
            finger_metrics['grip_patterns'].append(grip_pattern)
        
        # Analyze precision control
        precision_score = self._calculate_precision_control(frame_landmarks)
        finger_metrics['precision_control'] = max(
            finger_metrics['precision_control'],
            precision_score
        )
    
    return finger_metrics
def _extract_hand_trajectories(self, video_frames: List[Dict]) -> List[List[Dict]]:
    """
    Extract hand movement trajectories from video frames
    Returns a list of trajectories for each hand
    """
    trajectories = {
        'left_hand': [],
        'right_hand': []
    }
    
    for frame in video_frames:
        keypoints = frame.get('Persons', [{}])[0].get('KeyPoints', [])
        
        # Extract hand positions
        left_hand = next(
            (point for point in keypoints if point['BodyPart'] == 'leftHand'),
            None
        )
        right_hand = next(
            (point for point in keypoints if point['BodyPart'] == 'rightHand'),
            None
        )
        
        # Add to trajectories if detected
        timestamp = frame.get('Timestamp', 0)
        
        if left_hand:
            trajectories['left_hand'].append({
                'x': left_hand['X'],
                'y': left_hand['Y'],
                'confidence': left_hand['Confidence'],
                'timestamp': timestamp
            })
            
        if right_hand:
            trajectories['right_hand'].append({
                'x': right_hand['X'],
                'y': right_hand['Y'],
                'confidence': right_hand['Confidence'],
                'timestamp': timestamp
            })
    
    return [traj for traj in trajectories.values() if traj]

def _is_pointing_gesture(self, trajectory: List[Dict]) -> bool:
    """
    Detect if a hand trajectory represents a pointing gesture
    Characteristics: direct movement, held position, minimal variation
    """
    if len(trajectory) < 3:
        return False
        
    # Calculate movement direction
    dx = trajectory[-1]['x'] - trajectory[0]['x']
    dy = trajectory[-1]['y'] - trajectory[0]['y']
    movement_distance = (dx**2 + dy**2)**0.5
    
    # Check for directional consistency
    direction_changes = 0
    prev_dx = 0
    prev_dy = 0
    
    for i in range(1, len(trajectory)):
        curr_dx = trajectory[i]['x'] - trajectory[i-1]['x']
        curr_dy = trajectory[i]['y'] - trajectory[i-1]['y']
        
        if prev_dx * curr_dx < 0 or prev_dy * curr_dy < 0:
            direction_changes += 1
            
        prev_dx, prev_dy = curr_dx, curr_dy
    
    # Criteria for pointing gesture
    is_direct = direction_changes <= 2
    is_held = self._check_held_position(trajectory)
    has_sufficient_movement = movement_distance > 0.1  # threshold relative to frame size
    
    return is_direct and is_held and has_sufficient_movement

def _is_waving_gesture(self, trajectory: List[Dict]) -> bool:
    """
    Detect if a hand trajectory represents a waving gesture
    Characteristics: repetitive side-to-side or up-down movement
    """
    if len(trajectory) < 5:
        return False
    
    # Analyze movement pattern
    direction_changes = 0
    prev_direction = None
    
    for i in range(1, len(trajectory)):
        dx = trajectory[i]['x'] - trajectory[i-1]['x']
        dy = trajectory[i]['y'] - trajectory[i-1]['y']
        
        # Determine primary movement direction
        curr_direction = 'horizontal' if abs(dx) > abs(dy) else 'vertical'
        
        if prev_direction and curr_direction != prev_direction:
            direction_changes += 1
            
        prev_direction = curr_direction
    
    # Calculate movement regularity
    periodicity = self._calculate_movement_periodicity(trajectory)
    
    # Criteria for waving gesture
    has_repetition = direction_changes >= 4
    is_regular = periodicity > 0.7
    
    return has_repetition and is_regular

def _is_descriptive_gesture(self, trajectory: List[Dict]) -> bool:
    """
    Detect if a hand trajectory represents a descriptive gesture
    Characteristics: varied movement, multiple directions, flowing motion
    """
    if len(trajectory) < 5:
        return False
    
    # Calculate movement variety
    movement_patterns = self._analyze_movement_patterns(trajectory)
    direction_diversity = self._calculate_direction_diversity(trajectory)
    smoothness = self._calculate_movement_smoothness(trajectory)
    
    # Criteria for descriptive gesture
    has_varied_movement = movement_patterns['variety_score'] > 0.6
    has_direction_changes = direction_diversity > 0.5
    is_smooth = smoothness > 0.7
    
    return has_varied_movement and has_direction_changes and is_smooth

def _check_held_position(self, trajectory: List[Dict]) -> bool:
    """Check if a position is held steady"""
    if len(trajectory) < 3:
        return False
        
    # Calculate movement variation
    variations = []
    for i in range(1, len(trajectory)):
        dx = trajectory[i]['x'] - trajectory[i-1]['x']
        dy = trajectory[i]['y'] - trajectory[i-1]['y']
        variation = (dx**2 + dy**2)**0.5
        variations.append(variation)
    
    avg_variation = sum(variations) / len(variations)
    return avg_variation < 0.05  # threshold for stability

def _calculate_movement_periodicity(self, trajectory: List[Dict]) -> float:
    """Calculate the periodicity of movement"""
    if len(trajectory) < 5:
        return 0.0
        
    # Extract x and y coordinates over time
    x_coords = [point['x'] for point in trajectory]
    y_coords = [point['y'] for point in trajectory]
    
    # Calculate autocorrelation for both x and y
    x_autocorr = self._calculate_autocorrelation(x_coords)
    y_autocorr = self._calculate_autocorrelation(y_coords)
    
    # Return maximum periodicity found in either dimension
    return max(x_autocorr, y_autocorr)

def _calculate_autocorrelation(self, values: List[float]) -> float:
    """Calculate autocorrelation of a signal"""
    if len(values) < 2:
        return 0.0
        
    mean = sum(values) / len(values)
    variance = sum((x - mean)**2 for x in values)
    
    if variance == 0:
        return 0.0
        
    normalized_values = [(x - mean) for x in values]
    correlations = []
    
    for lag in range(1, len(values) // 2):
        correlation = sum(
            normalized_values[i] * normalized_values[i-lag]
            for i in range(lag, len(values))
        ) / variance
        
        correlations.append(abs(correlation))
    
    return max(correlations) if correlations else 0.0
def _extract_hand_landmarks(self, video_frames: List[Dict]) -> List[Dict]:
    """Extract detailed hand landmarks from video frames"""
    hand_landmarks = []
    
    for frame in video_frames:
        frame_landmarks = {
            'left_hand': [],
            'right_hand': []
        }
        
        persons = frame.get('Persons', [])
        for person in persons:
            if 'HandLandmarks' in person:
                for landmark in person['HandLandmarks']:
                    hand_type = 'left_hand' if landmark['HandType'] == 'Left' else 'right_hand'
                    frame_landmarks[hand_type].append({
                        'x': landmark['X'],
                        'y': landmark['Y'],
                        'z': landmark.get('Z', 0),
                        'type': landmark['Type'],
                        'confidence': landmark['Confidence']
                    })
        
        if frame_landmarks['left_hand'] or frame_landmarks['right_hand']:
            hand_landmarks.append(frame_landmarks)
    
    return hand_landmarks

def _calculate_finger_spread(self, landmarks: Dict) -> float:
    """Calculate the degree of finger spreading"""
    spread_score = 0.0
    
    for hand_type in ['left_hand', 'right_hand']:
        if not landmarks[hand_type]:
            continue
            
        # Get fingertip positions
        fingertips = [
            point for point in landmarks[hand_type]
            if 'TIP' in point['type']
        ]
        
        if len(fingertips) >= 2:
            # Calculate distances between adjacent fingertips
            distances = []
            for i in range(len(fingertips) - 1):
                dx = fingertips[i+1]['x'] - fingertips[i]['x']
                dy = fingertips[i+1]['y'] - fingertips[i]['y']
                distance = (dx**2 + dy**2)**0.5
                distances.append(distance)
            
            # Calculate average spread
            avg_spread = sum(distances) / len(distances)
            spread_score = max(spread_score, avg_spread)
    
    return min(100, spread_score *e * 100)  # Normalize to 0-100 scale

def _calculate_finger_curl(self, landmarks: Dict) -> float:
    """Calculate the degree of finger curling"""
    curl_score = 0.0
    
    for hand_type in ['left_hand', 'right_hand']:
        if not landmarks[hand_type]:
            continue
            
        for finger in ['THUMB', 'INDEX', 'MIDDLE', 'RING', 'PINKY']:
            # Get finger joint positions
            joints = [
                point for point in landmarks[hand_type]
                if finger in point['type']
            ]
            
            if len(joints) >= 3:
                # Calculate curl angle
                curl_angle = self._calculate_finger_curl_angle(joints)
                curl_score = max(curl_score, curl_angle)
    
    return min(100, curl_score)  # Normalize to 0-100 scale

def _calculate_finger_articulation(self, landmarks: Dict) -> float:
    """Calculate the degree of independent finger movement"""
    articulation_score = 0.0
    
    for hand_type in ['left_hand', 'right_hand']:
        if not landmarks[hand_type]:
            continue
            
        # Calculate relative movement of each finger
        finger_movements = {}
        for finger in ['THUMB', 'INDEX', 'MIDDLE', 'RING', 'PINKY']:
            joints = [
                point for point in landmarks[hand_type]
                if finger in point['type']
            ]
            
            if joints:
                movement = self._calculate_finger_movement(joints)
                finger_movements[finger] = movement
        
        # Calculate independence of movement
        if finger_movements:
            movement_correlation = self._calculate_movement_correlation(finger_movements)
            articulation_score = max(articulation_score, 100 * (1 - movement_correlation))
    
    return articulation_score

def _detect_grip_pattern(self, landmarks: Dict) -> str:
    """Detect specific grip patterns"""
    grip_patterns = {
        'precision_grip': 0.0,
        'power_grip': 0.0,
        'pinch_grip': 0.0,
        'open_palm': 0.0
    }
    
    for hand_type in ['left_hand', 'right_hand']:
        if not landmarks[hand_type]:
            continue
            
        # Analyze thumb-index relationship for precision/pinch grip
        thumb_index_distance = self._calculate_thumb_index_distance(landmarks[hand_type])
        
        # Analyze overall finger curl for power grip
        finger_curl = self._calculate_overall_finger_curl(landmarks[hand_type])
        
        # Analyze finger spreading for open palm
        finger_spread = self._calculate_finger_spread_single_hand(landmarks[hand_type])
        
        # Determine grip type based on measurements
        if thumb_index_distance < 0.1:  # Close thumb-index proximity
            if finger_curl < 0.3:
                grip_patterns['pinch_grip'] += 1
            else:
                grip_patterns['precision_grip'] += 1
        elif finger_curl > 0.7:
            grip_patterns['power_grip'] += 1
        elif finger_spread > 0.7:
            grip_patterns['open_palm'] += 1
    
    # Return most probable grip pattern
    if grip_patterns:
        return max(grip_patterns.items(), key=lambda x: x[1])[0]
    return 'undefined'

def _calculate_precision_control(self, landmarks: Dict) -> float:
    """Calculate the precision of finger control"""
    precision_score = 0.0
    
    for hand_type in ['left_hand', 'right_hand']:
        if not landmarks[hand_type]:
            continue
            
        # Analyze stability of fine motor control
        stability = self._calculate_finger_stability(landmarks[hand_type])
        
        # Analyze precision of finger positioning
        positioning = self._calculate_finger_positioning_precision(landmarks[hand_type])
        
        # Analyze smoothness of movement
        smoothness = self._calculate_movement_smoothness_single_hand(landmarks[hand_type])
        
        # Combine metrics
        hand_precision = (stability + positioning + smoothness) / 3
        precision_score = max(precision_score, hand_precision)
    
    return precision_score

def _calculate_thumb_index_distance(self, hand_landmarks: List[Dict]) -> float:
    """Calculate distance between thumb and index finger"""
    thumb_tip = next((point for point in hand_landmarks if 'THUMB_TIP' in point['type']), None)
    index_tip = next((point for point in hand_landmarks if 'INDEX_FINGER_TIP' in point['type']), None)
    
    if thumb_tip and index_tip:
        dx = thumb_tip['x'] - index_tip['x']
        dy = thumb_tip['y'] - index_tip['y']
        dz = thumb_tip.get('z', 0) - index_tip.get('z', 0)
        
        return (dx**2 + dy**2 + dz**2)**0.5
    return 1.0  # Maximum distance if not found

def _calculate_movement_smoothness(self, trajectory: List[Dict]) -> float:
    """Calculate smoothness of movement"""
    if len(trajectory) < 3:
        return 0.0
        
    # Calculate jerk (rate of change of acceleration)
    jerks = []
    for i in range(2, len(trajectory)):
        dt = trajectory[i]['timestamp'] - trajectory[i-1]['timestamp']
        if dt > 0:
            # Calculate acceleration changes
            d2x = (trajectory[i]['x'] - 2*trajectory[i-1]['x'] + trajectory[i-2]['x']) / (dt**2)
            d2y = (trajectory[i]['y'] - 2*trajectory[i-1]['y'] + trajectory[i-2]['y']) / (dt**2)
            
            jerk = (d2x**2 + d2y**2)**0.5
            jerks.append(jerk)
    
    if jerks:
        # Convert jerk to smoothness score (inverse relationship)
        avg_jerk = sum(jerks) / len(jerks)
        smoothness = 1 / (1 + avg_jerk)  # Normalize to 0-1 range
        return min(100, smoothness * 100)
    
    return 0.0
def _calculate_movement_patterns(self, trajectory: List[Dict]) -> Dict[str, float]:
    """Analyze detailed movement patterns from trajectory"""
    patterns = {
        'variety_score': 0.0,
        'repetition_score': 0.0,
        'direction_changes': 0,
        'movement_complexity': 0.0
    }
    
    if len(trajectory) < 3:
        return patterns
        
    directions = []
    speeds = []
    
    for i in range(1, len(trajectory)):
        # Calculate movement direction
        dx = trajectory[i]['x'] - trajectory[i-1]['x']
        dy = trajectory[i]['y'] - trajectory[i-1]['y']
        direction = np.arctan2(dy, dx)
        directions.append(direction)
        
        # Calculate speed
        dt = trajectory[i]['timestamp'] - trajectory[i-1]['timestamp']
        if dt > 0:
            speed = ((dx**2 + dy**2)**0.5) / dt
            speeds.append(speed)
    
    # Calculate variety score based on direction diversity
    if directions:
        unique_directions = len(set([round(d, 1) for d in directions]))
        patterns['variety_score'] = min(100, (unique_directions / len(directions)) * 100)
    
    # Calculate repetition score
    if speeds:
        patterns['repetition_score'] = self._calculate_repetition_score(speeds)
    
    # Count direction changes
    patterns['direction_changes'] = self._count_direction_changes(directions)
    
    # Calculate movement complexity
    patterns['movement_complexity'] = self._calculate_movement_complexity(
        directions, speeds
    )
    
    return patterns

def _calculate_finger_joint_angles(self, joints: List[Dict]) -> List[float]:
    """Calculate angles between finger joints"""
    angles = []
    
    if len(joints) < 3:
        return angles
        
    for i in range(1, len(joints)-1):
        # Calculate vectors between joints
        v1 = np.array([
            joints[i]['x'] - joints[i-1]['x'],
            joints[i]['y'] - joints[i-1]['y'],
            joints[i].get('z', 0) - joints[i-1].get('z', 0)
        ])
        
        v2 = np.array([
            joints[i+1]['x'] - joints[i]['x'],
            joints[i+1]['y'] - joints[i]['y'],
            joints[i+1].get('z', 0) - joints[i].get('z', 0)
        ])
        
        # Calculate angle between vectors
        dot_product = np.dot(v1, v2)
        norms = np.linalg.norm(v1) * np.linalg.norm(v2)
        
        if norms > 0:
            cos_angle = dot_product / norms
            angle = np.arccos(np.clip(cos_angle, -1.0, 1.0))
            angles.append(np.degrees(angle))
    
    return angles

def _calculate_movement_correlation(self, finger_movements: Dict[str, List[float]]) -> float:
    """Calculate correlation between finger movements"""
    correlations = []
    
    fingers = list(finger_movements.keys())
    if len(fingers) < 2:
        return 0.0
    
    for i in range(len(fingers)):
        for j in range(i+1, len(fingers)):
            mov1 = finger_movements[fingers[i]]
            mov2 = finger_movements[fingers[j]]
            
            if len(mov1) == len(mov2) and len(mov1) > 1:
                correlation = np.corrcoef(mov1, mov2)[0,1]
                correlations.append(abs(correlation))
    
    return np.mean(correlations) if correlations else 0.0

def _calculate_repetition_score(self, speeds: List[float]) -> float:
    """Calculate repetition score based on speed patterns"""
    if len(speeds) < 3:
        return 0.0
    
    # Use autocorrelation to detect repeating patterns
    autocorr = np.correlate(speeds, speeds, mode='full')
    autocorr = autocorr[len(speeds)-1:]
    
    # Normalize
    autocorr = autocorr / autocorr[0]
    
    # Find peaks in autocorrelation
    peaks = []
    for i in range(1, len(autocorr)-1):
        if autocorr[i-1] < autocorr[i] > autocorr[i+1]:
            peaks.append(autocorr[i])
    
    if peaks:
        return min(100, (np.mean(peaks) * 100))
    return 0.0

def _calculate_movement_complexity(self, directions: List[float], speeds: List[float]) -> float:
    """Calculate movement complexity based on direction and speed variations"""
    if not directions or not speeds:
        return 0.0
    
    # Calculate direction complexity
    direction_changes = self._count_direction_changes(directions)
    direction_complexity = min(1.0, direction_changes / len(directions))
    
    # Calculate speed complexity
    speed_variance = np.var(speeds) if len(speeds) > 1 else 0
    speed_complexity = min(1.0, speed_variance / np.mean(speeds) if np.mean(speeds) > 0 else 0)
    
    # Combine complexities
    total_complexity = (direction_complexity + speed_complexity) / 2
    return total_complexity * 100

def _count_direction_changes(self, directions: List[float]) -> int:
    """Count significant changes in movement direction"""
    if len(directions) < 2:
        return 0
    
    changes = 0
    threshold = np.pi/6  # 30 degrees threshold
    
    for i in range(1, len(directions)):
        diff = abs(directions[i] - directions[i-1])
        # Handle circular nature of angles
        diff = min(diff, 2*np.pi - diff)
        if diff > threshold:
            changes += 1
    
    return changes

def _calculate_finger_stability(self, hand_landmarks: List[Dict]) -> float:
    """Calculate stability of finger positions"""
    if not hand_landmarks:
        return 0.0
    
    # Calculate variance in finger positions
    finger_positions = defaultdict(list)
    for landmark in hand_landmarks:
        finger_type = landmark['type'].split('_')[0]
        finger_positions[finger_type].append([landmark['x'], landmark['y']])
    
    stability_scores = []
    for positions in finger_positions.values():
        if len(positions) > 1:
            # Calculate variance of positions
            variance = np.var(positions, axis=0)
            stability = 1 / (1 + np.mean(variance))
            stability_scores.append(stability)
    
    return np.mean(stability_scores) * 100 if stability_scores else 0.0
def _analyze_body_positioning(self, video_frames: List[Dict]) -> Dict[str, Any]:
    """Analyze body positioning characteristics"""
    return {
        'posture_type': self._detect_posture_type(video_frames),
        'posture_changes': self._analyze_posture_changes(video_frames),
        'weight_distribution': self._analyze_weight_distribution(video_frames),
        'body_openness': self._analyze_body_openness(video_frames),
        'body_mirroring': self._analyze_body_mirroring(video_frames)
    }

def _detect_posture_type(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Detect and classify posture types"""
    posture_scores = {
        'upright': 0.0,
        'slouched': 0.0,
        'leaning': 0.0,
        'twisted': 0.0,
        'aligned': 0.0
    }
    
    for frame in video_frames:
        keypoints = frame.get('Persons', [{}])[0].get('KeyPoints', [])
        if not keypoints:
            continue
            
        # Extract key body points
        shoulders = [p for p in keypoints if 'shoulder' in p['BodyPart'].lower()]
        hips = [p for p in keypoints if 'hip' in p['BodyPart'].lower()]
        spine = [p for p in keypoints if 'spine' in p['BodyPart'].lower()]
        
        if shoulders and hips and spine:
            # Calculate spine angle
            spine_angle = self._calculate_spine_angle(shoulders, hips, spine)
            
            # Calculate shoulder alignment
            shoulder_alignment = self._calculate_shoulder_alignment(shoulders)
            
            # Calculate hip alignment
            hip_alignment = self._calculate_hip_alignment(hips)
            
            # Update posture scores based on measurements
            posture_scores['upright'] = max(posture_scores['upright'], 
                                          self._score_upright_posture(spine_angle))
            posture_scores['slouched'] = max(posture_scores['slouched'], 
                                           self._score_slouched_posture(spine_angle))
            posture_scores['leaning'] = max(posture_scores['leaning'], 
                                          self._score_leaning_posture(spine_angle))
            posture_scores['twisted'] = max(posture_scores['twisted'], 
                                          self._score_twisted_posture(shoulder_alignment, hip_alignment))
            posture_scores['aligned'] = max(posture_scores['aligned'], 
                                          self._score_aligned_posture(spine_angle, shoulder_alignment, hip_alignment))
    
    return posture_scores

def _analyze_posture_changes(self, video_frames: List[Dict]) -> Dict[str, Any]:
    """Analyze changes in posture over time"""
    posture_changes = {
        'frequency': 0.0,
        'magnitude': 0.0,
        'pattern': [],
        'transition_speed': 0.0,
        'stability_periods': []
    }
    
    prev_keypoints = None
    changes = []
    stable_periods = []
    stable_start = None
    
    for frame_idx, frame in enumerate(video_frames):
        keypoints = frame.get('Persons', [{}])[0].get('KeyPoints', [])
        if not keypoints:
            continue
            
        if prev_keypoints:
            # Calculate change from previous frame
            change_magnitude = self._calculate_postusture_change_magnitude(
                prev_keypoints, keypoints
            )
            changes.append({
                'magnitude': change_magnitude,
                'timestamp': frame.get('Timestamp', frame_idx)
            })
            
            # Detect stable periods
            if change_magnitude < 0.1:  # threshold for stability
                if stable_start is None:
                    stable_start = frame.get('Timestamp', frame_idx)
            elif stable_start is not None:
                stable_periods.append({
                    'start': stable_start,
                    'end': frame.get('Timestamp', frame_idx)
                })
                stable_start = None
        
        prev_keypoints = keypoints
    
    # Calculate metrics from collected data
    if changes:
        posture_changes['frequency'] = len(changes) / len(video_frames)
        posture_changes['magnitude'] = np.mean([c['magnitude'] for c in changes])
        posture_changes['transition_speed'] = self._calculate_transition_speed(changes)
        posture_changes['pattern'] = self._detect_change_patterns(changes)
        posture_changes['stability_periods'] = stable_periods
    
    return posture_changes

def _analyze_weight_distribution(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze body weight distribution"""
    distribution_metrics = {
        'left_right_balance': 0.0,
        'front_back_balance': 0.0,
        'stability_score': 0.0,
        'weight_shifts': [],
        'center_of_gravity': {'x': 0.0, 'y': 0.0}
    }
    
    for frame in video_frames:
        keypoints = frame.get('Persons', [{}])[0].get('KeyPoints', [])
        if not keypoints:
            continue
            
        # Extract key points for weight distribution analysis
        feet = [p for p in keypoints if 'foot' in p['BodyPart'].lower()]
        hips = [p for p in keypoints if 'hip' in p['BodyPart'].lower()]
        shoulders = [p for p in keypoints if 'shoulder' in p['BodyPart'].lower()]
        
        if feet and hips and shoulders:
            # Calculate left-right balance
            lr_balance = self._calculate_lateral_balance(feet, hips, shoulders)
            distribution_metrics['left_right_balance'] = lr_balance
            
            # Calculate front-back balance
            fb_balance = self._calculate_anteroposterior_balance(feet, hips, shoulders)
            distribution_metrics['front_back_balance'] = fb_balance
            
            # Calculate center of gravity
            cog = self._calculate_center_of_gravity(feet, hips, shoulders)
            distribution_metrics['center_of_gravity'] = cog
            
            # Detect weight shifts
            if self._is_weight_shift(lr_balance, fb_balance):
                distribution_metrics['weight_shifts'].append({
                    'timestamp': frame.get('Timestamp', 0),
                    'direction': self._determine_shift_direction(lr_balance, fb_balance)
                })
            
            # Calculate stability score
            stability = self._calculate_stability_score(cog, feet)
            distribution_metrics['stability_score'] = stability
    
    return distribution_metrics

def _analyze_body_openness(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze body openness and approachability"""
    openness_metrics = {
        'chest_openness': 0.0,
        'arm_position': 0.0,
        'body_facing': 0.0,
        'approachability_score': 0.0,
        'defensive_indicators': 0.0
    }
    
    for frame in video_frames:
        keypoints = frame.get('Persons', [{}])[0].get('KeyPoints', [])
        if not keypoints:
            continue
            
        # Calculate chest openness
        chest_openness = self._calculate_chest_openness(keypoints)
        openness_metrics['chest_openness'] = max(
            openness_metrics['chest_openness'],
            chest_openness
        )
        
        # Analyze arm positions
        arm_position_score = self._analyze_arm_positions(keypoints)
        openness_metrics['arm_position'] = max(
            openness_metrics['arm_position'],
            arm_position_score
        )
        
        # Calculate body facing direction
        body_facing = self._calculate_body_facing(keypoints)
        openness_metrics['body_facing'] = body_facing
        
        # Calculate overall approachability
        approachability = self._calculate_approachability(
            chest_openness,
            arm_position_score,
            body_facing
        )
        openness_metrics['approachability_score'] = max(
            openness_metrics['approachability_score'],
            approachability
        )
        
        # Detect defensive body language
        defensive_score = self._detect_defensive_indicators(keypoints)
        openness_metrics['defensive_indicators'] = max(
            openness_metrics['defensive_indicators'],
            defensive_score
        )
    
    return openness_metrics
def _calculate_spine_angle(self, shoulders: List[Dict], hips: List[Dict], spine: List[Dict]) -> float:
    """Calculate the angle of the spine relative to vertical"""
    try:
        # Get average shoulder and hip positions
        shoulder_center = np.mean([[s['X'], s['Y']] for s in shoulders], axis=0)
        hip_center = np.mean([[h['X'], h['Y']] for h in hips], axis=0)
        
        # Calculate spine vector
        spine_vector = shoulder_center - hip_center
        vertical_vector = np.array([0, 1])
        
        # Calculate angle
        cos_angle = np.dot(spine_vector, vertical_vector) / (
            np.linalg.norm(spine_vector) * np.linalg.norm(vertical_vector)
        )
        angle = np.arccos(np.clip(cos_angle, -1.0, 1.0))
        
        return np.degrees(angle)
    except Exception:
        return 0.0

def _calculate_shoulder_alignment(self, shoulders: List[Dict]) -> float:
    """Calculate shoulder alignment (rotation and tilt)"""
    if len(shoulders) < 2:
        return 0.0
        
    try:
        # Calculate shoulder line angle relative to horizontal
        left_shoulder = next(s for s in shoulders if 'left' in s['BodyPart'].lower())
        right_shoulder = next(s for s in shoulders if 'right' in s['BodyPart'].lower())
        
        dx = right_shoulder['X'] - left_shoulder['X']
        dy = right_shoulder['Y'] - left_shoulder['Y']
        
        angle = np.degrees(np.arctan2(dy, dx))
        return angle
    except Exception:
        return 0.0

def _calculate_hip_alignment(self, hips: List[Dict]) -> float:
    """Calculate hip alignment (rotation and tilt)"""
    if len(hips) < 2:
        return 0.0
        
    try:
        # Calculate hip line angle relative to horizontal
        left_hip = next(h for h in hips if 'left' in h['BodyPart'].lower())
        right_hip = next(h for h in hips if 'right' in h['BodyPart'].lower())
        
        dx = right_hip['X'] - left_hip['X']
        dy = right_hip['Y'] - left_hip['Y']
        
        angle = np.degrees(np.arctan2(dy, dx))
        return angle
    except Exception:
        return 0.0

def _score_upright_posture(self, spine_angle: float) -> float:
    """Score how upright the posture is"""
    # Perfect upright posture would be 0 degrees from vertical
    deviation = abs(spine_angle)
    max_deviation = 30  # Maximum deviation considered for upright posture
    
    if deviation <= max_deviation:
        score = 100 * (1 - deviation / max_deviation)
        return max(0, score)
    return 0.0

def _score_slouched_posture(self, spine_angle: float) -> float:
    """Score how slouched the posture is"""
    # Slouched posture typically has forward lean
    forward_lean = spine_angle if spine_angle > 0 else 0
    slouch_threshold = 20  # Minimum angle for slouch
    max_slouch = 45  # Maximum angle considered for slouch
    
    if slouch_threshold <= forward_lean <= max_slouch:
        score = 100 * ((forward_lean - slouch_threshold) / (max_slouch - slouch_threshold))
        return min(100, max(0, score))
    return 0.0

def _score_leaning_posture(self, spine_angle: float) -> float:
    """Score how much the posture is leaning"""
    lean_threshold = 10  # Minimum angle for lean
    max_lean = 45  # Maximum angle considered for lean
    
    if abs(spine_angle) >= lean_threshold:
        score = 100 * (min(abs(spine_angle), max_lean) - lean_threshold) / (max_lean - lean_threshold)
        return min(100, score)
    return 0.0

def _score_twisted_posture(self, shoulder_alignment: float, hip_alignment: float) -> float:
    """Score how twisted the posture is"""
    # Calculate difference between shoulder and hip alignment
    twist_angle = abs(shoulder_alignment - hip_alignment)
    twist_threshold = 10  # Minimum angle for twist
    max_twist = 45  # Maximum angle considered for twist
    
    if twist_angle >= twist_threshold:
        score = 100 * (min(twist_angle, max_twist) - twist_threshold) / (max_twist - twist_threshold)
        return min(100, score)
    return 0.0

def _score_aligned_posture(self, spine_angle: float, shoulder_alignment: float, 
                          hip_alignment: float) -> float:
    """Score overall postural alignment"""
    # Consider multiple factors for alignment
    spine_score = max(0, 100 - abs(spine_angle) * 2)
    alignment_diff_score = max(0, 100 - abs(shoulder_alignment - hip_alignment) * 2)
    
    # Combine scores with weights
    total_score = (spine_score * 0.6 + alignment_diff_score * 0.4)
    return total_score

def _calculate_posture_change_magnitude(self, prev_keypoints: List[Dict], 
                                      curr_keypoints: List[Dict]) -> float:
    """Calculate magnitude of posture change between frames"""
    try:
        # Calculate average movement of key points
        movements = []
        for prev_point in prev_keypoints:
            curr_point = next(
                (p for p in curr_keypoints if p['BodyPart'] == prev_point['BodyPart']),
                None
            )
            
            if curr_point:
                dx = curr_point['X'] - prev_point['X']
                dy = curr_point['Y'] - prev_point['Y']
                movement = (dx**2 + dy**2)**0.5
                movements.append(movement)
        
        return np.mean(movements) if movements else 0.0
    except Exception:
        return 0.0

def _calculate_transition_speed(self, changes: List[Dict]) -> float:
    """Calculate speed of posture transitions"""
    if len(changes) < 2:
        return 0.0
        
    speeds = []
    for i in range(1, len(changes)):
        time_diff = changes[i]['timestamp'] - changes[i-1]['timestamp']
        if time_diff > 0:
            speed = changes[i]['magnitude'] / time_diff
            speeds.append(speed)
    
    return np.mean(speeds) if speeds else 0.0

def _detect_change_patterns(self, changes: List[Dict]) -> List[Dict]:
    """Detect patterns in posture changes"""
    patterns = []
    if len(changes) < 3:
        return patterns
        
    # Look for repetitive patterns
    window_size = min(5, len(changes) // 2)
    for i in range(len(changes) - window_size):
        window = changes[i:i + window_size]
        pattern = {
            'start_time': window[0]['timestamp'],
            'end_time': window[-1]['timestamp'],
            'average_magnitude': np.mean([c['magnitude'] for c in window]),
            'regularity': self._calculate_pattern_regularity(window)
        }
        patterns.append(pattern)
    
    return patterns

def _calculate_pattern_regularity(self, window: List[Dict]) -> float:
    """Calculate regularity of movement pattern"""
    if len(window) < 2:
        return 0.0
        
    magnitudes = [c['magnitude'] for c in window]
    times = [c['timestamp'] for c in window]
    
    # Calculate variance in magnitude and timing
    magnitude_variance = np.var(magnitudes)
    time_intervals = np.diff(times)
    timing_variance = np.var(time_intervals)
    
    # Combine into regularity score
    regularity = 1 / (1 + magnitude_variance + timing_variance)
    return min(1.0, regularity)

def _calculate_lateral_balance(self, feet: List[Dict], hips: List[Dict], 
                             shoulders: List[Dict]) -> float:
    """Calculate left-right balance distribution"""
    try:
        # Calculate weighted center points
        foot_center = np.mean([[f['X'], f['Y']] for f in feet], axis=0)
        hip_center = np.mean([[h['X'], h['Y']] for h in hips], axis=0)
        shoulder_center = np.mean([[s['X'], s['Y']] for s in shoulders], axis=0)
        
        # Combine centers with different weights
        weighted_x = (
            foot_center[0] * 0.5 +     # Feet have highest weight
            hip_center[0] * 0.3 +      # Hips second
            shoulder_center[0] * 0.2    # Shoulders least
        )
        
        # Calculate balance relative to to center (0.5)
        # Returns -100 (full left) to 100 (full right)
        return (weighted_x - 0.5) * 200
        
    except Exception:
        return 0.0

def _calculate_anteroposterior_balance(self, feet: List[Dict], hips: List[Dict], 
                                     shoulders: List[Dict]) -> float:
    """Calculate front-back balance distribution"""
    try:
        # Calculate weighted center points for Y axis (front-back)
        foot_center = np.mean([[f['X'], f['Y']] for f in feet], axis=0)
        hip_center = np.mean([[h['X'], h['Y']] for h in hips], axis=0)
        shoulder_center = np.mean([[s['X'], s['Y']] for s in shoulders], axis=0)
        
        # Combine centers with different weights
        weighted_y = (
            foot_center[1] * 0.5 +
            hip_center[1] * 0.3 +
            shoulder_center[1] * 0.2
        )
        
        # Calculate balance relative to center (0.5)
        # Returns -100 (forward) to 100 (backward)
        return (weighted_y - 0.5) * 200
        
    except Exception:
        return 0.0

def _calculate_center_of_gravity(self, feet: List[Dict], hips: List[Dict], 
                               shoulders: List[Dict]) -> Dict[str, float]:
    """Calculate approximate center of gravity"""
    try:
        # Get all key points
        all_points = feet + hips + shoulders
        
        # Calculate weighted average position
        weights = {
            'foot': 0.1,    # Each foot
            'hip': 0.2,     # Each hip
            'shoulder': 0.1  # Each shoulder
        }
        
        weighted_x = 0
        weighted_y = 0
        total_weight = 0
        
        for point in all_points:
            weight = weights.get(point['BodyPart'].lower().split('_')[0], 0)
            weighted_x += point['X'] * weight
            weighted_y += point['Y'] * weight
            total_weight += weight
        
        if total_weight > 0:
            return {
                'x': weighted_x / total_weight,
                'y': weighted_y / total_weight
            }
        return {'x': 0.5, 'y': 0.5}
        
    except Exception:
        return {'x': 0.5, 'y': 0.5}

def _is_weight_shift(self, lr_balance: float, fb_balance: float) -> bool:
    """Detect if there is a significant weight shift"""
    # Define thresholds for significant weight shifts
    SHIFT_THRESHOLD = 20  # Percentage of weight shift considered significant
    
    return abs(lr_balance) > SHIFT_THRESHOLD or abs(fb_balance) > SHIFT_THRESHOLD

def _determine_shift_direction(self, lr_balance: float, fb_balance: float) -> str:
    """Determine the primary direction of weight shift"""
    if abs(lr_balance) > abs(fb_balance):
        return 'right' if lr_balance > 0 else 'left'
    else:
        return 'backward' if fb_balance > 0 else 'forward'

def _calculate_stability_score(self, cog: Dict[str, float], feet: List[Dict]) -> float:
    """Calculate stability score based on center of gravity and foot position"""
    try:
        # Calculate base of support (area between feet)
        foot_positions = np.array([[f['X'], f['Y']] for f in feet])
        if len(foot_positions) >= 2:
            # Calculate the convex hull of foot positions
            hull = ConvexHull(foot_positions)
            base_area = hull.area
            
            # Check if COG is within base of support
            cog_point = np.array([cog['x'], cog['y']])
            is_cog_supported = self._point_in_polygon(cog_point, foot_positions[hull.vertices])
            
            # Calculate distance from COG to center of base
            base_center = np.mean(foot_positions, axis=0)
            cog_distance = np.linalg.norm(cog_point - base_center)
            
            # Calculate stability score
            stability = 100 * (1 - min(1, cog_distance / (base_area ** 0.5)))
            if not is_cog_supported:
                stability *= 0.5
                
            return stability
            
        return 50.0  # Default middle stability if not enough foot points
        
    except Exception:
        return 50.0

def _calculate_chest_openness(self, keypoints: List[Dict]) -> float:
    """Calculate chest openness based on shoulder and spine position"""
    try:
        # Get relevant keypoints
        shoulders = [p for p in keypoints if 'shoulder' in p['BodyPart'].lower()]
        spine = [p for p in keypoints if 'spine' in p['BodyPart'].lower()]
        
        if len(shoulders) >= 2 and spine:
            # Calculate shoulder plane angle relative to camera
            left_shoulder = next(s for s in shoulders if 'left' in s['BodyPart'].lower())
            right_shoulder = next(s for s in shoulders if 'right' in s['BodyPart'].lower())
            
            shoulder_width = abs(right_shoulder['X'] - left_shoulder['X'])
            shoulder_depth = abs(right_shoulder['Y'] - left_shoulder['Y'])
            
            # Calculate openness score
            # Maximum when shoulders are square to camera (width high, depth low)
            if shoulder_width > 0:
                openness = 100 * (1 - min(1, shoulder_depth / shoulder_width))
                return openness
                
        return 50.0  # Default middle openness if not enough points
        
    except Exception:
        return 50.0

def _analyze_arm_positions(self, keypoints: List[Dict]) -> float:
    """Analyze arm positions for openness"""
    try:
        # Get arm joints
        shoulders = [p for p in keypoints if 'shoulder' in p['BodyPart'].lower()]
        elbows = [p for p in keypoints if 'elbow' in p['BodyPart'].lower()]
        wrists = [p for p in keypoints if 'wrist' in p['BodyPart'].lower()]
        
        if shoulders and elbows and wrists:
            # Calculate arm angles
            arm_angles = []
            for side in ['left', 'right']:
                try:
                    shoulder = next(s for s in shoulders if side in s['BodyPart'].lower())
                    elbow = next(e for e in elbows if side in e['BodyPart'].lower())
                    wrist = next(w for w in wrists if side in w['BodyPart'].lower())
                    
                    angle = self._calculate_arm_angle(shoulder, elbow, wrist)
                    arm_angles.append(angle)
                except StopIteration:
                    continue
            
            if arm_angles:
                # Score arm positions (higher score for more open positions)
                openness_scores = [self._score_arm_openness(angle) for angle in arm_angles]
                return np.mean(openness_scores)
                
        return 50.0  # Default middle openness if not enough points
        
    except Exception:
        return 50.0

def _calculate_arm_angle(self, shoulder: Dict, elbow: Dict, wrist: Dict) -> float:
    """Calculate angle formed by shoulder-elbow-wrist"""
    try:
        # Calculate vectors
        shoulder_elbow = np.array([elbow['X'] - shoulder['X'], 
                                 elbow['Y'] - shoulder['Y']])
        elbow_wrist = np.array([wrist['X'] - elbow['X'], 
                               wrist['Y'] - elbow['Y']])
        
        # Calculate angle
        dot_product = np.dot(shoulder_elbow, elbow_wrist)
        norms = np.linalg.norm(shoulder_elbow) * np.linalg.norm(elbow_wrist)
        
        if norms > 0:
            cos_angle = dot_product / norms
            angle = np.degrees(np.arccos(np.clip(cos_angle, -1.0, 1.0)))
            return angle
            
        return 0.0
        
    except Exception:
        return 0.0
def _score_arm_openness(self, angle: float) -> float:
    """Score arm openness based on angle"""
    # Define ideal ranges for open body language
    CLOSED_THRESHOLD = 45  # Angles below this are considered closed
    OPEN_THRESHOLD = 120   # Angles above this are considered maximally open
    
    if angle <= CLOSED_THRESHOLD:
        return 0.0
    elif angle >= OPEN_THRESHOLD:
        return 100.0
    else:
        # Linear interpolation between thresholds
        return 100 * (angle - CLOSED_THRESHOLD) / (OPEN_THRESHOLD - CLOSED_THRESHOLD)

def _calculate_body_facing(self, keypoints: List[Dict]) -> float:
    """Calculate how directly the body is facing the camera"""
    try:
        # Get shoulder and hip points
        shoulders = [p for p in keypoints if 'shoulder' in p['BodyPart'].lower()]
        hips = [p for p in keypoints if 'hip' in p['BodyPart'].lower()]
        
        if len(shoulders) >= 2 and len(hips) >= 2:
            # Calculate shoulder and hip widths
            shoulder_width = self._calculate_width(shoulders)
            hip_width = self._calculate_width(hips)
            
            # Compare actual widths to expected widths
            # When facing camera directly, these should be at their maximum
            shoulder_ratio = shoulder_width / self.EXPECTED_SHOULDER_WIDTH
            hip_ratio = hip_width / self.EXPECTED_HIP_WIDTH
            
            # Average the ratios and convert to angle
            facing_score = 100 * (shoulder_ratio + hip_ratio) / 2
            return min(100, facing_score)
            
        return 50.0  # Default middle facing if not enough points
        
    except Exception:
        return 50.0

def _calculate_approachability(self, chest_openness: float, arm_position: float, 
                             body_facing: float) -> float:
    """Calculate overall approachability score"""
    weights = {
        'chest_openness': 0.4,
        'arm_position': 0.35,
        'body_facing': 0.25
    }
    
    approachability = (
        chest_openness * weights['chest_openness'] +
        arm_position * weights['arm_position'] +
        body_facing * weights['body_facing']
    )
    
    return approachability

def _detect_defensive_indicators(self, keypoints: List[Dict]) -> float:
    """Detect and score defensive body language indicators"""
    defensive_score = 0.0
    indicators_found = 0
    
    try:
        # Check for crossed arms
        if self._are_arms_crossed(keypoints):
            defensive_score += 100
            indicators_found += 1
        
        # Check for self-hugging
        if self._is_self_hugging(keypoints):
            defensive_score += 100
            indicators_found += 1
        
        # Check for blocked torso
        if self._is_torso_blocked(keypoints):
            defensive_score += 75
            indicators_found += 1
        
        # Check for turned away posture
        if self._is_turned_away(keypoints):
            defensive_score += 50
            indicators_found += 1
        
        return defensive_score / max(1, indicators_found)
        
    except Exception:
        return 0.0

def _are_arms_crossed(self, keypoints: List[Dict]) -> bool:
    """Detect crossed arms position"""
    try:
        # Get arm joints
        elbows = [p for p in keypoints if 'elbow' in p['BodyPart'].lower()]
        wrists = [p for p in keypoints if 'wrist' in p['BodyPart'].lower()]
        
        if len(elbows) >= 2 and len(wrists) >= 2:
            # Check if wrists have crossed sides
            left_wrist = next(w for w in wrists if 'left' in w['BodyPart'].lower())
            right_wrist = next(w for w in wrists if 'right' in w['BodyPart'].lower())
            
            # Arms are crossed if left wrist is on right side and vice versa
            wrists_crossed = (
                left_wrist['X'] > right_wrist['X'] and
                abs(left_wrist['Y'] - right_wrist['Y']) < 0.2  # Vertical proximity
            )
            
            # Check elbow positions for typical crossed arms pose
            elbows_positioned = self._check_elbow_positions(elbows)
            
            return wrists_crossed and elbows_positioned
            
        return False
        
    except Exception:
        return False

def _is_self_hugging(self, keypoints: List[Dict]) -> bool:
    """Detect self-hugging position"""
    try:
        # Get hand and torso points
        hands = [p for p in keypoints if 'hand' in p['BodyPart'].lower()]
        shoulders = [p for p in keypoints if 'shoulder' in p['BodyPart'].lower()]
        hips = [p for p in keypoints if 'hip' in p['BodyPart'].lower()]
        
        if hands and shoulders and hips:
            # Calculate torso region
            torso_bounds = self._calculate_torso_bounds(shoulders, hips)
            
            # Check if hands are positioned on opposite sides of torso
            hands_hugging = all(
                self._is_point_in_torso(hand, torso_bounds)
                for hand in hands
            )
            
            return hands_hugging
            
        return False
        
    except Exception:
        return False

def _calculate_torso_bounds(self, shoulders: List[Dict], hips: List[Dict]) -> Dict:
    """Calculate the boundaries of the torso region"""
    try:
        # Get corner points
        left_shoulder = min(shoulders, key=lambda s: s['X'])
        right_shoulder = max(shoulders, key=lambda s: s['X'])
        left_hip = min(hips, key=lambda s: s['X'])
        right_hip = max(hips, key=lambda s: s['X'])
        
        return {
            'top': min(left_shoulder['Y'], right_shoulder['Y']),
            'bottom': max(left_hip['Y'], right_hip['Y']),
            'left': min(left_shoulder['X'], left_hip['X']),
            'right': max(right_shoulder['X'], right_hip['X'])
        }
        
    except Exception:
        return {'top': 0, 'bottom': 1, 'left': 0, 'right': 1}

def _is_point_in_torso(self, point: Dict, bounds: Dict) -> bool:
    """Check if a point is within the torso region"""
    return (
        bounds['left'] <= point['X'] <= bounds['right'] and
        bounds['top'] <= point['Y'] <= bounds['bottom']
    )
def _analyze_movement_patterns(self, video_frames: List[Dict]) -> Dict[str, Any]:
    """Analyze movement patterns in detail"""
    return {
        'movement_speed': self._analyze_movement_speed(video_frames),
        'movement_fluidity': self._analyze_movement_fluidity(video_frames),
        'movement_range': self._analyze_movement_range(video_frames),
        'movement_rhythm': self._analyze_movement_rhythm(video_frames),
        'stillness_periods': self._analyze_stillness_periods(video_frames)
    }

def _analyze_movement_speed(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze speed characteristics of movement"""
    speed_metrics = {
        'average_speed': 0.0,
        'peak_speed': 0.0,
           'speed_variability': 0.0,
        'acceleration_patterns': [],
        'speed_segments': []
    }
    
    try:
        speeds = []
        timestamps = []
        
        for i in range(1, len(video_frames)):
            current_frame = video_frames[i]
            previous_frame = video_frames[i-1]
            
            # Calculate frame-to-frame movement
            movement = self._calculate_frame_movement(previous_frame, current_frame)
            
            # Calculate speed
            time_diff = current_framrame.get('Timestamp', i) - previous_frame.get('Timestamp', i-1)
            if time_diff > 0:
                speed = movement / time_diff
                speeds.append(speed)
                timestamps.append(current_frame.get('Timestamp', i))
        
        if speeds:
            # Calculate basic metrics
            speed_metrics['average_speed'] = np.mean(speeds)
            speed_metrics['peak_speed'] = max(speeds)
            speed_metrics['speed_variability'] = np.std(speeds)
            
            # Analyze acceleration patterns
            speed_metrics['acceleration_patterns'] = self._analyze_acceleration(speeds, timestamps)
            
            # Identify speed segments
            speed_metrics['speed_segments'] = self._identify_speed_segments(speeds, timestamps)
    
    except Exception as e:
        print(f"Error in movement speed analysis: {str(e)}")
    
    return speed_metrics

def _analyze_movement_fluidity(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze fluidity and smoothness of movement"""
    fluidity_metrics = {
        'smoothness_score': 0.0,
        'jerk_score': 0.0,
        'continuity': 0.0,
        'transition_quality': 0.0,
        'flow_patterns': []
    }
    
    try:
        movements = []
        for i in range(2, len(video_frames)):
            # Get three consecutive frames for jerk calculation
            frame1 = video_frames[i-2]
            frame2 = video_frames[i-1]
            frame3 = video_frames[i]
            
            # Calculate movement characteristics
            jerk = self._calculate_jerk(frame1, frame2, frame3)
            smoothness = self._calculate_smoothness(frame1, frame2, frame3)
            
            movements.append({
                'jerk': jerk,
                'smoothness': smoothness,
                'timestamp': frame3.get('Timestamp', i)
            })
        
        if movements:
            # Calculate overall metrics
            fluidity_metrics['smoothness_score'] = np.mean([m['smoothness'] for m in movements])
            fluidity_metrics['jerk_score'] = np.mean([m['jerk'] for m in movements])
            
            # Calculate movement continuity
            fluidity_metrics['continuity'] = self._calculate_movement_continuity(movements)
            
            # Analyze transition quality
            fluidity_metrics['transition_quality'] = self._analyze_transition_quality(movements)
            
            # Identify flow patterns
            fluidity_metrics['flow_patterns'] = self._identify_flow_patterns(movements)
    
    except Exception as e:
        print(f"Error in movement fluidity analysis: {str(e)}")
    
    return fluidity_metrics

def _analyze_movement_range(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze the range and extent of movement"""
    range_metrics = {
        'vertical_range': 0.0,
        'horizontal_range': 0.0,
        'movement_area': 0.0,
        'range_utilization': 0.0,
        'spatial_patterns': []
    }
    
    try:
        # Track movement boundaries
        x_coords = []
        y_coords = []
        
        for frame in video_frames:
            keypoints = frame.get('Persons', [{}])[0].get('KeyPoints', [])
            if keypoints:
                x_coords.extend([p['X'] for p in keypoints])
                y_coords.extend([p['Y'] for p in keypoints])
        
        if x_coords and y_coords:
            # Calculate ranges
            range_metrics['horizontal_range'] = max(x_coords) - min(x_coords)
            range_metrics['vertical_range'] = max(y_coords) - min(y_coords)
            
            # Calculate movement area
            range_metrics['movement_area'] = range_metrics['horizontal_range'] * range_metrics['vertical_range']
            
            # Calculate range utilization
            range_metrics['range_utilization'] = self._calculate_range_utilization(x_coords, y_coords)
            
            # Identify spatial patterns
            range_metrics['spatial_patterns'] = self._identify_spatial_patterns(x_coords, y_coords)
    
    except Exception as e:
        print(f"Error in movement range analysis: {str(e)}")
    
    return range_metrics

def _analyze_movement_rhythm(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze rhythmic patterns in movement"""
    rhythm_metrics = {
        'regularity_score': 0.0,
        'tempo': 0.0,
        'rhythm_patterns': [],
        'movement_periodicity': 0.0,
        'rhythm_changes': []
    }
    
    try:
        movements = []
        for i in range(1, len(video_frames)):
            current_frame = video_frames[i]
            previous_frame = video_frames[i-1]
            
            movement = self._calculate_frame_movement(previous_frame, current_frame)
            timestamp = current_frame.get('Timestamp', i)
            
            movements.append({
                'magnitude': movement,
                'timestamp': timestamp
            })
        
        if movements:
            # Calculate rhythm regularity
            rhythm_metrics['regularity_score'] = self._calculate_rhythm_regularity(movements)
            
            # Calculate movement tempo
            rhythm_metrics['tempo'] = self._calculate_movement_tempo(movements)
            
            # Identify rhythm patterns
            rhythm_metrics['rhythm_patterns'] = self._identify_rhythm_patterns(movements)
            
            # Calculate movement periodicity
            rhythm_metrics['movement_periodicity'] = self._calculate_movement_periodicity(movements)
            
            # Detect rhythm changes
            rhythm_metrics['rhythm_changes'] = self._detect_rhythm_changes(movements)
    
    except Exception as e:
        print(f"Error in movement rhythm analysis: {str(e)}")
    
    return rhythm_metrics

def _analyze_stillness_periods(self, video_frames: List[Dict]) -> Dict[str, Any]:
    """Analyze periods of stillness in movement"""
    stillness_metrics = {
        'stillness_periods': [],
        'total_stillness_time': 0.0,
        'average_stillness_duration': 0.0,
        'stillness_frequency': 0.0,
        'stillness_distribution': []
    }
    
    try:
        current_stillness = None
        movement_threshold = 0.05  # Threshold for considering movement as stillness
        
        for i in range(1, len(video_frames)):
            current_frame = video_frames[i]
            previous_frame = video_frames[i-1]
            
            movement = self._calculate_frame_movement(previous_frame, current_frame)
            timestamp = current_frame.get('Timestamp', i)
            
            # Detect stillness periods
            if movement < movement_threshold:
                if current_stillness is None:
                    current_stillness = {'start': timestamp}
            elif current_stillness is not None:
                current_stillness['end'] = previous_frame.get('Timestamp', i-1)
                current_stillness['duration'] = current_stillness['end'] - current_stillness['start']
                stillness_metrics['stillness_periods'].append(current_stillness)
                current_stillness = None
        
        # Calculate stillness metrics
        if stillness_metrics['stillness_periods']:
            durations = [p['duration'] for p in stillness_metrics['stillness_periods']]
            stillness_metrics['total_stillness_time'] = sum(durations)
            stillness_metrics['average_stillness_duration'] = np.mean(durations)
            stillness_metrics['stillness_frequency'] = len(stillness_metrics['stillness_periods']) / len(video_frames)
            stillness_metrics['stillness_distribution'] = self._analyze_stillness_distribution(
                stillness_metrics['stillness_periods'], video_frames[-1].get('Timestamp', len(video_frames))
            )
    
    except Exception as e:
        print(f"Error in stillness analysis: {str(e)}")
    
    return stillness_metrics
def _calculate_frame_movement(self, previous_frame: Dict, current_frame: Dict) -> float:
    """Calculate total movement between two frames"""
    try:
        prev_keypoints = previous_frame.get('Persons', [{}])[0].get('KeyPoints', [])
        curr_keypoints = current_frame.get('Persons', [{}])[0].get('KeyPoints', [])
        
        total_movement = 0.0
        point_count = 0
        
        for prev_point in prev_keypoints:
            # Find corresponding point in current frame
            curr_point = next(
                (p for p in curr_keypoints if p['BodyPart'] == prev_point['BodyPart']),
                None
            )
            
            if curr_point:
                # Calculate Euclidean distance
                dx = curr_point['X'] - prev_point['X']
                dy = curr_point['Y'] - prev_point['Y']
                movement = (dx**2 + dy**2)**0.5
                
                total_movement += movement
                point_count += 1
        
        return total_movement / max(1, point_count)
    
    except Exception:
        return 0.0

def _analyze_acceleration(self, speeds: List[float], timestamps: List[float]) -> List[Dict]:
    """Analyze acceleration patterns in movement"""
    acceleration_patterns = []
    
    try:
        for i in range(1, len(speeds)):
            speed_change = speeds[i] - speeds[i-1]
            time_diff = timestamps[i] - timestamps[i-1]
            
            if time_diff > 0:
                acceleration = speed_change / time_diff
                
                pattern = {
                    'timestamp': timestamps[i],
                    'acceleration': acceleration,
                    'type': 'acceleration' if acceleration > 0 else 'deceleration',
                    'magnitude': abs(acceleration)
                }
                
                acceleration_patterns.append(pattern)
    
    except Exception:
        pass
    
    return acceleration_patterns

def _identify_speed_segments(self, speeds: List[float], timestamps: List[float]) -> List[Dict]:
    """Identify distinct segments of movement speed"""
    segments = []
    current_segment = None
    SPEED_THRESHOLD = 0.1  # Threshold for speed change
    
    try:
        for i in range(len(speeds)):
            if current_segment is None:
                current_segment = {
                    'start_time': timestamps[i],
                    'start_speed': speeds[i],
                    'min_speed': speeds[i],
                    'max_speed': speeds[i]
                }
            else:
                # Check if speed has changed significantly
                speed_diff = abs(speeds[i] - current_segment['start_speed'])
                if speed_diff > SPEED_THRESHOLD:
                    # End current segment
                    current_segment['end_time'] = timestamps[i]
                    current_segment['end_speed'] = speeds[i]
                    current_segment['duration'] = current_segment['end_time'] - current_segment['start_time']
                    segments.append(current_segment)
                    
                    # Start new segment
                    current_segment = {
                        'start_time': timestamps[i],
                        'start_speed': speeds[i],
                        'min_speed': speeds[i],
                        'max_speed': speeds[i]
                    }
                else:
                    # Update min/max speeds
                    current_segment['min_speed'] = min(current_segment['min_speed'], speeds[i])
                    current_segment['max_speed'] = max(current_segment['max_speed'], speeds[i])
        
        # Add final segment
        if current_segment:
            current_segment['end_time'] = timestamps[-1]
            current_segment['end_speed'] = speeds[-1]
            current_segment['duration'] = current_segment['end_time'] - current_segment['start_time']
            segments.append(current_segment)
    
    except Exception:
        pass
    
    return segments

def _calculate_jerk(self, frame1: Dict, frame2: Dict, frame3: Dict) -> float:
    """Calculate jerk (rate of change of acceleration)"""
    try:
        # Get timestamps
        t1 = frame1.get('Timestamp', 0)
        t2 = frame2.get('Timestamp', 1)
        t3 = frame3.get('Timestamp', 2)
        
        # Calculate velocities
        v1 = self._calculate_frame_movement(frame1, frame2) / (t2 - t1)
        v2 = self._calculate_frame_movement(frame2, frame3) / (t3 - t2)
        
        # Calculate acceleration change
        acceleration_change = v2 - v1
        time_diff = t3 - t1
        
        if time_diff > 0:
            return acceleration_change / time_diff
        return 0.0
    
    except Exception:
        return 0.0

def _calculate_smoothness(self, frame1: Dict, frame2: Dict, frame3: Dict) -> float:
    """Calculate movement smoothness"""
    try:
        # Calculate jerk
        jerk = self._calculate_jerk(frame1, frame2, frame3)
        
        # Convert jerk to smoothness score (inverse relationship)
        smoothness = 1 / (1 + abs(jerk))
        
        return smoothness * 100  # Convert to percentage
    
    except Exception:
        return 0.0

def _calculate_movement_continuity(self, movements: List[Dict]) -> float:
    """Calculate continuity of movement"""
    try:
        # Analyze gaps and discontinuities in movement
        discontinuities = 0
        total_measurements = len(movements)
        
        for i in range(1, len(movements)):
            # Check for sudden changes in smoothness
            smoothness_change = abs(movements[i]['smoothness'] - movements[i-1]['smoothness'])
            if smoothness_change > 0.3:  # Threshold for discontinuity
                discontinuities += 1
        
        # Calculate continuity score
        continuity = 1 - (discontinuities / max(1, total_measurements))
        return continuity * 100
    
    except Exception:
        return 0.0

def _analyze_transition_quality(self, movements: List[Dict]) -> float:
    """Analyze quality of movement transitions"""
    try:
        transition_scores = []
        
        for i in range(2, len(movements)):
            # Analyze three consecutive measurements
            prev_movement = movements[i-2]
            curr_movement = movements[i-1]
            next_movement = movements[i]
            
            # Calculate transition smoothness
            transition_smoothness = self._calculate_transition_smoothness(
                prev_movement, curr_movement, next_movement
            )
            transition_scores.append(transition_smoothness)
        
        return np.mean(transition_scores) if transition_scores else 0.0
    
    except Exception:
        return 0.0

def _calculate_transition_smoothness(self, prev: Dict, curr: Dict, next: Dict) -> float:
    """Calculate smoothness of transition between movements"""
    try:
        # Calculate changes in movement characteristics
        jerk_consistency = abs(curr['jerk'] - (prev['jerk'] + next['jerk'])/2)
        smoothness_consistency = abs(curr['smoothness'] - (prev['smoothness'] + next['smoothness'])/2)
        
        # Combine metrics (lower is better)
        transition_quality = 1 - (jerk_consistency + smoothness_consistency) / 2
        return max(0, min(100, transition_quality * 100))
    
    except Exception:
        return 0.0
def _identify_flow_patterns(self, movements: List[Dict]) -> List[Dict]:
    """Identify patterns in movement flow"""
    flow_patterns = []
    
    try:
        window_size = 5  # Look at 5 movements at a time
        for i in range(len(movements) - window_size):
            window = movements[i:i + window_size]
            
            # Calculate flow characteristics
            avg_smoothness = np.mean([m['smoothness'] for m in window])
            avg_jerk = np.mean([m['jerk'] for m in window])
            
            # Identify pattern type
            if avg_smoothness > 80 and avg_jerk < 0.2:
                pattern_type = 'fluid'
            elif avg_smoothness < 40 and avg_jerk > 0.5:
                pattern_type = 'choppy'
            else:
                pattern_type = 'mixed'
            
            flow_patterns.append({
                'start_time': window[0]['timestamp'],
                'end_time': window[-1]['timestamp'],
                'type': pattern_type,
                'smoothness': avg_smoothness,
                'jerk': avg_jerk
            })
    
    except Exception:
        pass
    
    return flow_patterns

def _calculate_range_utilization(self, x_coords: List[float], y_coords: List[float]) -> float:
    """Calculate how effectively the available movement range is utilized"""
    try:
        # Create 2D histogram of positions
        hist, _, _ = np.histogram2d(x_coords, y_coords, bins=10)
        
        # Calculate percentage of bins used
        total_bins = hist.size
        used_bins = np.count_nonzero(hist)
        
        return (used_bins / total_bins) * 100
    
    except Exception:
        return 0.0

def _identify_spatial_patterns(self, x_coords: List[float], y_coords: List[float]) -> List[Dict]:
    """Identify patterns in spatial movement"""
    patterns = []
    
    try:
        # Combine coordinates into points
        points = list(zip(x_coords, y_coords))
        
        # Look for common patterns
        patterns.extend(self._detect_linear_movements(points))
        patterns.extend(self._detect_circular_movements(points))
        patterns.extend(self._detect_repetitive_movements(points))
    
    except Exception:
        pass
    
    return patterns

def _calculate_rhythm_regularity(self, movements: List[Dict]) -> float:
    """Calculate how regular the movement rhythm is"""
    try:
        # Calculate intervals between movement peaks
        intervals = []
        for i in range(1, len(movements)):
            if movements[i]['magnitude'] > movements[i-1]['magnitude']:
                intervals.append(movements[i]['timestamp'] - movements[i-1]['timestamp'])
        
        if intervals:
            # Calculate variance in intervals
            variance = np.var(intervals)
            # Convert to regularity score (inverse relationship)
            regularity = 1 / (1 + variance)
            return regularity * 100
        
        return 0.0
    
    except Exception:
        return 0.0

def _analyze_stillness_distribution(self, stillness_periods: List[Dict], total_time: float) -> List[Dict]:
    """Analyze the distribution of stillness periods"""
    try:
        # Divide total time into segments
        segment_count = 10
        segment_duration = total_time / segment_count
        
        distribution = []
        for i in range(segment_count):
            segment_start = i * segment_duration
            segment_end = (i + 1) * segment_duration
            
            # Calculate stillness time in this segment
            stillness_time = sum(
                min(period['end'], segment_end) - max(period['start'], segment_start)
                for period in stillness_periods
                if period['end'] > segment_start and period['start'] < segment_end
            )
            
            distribution.append({
                'segment_start': segment_start,
                'segment_end': segment_end,
                'stillness_percentage': (stillness_time / segment_duration) * 100
            })
        
        return distribution
    
    except Exception:
        return []
def _analyze_interpersonal_distance(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze distance between people in the video"""
    distance_metrics = {
        'average_distance': 0.0,
        'minimum_distance': float('inf'),
        'maximum_distance': 0.0,
        'distance_variation': 0.0,
        'proximity_zones': {
            'intimate': 0,    # 0-1.5 feet
            'personal': 0,    # 1.5-4 feet
            'social': 0,      # 4-12 feet
            'public': 0       # 12+ feet
        }
    }

    distances = []
    for frame in video_frames:
        persons = frame.get('Persons', [])
        if len(persons) >= 2:
            for i in range(len(persons)):
                for j in range(i + 1, len(persons)):
                    distance = self._calculate_person_distance(persons[i], persons[j])
                    distances.append(distance)
                    
                    # Update proximity zones
                    if distance < 0.5:  # 1.5 feet
                        distance_metrics['proximity_zones']['intimate'] += 1
                    elif distance < 1.3:  # 4 feet
                        distance_metrics['proximity_zones']['personal'] += 1
                    elif distance < 4.0:  # 12 feet
                        distance_metrics['proximity_zones']['social'] += 1
                    else:
                        distance_metrics['proximity_zones']['public'] += 1

    if distances:
        distance_metrics['average_distance'] = np.mean(distances)
        distance_metrics['minimum_distance'] = min(distances)
        distance_metrics['maximum_distance'] = max(distances)
        distance_metrics['distance_variation'] = np.std(distances)

    return distance_metrics

def _analyze_body_angling(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze body orientation between people"""
    angling_metrics = {
        'face_to_face_time': 0,
        'angular_relationship': [],
        'mutual_orientation': 0.0,
        'engagement_angle': 0.0,
        'orientation_changes': []
    }

    frame_count = len(video_frames)
    face_to_face_frames = 0

    for frame in video_frames:
        persons = frame.get('Persons', [])
        if len(persons) >= 2:
            angles = self._calculate_interpersonal_angles(persons)
            
            # Analyze face-to-face positioning
            if self._is_face_to_face(angles):
                face_to_face_frames += 1

            # Track angular relationships
            angling_metrics['angular_relationship'].append({
                'timestamp': frame.get('Timestamp', 0),
                'angles': angles,
                'engagement_levelvel': self._calculate_engagement_from_angles(angles)
            })

    if frame_count > 0:
        angling_metrics['face_to_face_time'] = (face_to_face_frames / frame_count) * 100
        angling_metrics['mutual_orientation'] = self._calculate_mutual_orienientation(
            angling_metrics['angular_relationship']
        )
        angling_metrics['engagement_angle'] = self._calculate_average_engagemgement_angle(
            angling_metrics['angular_relationship']
        )
        angling_metrics['orientation_changes'] = self._detect_orientation_changes(
            angling_metrics['angular_relationship']
        )

    return angling_metrics

def _analyze_eye_level_matching(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze eye level matching between people"""
    eye_level_metrics = {
        'matching_percentage': 0.0,
        'height_differences': [],
        'gaze_alignment': 0.0,
        'power_dynamics': 0.0,
        'eye_level_adjustments': []
    }

    matched_frames = 0
    for frame in video_frames:
        persons = frame.get('Persons', [])
        if len(persons) >= 2:
            eye_levels = self._extract_eye_levels(persons)
            
            # Calculate eye level differences
            if eye_levels:
                height_diff = max(eye_levels) - min(eye_levels)
                eye_level_metrics['height_differences'].append({
                    'timestamp': frame.get('Timestamp', 0),
                    'difference': height_diff
                })

                # Check if eyes are at matching level (within threshold)
                if height_diff < 0.1:  # threshold for matching
                    matched_frames += 1

    if video_frames:
        eye_level_metrics['matching_percentage'] = (matched_frames / len(video_frames)) * 100
        eye_level_metrics['gaze_alignment'] = self._calculate_gaze_alignment(
            eye_level_metrics['height_differences']
        )
        eye_level_metrics['power_dynamics'] = self._analyze_power_dynamics(
            eye_level_metrics['height_differences']
        )
        eye_level_metrics['eye_level_adjustments'] = self._detect_eye_level_adjustments(
            eye_level_metrics['height_differences']
        )

    return eye_level_metrics

def _analyze_touch_frequency(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze frequency and type of touch interactions"""
    touch_metrics = {
        'touch_count': 0,
        'touch_duration': [],
        'touch_types': {
            'handshake': 0,
            'high_five': 0,
            'pat': 0,
            'other': 0
        },
        'touch_intensity': [],
        'touch_patterns': []
    }

    current_touch = None
    for frame in video_frames:
        persons = frame.get('Persons', [])
        if len(persons) >= 2:
            touch_detected = self._detect_touch(persons)
            
            if touch_detected:
                if current_touch is None:
                    current_touch = {
                        'start_time': frame.get('Timestamp', 0),
                        'type': touch_detected['type'],
                        'intensity': touch_detected['intensity']
                    }
                    touch_metrics['touch_types'][touch_detected['type']] += 1
                    touch_metrics['touch_intensity'].append(touch_detected['intensity'])
            elif current_touch is not None:
                current_touch['end_time'] = frame.get('Timestamp', 0)
                current_touch['duration'] = current_touch['end_time'] - current_touch['start_time']
                touch_metrics['touch_duration'].append(current_touch['duration'])
                touch_metrics['touch_patterns'].append(current_touch)
                current_touch = None
                touch_metrics['touch_count'] += 1

    return touch_metrics

def _analyze_personal_space(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze personal space management"""
    space_metrics = {
        'average_bubble_size': 0.0,
        'space_violations': [],
        'adaptation_patterns': [],
        'comfort_zone_maintenance': 0.0,
        'space_negotiation': []
    }

    for frame in video_frames:
        persons = frame.get('Persons', [])
        if len(persons) >= 2:
            # Analyze personal space bubbles
            bubbles = self._calculate_personal_bubbles(persons)
            space_metrics['average_bubble_size'] = np.mean([b['size'] for b in bubbles])

            # Detect personal space violations
            violations = self._detect_space_violations(persons, bubbles)
            if violations:
                space_metrics['space_violations'].append({
                    'timestamp': frame.get('Timestamp', 0),
                    'violations': violations
                })

            # Analyze space adaptation
            adaptations = self._analyze_space_adaptations(persons, bubbles)
            if adaptations:
                space_metrics['adaptation_patterns'].append({
                    'timestamp': frame.get('Timestamp', 0),
                    'adaptations': adaptations
                })

    # Calculate comfort zone maintenance score
    space_metrics['comfort_zone_maintenance'] = self._calculate_comfort_zone_score(
        space_metrics['space_violations'],
        space_metrics['adaptation_patterns']
    )

    return space_metrics

def _analyze_movement_frequency(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze frequency and intensity of movements"""
    frequency_metrics = {
        'movement_rate': 0.0,
        'active_periods': [],
        'rest_periods': [],
        'energy_distribution': [],
        'activity_patterns': []
    }

    movement_threshold = 0.1
    current_period = None
    
    for frame_idx, frame in enumerate(video_frames[1:], 1):
        movement = self._calculate_frame_movement(video_frames[frame_idx-1], frame)
        
        # Detect active vs rest periods
        if movement > movement_threshold:
            if current_period is None or current_period['type'] == 'rest':
                if current_period:
                    current_period['end_time'] = frame.get('Timestamp', frame_idx)
                    if current_period['type'] == 'rest':
                        frequency_metrics['rest_periods'].append(current_period)
                current_period = {
                    'type': 'active',
                    'start_time': frame.get('Timestamp', frame_idx),
                    'intensity': movement
                }
        else:
            if current_period is None or current_period['type'] == 'active':
                if current_period:
                    current_period['end_time'] = frame.get('Timestamp', frame_idx)
                    if current_period['type'] == 'active':
                        frequency_metrics['active_periods'].append(current_period)
                current_period = {
                    'type': 'rest',
                    'start_time': frame.get('Timestamp', frame_idx)
                }

    # Calculate overall movement rate
    if video_frames:
        total_active_time = sum(
            period['end_time'] - period['start_time']
            for period in frequency_metrics['active_periods']
        )
        total_time = video_frames[-1].get('Timestamp', len(video_frames)) - video_frames[0].get('Timestamp', 0)
        frequency_metrics['movement_rate'] = (total_active_time / total_time) * 100

    return frequency_metrics

def _analyze_gesture_enthusiasm(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze enthusiasm in gestures"""
    enthusiasm_metrics = {
        'average_amplitude': 0.0,
        'gesture_speed': 0.0,
        'expressiveness_score': 0.0,
        'enthusiasm_peaks': [],
        'engagement_level': 0.0
    }

    amplitudes = []
    speeds = []
    
    for frame_idx, frame in enumerate(video_frames[1:], 1):
        gesture_data = self._analyze_frame_gestures(frame)
        if gesture_data:
            amplitudes.append(gesture_data['amplitude'])
            speeds.append(gesture_data['speed'])
            
            # Detect enthusiasm peaks
            if self._is_enthusiasm_peak(gesture_data):
                enthusiasm_metrics['enthusiasm_peaks'].append({
                    'timestamp': frame.get('Timestamp', frame_idx),
                    'intensity': gesture_data['intensity']
                })

    if amplitudes and speeds:
        enthusiasm_metrics['average_amplitude'] = np.mean(amplitudes)
        enthusiasm_metrics['gesture_speed'] = np.mean(speeds)
        enthusiasm_metrics['expressiveness_score'] = self._calculate_expressiveness(
            amplitudes, speeds
        )
        enthusiasm_metrics['engagement_level'] = self._calculate_engagement_from_enthusiasm(
            enthusiasm_metrics
        )

    return enthusiasm_metrics

def _analyze_facial_animation(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze facial animation and expressiveness"""
    animation_metrics = {
        'expression_changes': 0,
        'expression_intensity': 0.0,
        'facial_movement_score': 0.0,
        'emotional_range': 0.0,
        'animation_patterns': []
    }

    prev_expression = None
    intensities = []
    
    for frame in video_frames:
        faces = frame.get('FaceDetails', [])
        for face in faces:
            # Analyze expression changes
            current_expression = self._get_dominant_expression(face)
            if prev_expression and current_expression != prev_expression:
                animation_metrics['expression_changes'] += 1
            prev_expression = current_expression

            # Calculate expression intensity
            intensity = self._calculate_expression_intensity(face)
            intensities.append(intensity)

            # Track animation patterns
            animation_metrics['animation_patterns'].append({
                'timestamp': frame.get('Timestamp', 0),
                'expression': current_expression,
                'intensity': intensity
            })

    if intensities:
        animation_metrics['expression_intensity'] = np.mean(intensities)
        animation_metrics['facial_movement_score'] = self._calculate_facial_movement_score(
            animation_metrics['animation_patterns']
        )
        animation_metrics['emotional_range'] = self._calculate_emotional_range(
            animation_metrics['animation_patterns']
        )

    return animation_metrics

def _analyze_overall_dynamism(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze overall energy and dynamism"""
    dynamism_metrics = {
        'energy_score': 0.0,
        'variability': 0.0,
        'sustained_energy': 0.0,
        'energy_peaks': [],
        'energy_flow': []
    }

    # Combine multiple energy indicators
    movement_energy = self._analyze_movement_energy(video_frames)
    gesture_energy = self._analyze_gesture_energy(video_frames)
    facial_energy = self._analyze_facial_energy(video_frames)
    
    # Calculate overall energy score with weights
    weights = {
        'movement': 0.4,
        'gesture': 0.3,
        'facial': 0.3
    }
    
    dynamism_metrics['energy_score'] = (
        movement_energy * weights['movement'] +
        gesture_energy * weights['gesture'] +
        facial_energy * weights['facial']
    )

    # Calculate energy variability
    energy_values = [
        self._calculate_frame_energy(frame)
        for frame in video_frames
    ]
    
    if energy_values:
        dynamism_metrics['variability'] = np.std(energy_values)
        dynamism_metrics['sustained_energy'] = self._calculate_sustained_energy(energy_values)
        dynamism_metrics['energy_peaks'] = self._identify_energy_peaks(energy_values)
        dynamism_metrics['energy_flow'] = self._analyze_energy_flow(energy_values)

    return dynamism_metrics

def _calculate_engagement_from_angles(self, angles: List[float]) -> float:
    """Calculate engagement level based on angular relationships"""
    if not angles:
        return 0.0
    
    # Higher engagement when angles are smaller (people facing each other)
    engagement_scores = [max(0, 100 - angle) for angle in angles]
    return sum(engagement_scores) / len(engagement_scores)

def _extract_eye_levels(self, persons: List[Dict]) -> List[float]:
    """Extract eye height levels for all people"""
    eye_levels = []
    
    for person in persons:
        keypoints = person.get('KeyPoints', [])
        eyes = [p['Y'] for p in keypoints if 'eye' in p['BodyPart'].lower()]
        if eyes:
            eye_levels.append(sum(eyes) / len(eyes))
    
    return eye_levels

# Helper methods for interaction positioning

def _calculate_person_distance(self, person1: Dict, person2: Dict) -> float:
    """Calculate distance between two people"""
    try:
        # Get center points of each person
        p1_keypoints = person1.get('KeyPoints', [])
        p2_keypoints = person2.get('KeyPoints', [])
        
        p1_center = self._calculate_person_center(p1_keypoints)
        p2_center = self._calculate_person_center(p2_keypoints)
        
        # Calculate Euclidean distance
        dx = p2_center['x'] - p1_center['x']
        dy = p2_center['y'] - p1_center['y']
        
        return (dx**2 + dy**2)**0.5
    except Exception:
        return float('inf')

def _calculate_person_center(self, keypoints: List[Dict]) -> Dict[str, float]:
    """Calculate center point of a person from their keypoints"""
    if not keypoints:
        return {'x': 0, 'y': 0}
    
    x_coords = [p['X'] for p in keypoints]
    y_coords = [p['Y'] for p in keypoints]
    
    return {
        'x': sum(x_coords) / len(x_coords),
        'y': sum(y_coords) / len(y_coords)
    }

def _calculate_interpersonal_angles(self, persons: List[Dict]) -> List[float]:
    """Calculate angular relationships between people"""
    angles = []
    
    for i in range(len(persons)):
        for j in range(i + 1, len(persons)):
            angle = self._calculate_facing_angle(persons[i], persons[j])
            angles.append(angle)
    
    return angles

def _calculate_facing_angle(self, person1: Dict, person2: Dict) -> float:
    """Calculate the angle between two people's facing directions"""
    try:
        # Get face direction for each person
        p1_direction = self._get_face_direction(person1)
        p2_direction = self._get_face_direction(person2)
        
        # Calculate angle between directions
        angle_diff = abs(p1_direction - p2_direction)
        return min(angle_diff, 360 - angle_diff)
    except Exception:
        return 0.0

def _get_face_direction(self, person: Dict) -> float:
    """Get the direction a person is facing in degrees"""
    try:
        face = person.get('Face', {})
        pose = face.get('Pose', {})
        return pose.get('Yaw', 0)
    except Exception:
        return 0.0

def _is_face_to_face(self, angles: List[float]) -> bool:
    """Determine if people are facing each other"""
    # Consider face-to-face if angle is within 45 degrees
    return any(angle <= 45 for angle in angles)
def analyze_appearance(self, video_frames: List[Dict]) -> Dict[str, Any]:
    """Analyze appearance characteristics"""
    return {
        'outfit_analysis': {
            'color_scheme': self._analyze_color_scheme(video_frames),
            'pattern_types': self._analyze_pattern_types(video_frames),
            'outfit_components': self._analyze_outfit_components(video_frames),
            'fit_characteristics': self._analyze_fit_characteristics(video_frames),
            'style_categorization': self._analyze_style_categorization(video_frames)
        }
    }

def _analyze_color_scheme(self, video_frames: List[Dict]) -> Dict[str, Any]:
    """Analyze color scheme of outfit"""
    color_metrics = {
        'primary_color': '',
        'secondary_color': '',
        'color_contrast': 0.0,
        'color_coordination': 0.0,
        'color_psychology': {}
    }
    
    try:
        # Analyze colors across frames
        colors = []
        for frame in video_frames:
            person = frame.get('Persons', [{}])[0]
            detected_colors = self._detect_colors(person)
            if detected_colors:
                colors.append(detected_colors)
        
        if colors:
            # Identify primary and secondary colors
            dominant_colors = self._find_dominant_colors(colors)
            color_metrics['primary_color'] = dominant_colors[0] if dominant_colors else ''
            color_metrics['secondary_color'] = dominant_colors[1] if len(dominant_colors) > 1 else ''
            
            # Calculate contrast and coordination
            color_metrics['color_contrast'] = self._calculate_color_contrast(dominant_colors)
            color_metrics['color_coordination'] = self._analyze_color_coordination(dominant_colors)
            
            # Analyze color psychology
            color_metrics['color_psychology'] = self._analyze_color_psychology(dominant_colors)
    
    except Exception as e:
        print(f"Error in color scheme analysis: {str(e)}")
    
    return color_metrics

def _analyze_pattern_types(self, video_frames: List[Dict]) -> Dict[str, Any]:
    """Analyze patterns in clothing"""
    pattern_metrics = {
        'pattern_style': '',
        'pattern_complexity': 0.0,
        'pattern_size': 0.0,
        'pattern_contrast': 0.0,
        'pattern_distribution': 0.0
    }
    
    try:
        patterns = []
        for frame in video_frames:
            person = frame.get('Persons', [{}])[0]
            detected_patterns = self._detect_patterns(person)
            if detected_patterns:
                patterns.append(detected_patterns)
        
        if patterns:
            # Identify dominant pattern style
            pattern_metrics['pattern_style'] = self._identify_pattern_style(patterns)
            
            # Calculate pattern characteristics
            patternern_metrics['pattern_complexity'] = self._calculate_pattern_complexity(patterns)
            pattern_metrics['pattern_size'] = self._calculate_pattern_size(patterns)
            pattern_metrics['pattern_contrast'] = self._calculate_pattern_contrast(patterns)
            pattern_metrics['pattern_distribution'] = self._analyze_pattern_distribution(patterns)
    
    except Exception as e:
        print(f"Error in pattern analysis: {str(e)}")
    
    return pattern_metrics

def _analyze_outfit_components(self, video_frames: List[Dict]) -> Dict[str, Any]:
    """Analyze indivdividual components of the outfit"""
    component_metrics = {
        'top_style': '',
        'bottom_style': '',
        'outerwear_type': '',
        'footwear_style': '',
        'accessory_count': 0
    }
    
    try:
        for frame in video_frames:
            person = frame.get('Persons', [{}])[0]
            
            # Detect clothing items
            top = self._detect_top_style(person)
            bottom = self._detect_bottom_style(person)
            outerwear = self._detect_outerwear(person)
            footwear = self._detect_footwear(person)
            accessories = self._count_accessories(person)
            
            # Update metrics if detection confidence is high
            if top.get('confidence', 0) > self.confidence_threshold:
                component_metrics['top_style'] = top['style']
            if bottom.get('confidence', 0) > self.confidence_threshold:
                component_metrics['bottom_style'] = bottom['style']
            if outerwear.get('confidence', 0) > self.confidence_threshold:
                component_metrics['outerwear_type'] = outerwear['type']
            if footwear.get('confidence', 0) > self.confidence_threshold:
                component_metrics['footwear_style'] = footwear['style']
            
            component_metrics['accessory_count'] = max(
                component_metrics['accessory_count'],
                accessories
            )
    
    except Exception as e:
        print(f"Error in outfit component analysis: {str(e)}")
    
    return component_metrics

def _analyze_fit_characteristics(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze how clothing fits the person"""
    fit_metrics = {
        'looseness': 0.0,
        'tightness': 0.0,
        'draping': 0.0,
        'tailoring_quality': 0.0,
        'proportion_balance': 0.0
    }
    
    try:
        fits = []
        for frame in video_frames:
            person = frame.get('Persons', [{}])[0]
            detected_fit = self._analyze_clothing_fit(person)
            if detected_fit:
                fits.append(detected_fit)
        
        if fits:
            # Calculate average fit metrics
            fit_metrics['looseness'] = np.mean([f['looseness'] for f in fits])
            fit_metrics['tightness'] = np.mean([f['tightness'] for f in fits])
            fit_metrics['draping'] = np.mean([f['draping'] for f in fits])
            fit_metrics['tailoring_quality'] = np.mean([f['tailoring_quality'] for f in fits])
            fit_metrics['proportion_balance'] = self._calculate_proportion_balance(fits)
    
    except Exception as e:
        print(f"Error in fit analysis: {str(e)}")
    
    return fit_metrics

def _analyze_style_categorization(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Categorize overall style of outfit"""
    style_scores = {
        'casual': 0.0,
        'formal': 0.0,
        'athletic': 0.0,
        'bohemian': 0.0,
        'preppy': 0.0,
        'edgy': 0.0,
        'vintage': 0.0,
        'minimalist': 0.0,
        'maximalist': 0.0
    }
    
    try:
        for frame in video_frames:
            person = frame.get('Persons', [{}])[0]
            
            # Analyze various style indicators
            colors = self._detect_colors(person)
            patterns = self._detect_patterns(person)
            components = self._analyze_outfit_components(video_frames)
            fit = self._analyze_clothing_fit(person)
            
            # Update style scores based on various factors
            style_scores = self._calculate_style_scores(
                style_scores,
                colors,
                patterns,
                components,
                fit
            )
    
    except Exception as e:
        print(f"Error in style categorization: {str(e)}")
    
    return style_scores
def _detect_colors(self, person: Dict) -> List[Dict]:
    """Detect colors in clothing"""
    try:
        colors = []
        labels = person.get('Labels', [])
        
        for label in labels:
            if 'color' in label.get('Name', '').lower():
                colors.append({
                    'color': label['Name'].replace('Color', '').strip(),
                    'confidence': label['Confidence']
                })
        
        return sorted(colors, key=lambda x: x['confidence'], reverse=True)
    except Exception:
        return []

def _find_dominant_colors(self, colors_list: List[List[Dict]]) -> List[str]:
    """Find the most dominant colors across frames"""
    try:
        color_counts = {}
        for colors in colors_list:
            for color in colors:
                color_name = color['color']
                if color_name in color_counts:
                    color_counts[color_name] += color['confidence']
                else:
                    color_counts[color_name] = color['confidence']
        
        # Sort colors by total confidence
        sorted_colors = sorted(color_counts.items(), key=lambda x: x[1], reverse=True)
        return [color[0] for color in sorted_colors[:2]]
    except Exception:
        return []

def _calculate_color_contrast(self, colors: List[str]) -> float:
    """Calculate contrast between dominant colors"""
    if len(colors) < 2:
        return 0.0
    
    try:
        # Color contrast values (pre-defined)
        contrast_matrix = {
            'black': {'white': 100, 'red': 80, 'blue': 70},
            'white': {'black': 100, 'red': 60, 'blue': 50},
            # Add more color combinations as needed
        }
        
        color1, color2 = colors[0].lower(), colors[1].lower()
        if color1 in contrast_matrix and color2 in contrast_matrix[color1]:
            return contrast_matrix[color1][color2]
        return 50.0  # Default contrast value
    except Exception:
        return 0.0

def _analyze_color_coordination(self, colors: List[str]) -> float:
    """Analyze how well colors coordinate"""
    try:
        # Color harmony rules
        complementary_pairs = [
            {'red', 'green'},
            {'blue', 'orange'},
            {'yellow', 'purple'}
        ]
        
        analogous_groups = [
            {'red', 'orange', 'yellow'},
            {'blue', 'purple', 'red'},
            {'yellow', 'green', 'blue'}
        ]
        
        color_set = set(color.lower() for color in colors)
        
        # Check for complementary colors
        for pair in complementary_pairs:
            if color_set.intersection(pair) == pair:
                return 100.0
        
        # Check for analogous colors
        for group in analogous_groups:
            if len(color_set.intersection(group)) >= 2:
                return 80.0
        
        return 50.0  # Default coordination score
    except Exception:
        return 0.0

def _analyze_color_psychology(self, colors: List[str]) -> Dict[str, float]:
    """Analyze psychological implications of colors"""
    psychology = {
        'energy': 0.0,
        'professionalism': 0.0,
        'creativity': 0.0,
        'approachability': 0.0
    }
    
    color_traits = {
        'red': {'energy': 90, 'professionalism': 40, 'creativity': 60, 'approachability': 50},
        'blue': {'energy': 40, 'professionalism': 90, 'creativity': 50, 'approachability': 70},
        'black': {'energy': 50, 'professionalism': 100, 'creativity': 40, 'approachability': 30},
        # Add more colors and their psychological traits
    }
    
    try:
        for color in colors:
            color = color.lower()
            if color in color_traits:
                for trait, value in color_traits[color].items():
                    psychology[trait] += value
        
        # Average the scores
        for trait in psychology:
            psychology[trait] /= len(colors) if colors else 1
    except Exception:
        pass
    
    return psychology

def _detect_patterns(self, person: Dict) -> Dict[str, Any]:
    """Detect clothing patterns"""
    pattern_info = {
        'type': 'solid',  # default
        'complexity': 0.0,
        'size': 0.0,
        'confidence': 0.0
    }
    
    try:
        labels = person.get('Labels', [])
        pattern_keywords = {
            'striped': 'stripes',
            'plaid': 'plaid',
            'floral': 'floral',
            'dotted': 'polka dots',
            'checked': 'checkered'
        }
        
        for label in labels:
            name = label['Name'].lower()
            for keyword, pattern in pattern_keywords.items():
                if keyword in name:
                    pattern_info['type'] = pattern
                    pattern_info['confidence'] = label['Confidence']
                    pattern_info['complexity'] = self._calculate_pattern_complexity_score(pattern)
                    pattern_info['size'] = self._estimate_pattern_size(label)
                    break
    except Exception:
        pass
    
    return pattern_info

def _calculate_pattern_complexity_score(self, pattern_type: str) -> float:
    """Calculate complexity score for different patterns"""
    complexity_scores = {
        'solid': 0.0,
        'stripes': 30.0,
        'polka dots': 50.0,
        'checkered': 70.0,
        'plaid': 80.0,
        'floral': 90.0
    }
    return complexity_scores.get(pattern_type, 0.0)

def _analyze_clothing_fit(self, person: Dict) -> Dict[str, float]:
    """Analyze how clothing fits the person"""
    fit_metrics = {
        'looseness': 0.0,
        'tightness': 0.0,
        'draping': 0.0,
        'tailoring_quality': 0.0
    }
    
    try:
        keypoints = person.get('KeyPoints', [])
        
        # Calculate clothing boundaries
        clothing_bounds = self._calculate_clothing_bounds(keypoints)
        
        # Analyze fit based on body landmarks and clothing boundaries
        fit_metrics['looseness'] = self._calculate_looseness(keypoints, clothing_bounds)
        fit_metrics['tightness'] = 100 - fit_metrics['looseness']
        fit_metrics['draping'] = self._analyze_draping(keypoints, clothing_bounds)
        fit_metrics['tailoring_quality'] = self._analyze_tailoring(keypoints, clothing_bounds)
    
    except Exception:
        pass
    
    return fit_metrics

def _calculate_style_scores(self, base_scores: Dict[str, float], 
                          colors: List[Dict], patterns: Dict, 
                          components: Dict, fit: Dict) -> Dict[str, float]:
    """Calculate scores for different style categories"""
    try:
        # Update casual score
        if components.get('top_style') in ['t-shirt', 'sweater'] or \
           components.get('bottom_style') in ['jeans', 'shorts']:
            base_scores['casual'] += 30
        
        # Update formal score
        if components.get('top_style') in ['suit jacket', 'blazer'] or \
           components.get('bottom_style') in ['suit pants', 'dress']:
            base_scores['formal'] += 30
        
        # Update athletic score
        if components.get('top_style') in ['athletic shirt', 'tank top'] or \
           components.get('bottom_style') in ['athletic shorts', 'leggings']:
            base_scores['athletic'] += 30
        
        # Update based on patterns
        if patterns.get('type') == 'floral':
            base_scores['bohemian'] += 20
        elif patterns.get('type') == 'solid':
            base_scores['minimalist'] += 20
        
        # Update based on fit
        if fit.get('tailoring_quality', 0) > 70:
            base_scores['formal'] += 20
        
        # Normalize scores to 0-100 range
        for style in base_scores:
            base_scores[style] = min(100, max(0, base_scores[style]))
    
    except Exception:
        pass
    
    return base_scores

def _calculate_clothing_bounds(self, keypoints: List[Dict]) -> Dict[str, float]:
    """Calculate boundaries of clothing from keypoints"""
    bounds = {
        'top': {'min_y': float('inf'), 'max_y': float('-inf')},
        'bottom': {'min_y': float('inf'), 'max_y': float('-inf')}
    }
    
    try:
        for point in keypoints:
            y = point['Y']
            if 'shoulder' in point['BodyPart'].lower() or 'chest' in point['BodyPart'].lower():
                bounds['top']['min_y'] = min(bounds['top']['min_y'], y)
                bounds['top']['max_y'] = max(bounds['top']['max_y'], y)
            elif 'hip' in point['BodyPart'].lower() or 'knee' in point['BodyPart'].lower():
                bounds['bottom']['min_y'] = min(bounds['bottom']['min_y'], y)
                bounds['bottom']['max_y'] = max(bounds['bottom']['max_y'], y)
    except Exception:
        pass
    
    return bounds

def _calculate_proportion_balance(self, fits: List[Dict]) -> float:
    """Calculate balance of proportions in outfit"""
    try:
        # Analyze ratio between top and bottom
        top_measurements = [fit.get('top_length', 0) for fit in fits]
        bottom_measurements = [fit.get('bottom_length', 0) for fit in fits]
        
        if top_measurements and bottom_measurements:
            avg_top = np.mean(top_measurements)
            avg_bottom = np.mean(bottom_measurements)
            
            # Calculate how close the ratio is to golden ratio (1.618)
            if avg_bottom > 0:
                ratio = avg_top / avg_bottom
                ideal_ratio = 1.618
                
                # Convert difference from ideal ratio to a score
                difference = abs(ratio - ideal_ratio)
                return max(0, 100 - (difference * 50))
    
    except Exception:
        pass
    
    return 50.0  # Default balance score
def _detect_top_style(self, person: Dict) -> Dict[str, Any]:
    """Detect style of top/upper body clothing"""
    try:
        labels = person.get('Labels', [])
        top_styles = {
            'shirt': ['t-shirt', 'shirt', 'blouse', 'top'],
            'jacket': ['jacket', 'blazer', 'coat'],
            'sweater': ['sweater', 'hoodie', 'sweatshirt'],
            'dress': ['dress', 'gown']
        }
        
        for label in labels:
            name = label['Name'].lower()
            for style, keywords in top_styles.items():
                if any(keyword in name for keyword in keywords):
                    return {
                        'style': style,
                        'confidence': label['Confidence']
                    }
        
        return {'style': 'unknown', 'confidence': 0.0}
    except Exception:
        return {'style': 'unknown', 'confidence': 0.0}

def _detect_bottom_style(self, person: Dict) -> Dict[str, Any]:
    """Detect style of bottom/lower body clothing"""
    try:
        labels = person.get('Labels', [])
        bottom_styles = {
            'pants': ['pants', 'trousers', 'jeans'],
            'skirt': ['skirt'],
            'shorts': ['shorts'],
            'leggings': ['leggings', 'tights']
        }
        
        for label in labels:
            name = label['Name'].lower()
            for style, keywords in bottom_styles.items():
                if any(keyword in name for keyword in keywords):
                    return {
                        'style': style,
                        'confidence': label['Confidence']
                    }
        
        return {'style': 'unknown', 'confidence': 0.0}
    except Exception:
        return {'style': 'unknown', 'confidence': 0.0}

def _detect_outerwear(self, person: Dict) -> Dict[str, Any]:
    """Detect type of outerwear"""
    try:
        labels = person.get('Labels', [])
        outerwear_types = {
            'coat': ['coat', 'overcoat', 'winter coat'],
            'jacket': ['jacket', 'windbreaker'],
            'blazer': ['blazer', 'suit jacket'],
            'cardigan': ['cardigan']
        }
        
        for label in labels:
            name = label['Name'].lower()
            for type_, keywords in outerwear_types.items():
                if any(keyword in name for keyword in keywords):
                    return {
                        'type': type_,
                        'confidence': label['Confidence']
                    }
        
        return {'type': 'none', 'confidence': 0.0}
    except Exception:
        return {'type': 'none', 'confidence': 0.0}

def _detect_footwear(self, person: Dict) -> Dict[str, Any]:
    """Detect style of footwear"""
    try:
        labels = person.get('Labels', [])
        footwear_styles = {
            'sneakers': ['sneakers', 'athletic shoes', 'sports shoes'],
            'dress_shoes': ['dress shoes', 'formal shoes', 'oxfords'],
            'boots': ['boots', 'ankle boots'],
            'sandals': ['sandals', 'flip flops']
        }
        
        for label in labels:
            name = label['Name'].lower()
            for style, keywords in footwear_styles.items():
                if any(keyword in name for keyword in keywords):
                    return {
                        'style': style,
                        'confidence': label['Confidence']
                    }
        
        return {'style': 'unknown', 'confidence': 0.0}
    except Exception:
        return {'style': 'unknown', 'confidence': 0.0}

def _count_accessories(self, person: Dict) -> int:
    """Count number of accessories"""
    try:
        labels = person.get('Labels', [])
        accessory_keywords = [
            'necklace', 'bracelet', 'watch', 'ring', 'earring',
            'belt', 'scarf', 'hat', 'glasses', 'bag'
        ]
        
        count = 0
        for label in labels:
            if any(keyword in label['Name'].lower() for keyword in accessory_keywords):
                count += 1
        
        return count
    except Exception:
        return 0
def analyze_location_setting(self, video_frames: List[Dict]) -> Dict[str, Any]:
    """Analyze the location and setting characteristics"""
    return {
        'environment_type': {
            'indoor_settings': self._analyze_indoor_settings(video_frames),
            'outdoor_settings': self._analyze_outdoor_settings(video_frames)
        }
    }

def _analyze_indoor_settings(self, video_frames: List[Dict]) -> Dict[str, Any]:
    """Analyze indoor setting characteristics"""
    return {
        'kitchen': self._analyze_kitchen_setting(video_frames),
        'living_room': self._analyze_living_room_setting(video_frames),
        'bedroom': self._analyze_bedroom_setting(video_frames),
        'bathroom': self._analyze_bathroom_setting(video_frames),
        'home_office': self._analyze_home_office_setting(video_frames),
        'gym': self._analyze_gym_setting(video_frames),
        'restaurant': self._analyze_restaurant_setting(video_frames),
        'retail_space': self._analyze_retail_setting(video_frames),
        'professional_setting': self._analyze_professional_setting(video_frames)
    }

def _analyze_kitchen_setting(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze kitchen environment characteristics"""
    kitchen_metrics = {
        'kitchen_size': 0.0,
        'appliance_modernity': 0.0,
        'counter_space': 0.0,
        'kitchen_organization': 0.0,
        'cooking_activity_level': 0.0
    }

    try:
        kitchen_detections = []
        for frame in video_frames:
            # Detect kitchen-related elements
            labels = frame.get('Labels', [])
            kitchen_elements = self._detect_kitchen_elements(labels)
            if kitchen_elements:
                kitchen_detections.append(kitchen_elements)

        if kitchen_detections:
            # Calculate kitchen size
            kitchen_metrics['kitchen_size'] = self._calculate_kitchen_size(kitchen_detections)
            
            # Analyze appliances
            kitchen_metrics['appliance_modernity'] = self._analyze_appliance_modernity(
                kitchen_detections
            )
            
            # Analyze counter space
            kitchen_metrics['counter_space'] = self._analyzlyze_counter_space(kitchen_detections)
            
            # Analyze organization
            kitchen_metrics['kitchen_organization'] = self._analyze_kitchen_organization(
                kitchen_detections
            )
            
            # Analyze cooking activity
            kitchen_metrics['cooking_activity_level'] = self._analyze_cooking_activity(
                kitchen_detections
            )

    except Exception as e:
        printint(f"Error in kitchen analysis: {str(e)}")

    return kitchen_metrics

def _analyze_living_room_setting(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze living room environment characteristics"""
    living_room_metrics = {
        'seating_arrangement': 0.0,
        'decor_style': 0.0,
        'natural_light_level': 0.0,
        'tv_presence': 0.0,
        'living_room_size': 0.0
    }

    try:
        living_room_detections = []
        for frame in video_frames:
            labels = frame.get('Labels', [])
            living_room_elements = self._detect_living_room_elements(labels)
            if living_room_elements:
                living_room_detections.append(living_room_elements)

        if living_room_detections:
            # Analyze seating arrangement
            living_room_metrics['seating_arrangement'] = self._analyze_seating_arrangement(
                living_room_detections
            )
            
            # Analyze decor
            living_room_metrics['decor_style'] = self._analyze_decor_style(
                living_room_detections
            )
            
            # Analyze lighting
            living_room_metrics['natural_light_level'] = self._analyze_natural_light(
                living_room_detections
            )
            
            # Detect TV
            living_room_metrics['tv_presence'] = self._detect_tv_presence(
                living_room_detections
            )
            
            # Calculate room size
            living_room_metrics['living_room_size'] = self._calculate_living_room_size(
                living_room_detections
            )

    except Exception as e:
        print(f"Error in living room analysis: {str(e)}")

    return living_room_metrics

def _analyze_bedroom_setting(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze bedroom environment characteristics"""
    bedroom_metrics = {
        'bed_size': 0.0,
        'bedroom_organization': 0.0,
        'personal_touches': 0.0,
        'sleeping_area_vs_workspace': 0.0,
        'bedroom_color_scheme': 0.0
    }

    try:
        bedroom_detections = []
        for frame in video_frames:
            labels = frame.get('Labels', [])
            bedroom_elements = self._detect_bedroom_elements(labels)
            if bedroom_elements:
                bedroom_detections.append(bedroom_elements)

        if bedroom_detections:
            # Analyze bed size
            bedroom_metrics['bed_size'] = self._analyze_bed_size(bedroom_detections)
            
            # Analyze organization
            bedroom_metrics['bedroom_organization'] = self._analyze_bedroom_organization(
                bedroom_detections
            )
            
            # Analyze personal elements
            bedroom_metrics['personal_touches'] = self._analyze_personal_elements(
                bedroom_detections
            )
            
            # Analyze space usage
            bedroom_metrics['sleeping_area_vs_workspace'] = self._analyze_bedroom_space_usage(
                bedroom_detections
            )
            
            # Analyze color scheme
            bedroom_metrics['bedroom_color_scheme'] = self._analyze_bedroom_colors(
                bedroom_detections
            )

    except Exception as e:
        print(f"Error in bedroom analysis: {str(e)}")

    return bedroom_metrics
def _analyze_gym_setting(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze gym environment characteristics"""
    gym_metrics = {
        'equipment_variety': 0.0,
        'gym_spaciousness': 0.0,
        'cleanliness_level': 0.0,
        'gym_occupancy': 0.0,
        'gym_lighting': 0.0
    }

    try:
        gym_detections = []
        for frame in video_frames:
            labels = frame.get('Labels', [])
            gym_elements = self._detect_gym_elements(labels)
            if gym_elements:
                gym_detections.append(gym_elements)

        if gym_detections:
            # Analyze equipment variety
            gym_metrics['equipment_variety'] = self._analyze_equipment_variety(
                gym_detections
            )
            
            # Analyze spaciousness
            gym_metrics['gym_spaciousness'] = self._analyze_gym_space(
                gym_detections
            )
            
            # Analyze cleanliness
            gym_metrics['cleanliness_level'] = self._analyze_gym_cleanliness(
                gym_detections
            )
            
            # Analyze occupancy
            gym_metrics['gym_occupancy'] = self._analyze_gym_occupancy(
                gym_detections
            )
            
            # Analyze lighting
            gym_metrics['gym_lighting'] = self._analyze_gym_lighting(
                gym_detections
            )

    except Exception as e:
        print(f"Error in gym analysis: {str(e)}")

    return gym_metrics

def _analyze_restaurant_setting(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze restaurant environment characteristics"""
    restaurant_metrics = {
        'seating_capacity': 0.0,
        'ambiance_type': 0.0,
        'table_arrangement': 0.0,
        'restaurant_theme_consistency': 0.0,
        'kitchen_visibility': 0.0
    }

    try:
        restaurant_detections = []
        for frame in video_frames:
            labels = frame.get('Labels', [])
            restaurant_elements = self._detect_restaurant_elements(labels)
            if restaurant_elements:
                restaurant_detections.append(restaurant_elements)

        if restaurant_detections:
            # Analyze seating capacity
            restaurant_metrics['seating_capacity'] = self._analyze_seating_capacity(
                restaurant_detections
            )
            
            # Analyze ambiance
            restaurant_metrics['ambiance_type'] = self._analyze_restaurant_ambiance(
                restaurant_detections
            )
            
            # Analyze table arrangement
            restaurant_metrics['table_arrangement'] = self._analyze_table_layout(
                restaurant_detections
            )
            
            # Analyze theme consistency
            restaurant_metrics['restaurant_theme_consistency'] = self._analyze_theme_consistestency(
                restaurant_detections
            )
            
            # Analyze kitchen visibility
            restaurant_metrics['kitchen_visibility'] = self._analyze_kitchen_visibility(
                restaurant_detections
            )

    except Exception as e:
        print(f"Error in restaurant analysis: {str(e)}")

    return restaurant_metrics

def _analyze_retail_setting(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze retail space characteristics"""
    retail_metrics = {
        'store_layout': 0.0,
        'product_display_method': 0.0,
        'checkout_area_visibility': 0.0,
        'store_branding_presence': 0.0,
        'customer_density': 0.0
    }

    try:
        retail_detections = []
        for frame in video_frames:
            labels = frame.get('Labels', [])
            retail_elements = self._detect_retail_elements(labels)
            if retail_elements:
                retail_detections.append(retail_elements)

        if retail_detections:
            # Analyze store layout
            retail_metrics['store_layout'] = self._analyze_store_layout(
                retail_detections
            )
            
            # Analyze product displays
            retail_metrics['product_display_method'] = self._analyze_product_displays(
                retail_detections
            )
            
            # Analyze checkout area
            retail_metrics['checkout_area_visibility'] = self._analyze_checkout_area(
                retail_detections
            )
            
            # Analyze branding
            retail_metrics['store_branding_presence'] = self._analyze_store_branding(
                retail_detections
            )
            
            # Analyze customer density
            retail_metrics['customer_density'] = self._analyze_customer_density(
                retail_detections
            )

    except Exception as e:
        print(f"Error in retail analysis: {str(e)}")

    return retail_metrics

def _analyze_professional_setting(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze professional environment characteristics"""
    professional_metrics = {
        'office_type': 0.0,
        'workspace_modernity': 0.0,
        'collaboration_spaces': 0.0,
        'professional_environment_formality': 0.0,
        'brand_presence_in_workspace': 0.0
    }

    try:
        professional_detections = []
        for frame in video_frames:
            labels = frame.get('Labels', [])
            professional_elements = self._detect_professional_elements(labels)
            if professional_elements:
                professional_detections.append(professional_elements)

        if professional_detections:
            # Analyze office type
            professional_metrics['office_type'] = self._analyze_office_type(
                professional_detections
            )
            
            # Analyze workspace modernity
            professional_metrics['workspace_modernity'] = self._analyze_workspace_modernity(
                professional_detections
            )
            
            # Analyze collaboration spaces
            professional_metrics['collaboration_spaces'] = self._analyze_collaboration_areas(
                professional_detections
            )
            
            # Analyze formality
            professional_metrics['professional_environment_formality'] = self._analyze_environment_formality(
                professional_detections
            )
            
            # Analyze brand presence
            professional_metrics['brand_presence_in_workspace'] = self._analyze_workspace_branding(
                professional_detections
            )

    except Exception as e:
        print(f"Error in professional setting analysis: {str(e)}")

    return professional_metrics
def _analyze_outdoor_settings(self, video_frames: List[Dict]) -> Dict[str, Any]:
    """Analyze outdoor setting characteristics"""
    return {
        'beach': self._analyze_beach_setting(video_frames),
        'park': self._analyze_park_setting(video_frames),
        'city_street': self._analyze_city_street_setting(video_frames),
        'rural_area': self._analyze_rural_setting(video_frames),
        'mountain': self._analyze_mountain_setting(video_frames),
        'forest': self._analyze_forest_setting(video_frames),
        'suburban_area': self._analyze_suburban_setting(video_frames)
    }

def _analyze_beach_setting(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze beach environment characteristics"""
    beach_metrics = {
        'sand_quality': 0.0,
        'water_visibility': 0.0,
        'beach_occupancy': 0.0,
        'natural_features': 0.0,
        'beach_activities_present': 0.0
    }

    try:
        beach_detections = []
        for frame in video_frames:
            labels = frame.get('Labels', [])
            if self._is_beach_setting(labels):
                beach_elements = self._detect_beach_elements(labels)
                beach_detections.append(beach_elements)

        if beach_detections:
            # Analyze sand quality
            beach_metrics['sand_quality'] = self._analyze_sand_characteristics(
                beach_detections
            )
            
            # Analyze water
            beach_metrics['water_visibility'] = self._analyze_water_visibility(
                beach_detections
            )
            
            # Analyze occupancy
            beach_metrics['beach_occupancy'] = self._analyze_beach_crowd_density(
                beach_detections
            )
            
            # Analyze natural features
            beach_metrics['natural_features'] = self._analyze_beach_features(
                beach_detections
            )
            
            # Analyze activities
            beach_metrics['beach_activities_present'] = self._detect_beach_activities(
                beach_detections
            )

    except Exception as e:
        print(f"Error in beach analysis: {str(e)}")

    return beach_metrics

def _analyze_park_setting(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze park environment characteristics"""
    park_metrics = {
        'greenery_level': 0.0,
        'park_facilities': 0.0,
        'path_types': 0.0,
        'park_size_indication': 0.0,
        'park_usage_diversity': 0.0
    }

    try:
        park_detections = []
        for frame in video_frames:
            labels = frame.get('Labels', [])
            if self._is_park_setting(labels):
                park_elements = self._detect_park_elements(labels)
                park_detections.append(park_elements)

        if park_detections:
            # Analyze greenery
            park_metrics['greenery_level'] = self._analyze_greenery_density(
                park_detections
            )
            
            # Analyze facilities
            park_metrics['park_facilities'] = self._analyze_park_facilities(
                park_detections
            )
            
            # Analyze paths
            park_metrics['path_types'] = self._analyze_path_characteristics(
                park_detections
            )
            
            # Analyze size
            park_metrics['park_size_indication'] = self._estimate_park_size(
                park_detections
            )
            
            # Analyze usage
            park_metrics['park_usage_diversity'] = self._analyze_park_activities(
                park_detections
            )

    except Exception as e:
        print(f"Error in park analysis: {str(e)}")

    return park_metrics

def _analyze_city_street_setting(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze city street environment characteristics"""
    street_metrics = {
        'building_density': 0.0,
        'street_busyness': 0.0,
        'urban_feature_types': 0.0,
        'street_cleanliness': 0.0,
        'city_skyline_visibility': 0.0
    }

    try:
        street_detections = []
        for frame in video_frames:
            labels = frame.get('Labels', [])
            if self._is_city_street_setting(labels):
                street_elements = self._detect_street_elements(labels)
                street_detections.append(street_elements)

        if street_detections:
            # Analyze building density
            street_metrics['building_density'] = self._analyze_building_density(
                street_detections
            )
            
            # Analyze street activity
            street_metrics['street_busyness'] = self._analyze_street_activity(
                street_detections
            )
            
            # Analyze urban features
            street_metrics['urban_feature_types'] = self._analyze_urban_features(
                street_detections
            )
            
            # Analyze cleanliness
            street_metrics['street_cleanliness'] = self._analyze_street_cleanliness(
                street_detections
            )
            
            # Analyze skyline
            street_metrics['city_skyline_visibility'] = self._analyze_skyline_visibility(
                street_detections
            )

    except Exception as e:
        print(f"Error in city street analysis: {str(e)}")

    return street_metrics

def _analyze_rural_setting(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze rural environment characteristics"""
    rural_metrics = {
        'landscape_type': 0.0,
        'vegetation_density': 0.0,
        'man-made_structure_presence': 0.0,
        'animal_presence': 0.0,
        'rural_activity_indicators': 0.0
    }

    try:
        rural_detections = []
        for frame in video_frames:
            labels = frame.get('Labels', [])
            if self._is_rural_setting(labels):
                rural_elements = self._detect_rural_elements(labels)
                rural_detections.append(rural_elements)

        if rural_detections:
            # Analyze landscape
            rural_metrics['landscape_type'] = self._analyze_landscape_type(
                rural_detections
            )
            
            # Analyze vegetation
            rural_metrics['vegetation_density'] = self._analyze_vegetation_coverage(
                rural_detections
            )
            
            # Analyze structures
            rural_metrics['man-made_structure_presence'] = self._analyze_rural_structures(
                rural_detections
            )
            
            # Analyze animals
            rural_metrics['animal_presence'] = self._detect_animal_presence(
                rural_detections
            )
            
            # Analyze activities
            rural_metrics['rural_activity_indicators'] = self._analyze_rural_activities(
                rural_detections
            )

    except Exception as e:
        print(f"Error in rural analysis: {str(e)}")

    return rural_metrics
def _analyze_mountain_setting(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze me mountain environment characteristics"""
    mountain_metrics = {
        'mountain_range_visibility': 0.0,
        'elevation_indication': 0.0,
        'terrain_type': 0.0,
        'mountain_activity_presence': 0.0,
        'weather_conditions': 0.0
    }

    try:
        mountain_detections = []
        for frame in video_frames:
            labels = frame.get('Labels', [])
            if self._is_mountain_setting(labels):
                mountain_elements = self._detect_mountain_elements(labels)
                mountain_detections.append(mountain_elements)

        if mountain_detections:
            # Analyze mountain visibility
            mountain_metrics['mountain_range_visibility'] = self._analyze_mountain_visibility(
                mountain_detections
            )
            
            # Analyze elevation
            mountain_metrics['elevation_indication'] = self._analyze_elevation_indicators(
                mountain_detections
            )
            
            # Analyze terrain
            mountain_metrics['terrain_type'] = self._analyze_terrain_characteristics(
                mountain_detections
            )
            
            # Analyze activities
            mountain_metrics['mountain_activity_presence'] = self._detect_mountain_activities(
                mountain_detections
            )
            
            # Analyze weather
            mountain_metrics['weather_conditions'] = self._analyze_mountain_weather(
                mountain_detections
            )

    except Exception as e:
        print(f"Error in mountain analysis: {str(e)}")

    return mountain_metrics

def _analyze_forest_setting(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze forest environment characteristics"""
    forest_metrics = {
        'tree_density': 0.0,
        'forest_type': 0.0,
        'undergrowth_level': 0.0,
        'forest_light_filtering': 0.0,
        'wildlife_indicators': 0.0
    }

    try:
        forest_detections = []
        for frame in video_frames:
            labels = frame.get('Labels', [])
            if self._is_forest_setting(labels):
                forest_elements = self._detect_forest_elements(labels)
                forest_detections.append(forest_elements)

        if forest_detections:
            # Analyze tree density
            forest_metrics['tree_density'] = self._analyze_tree_density(
                forest_detections
            )
            
            # Analyze forest type
            forest_metrics['forest_type'] = self._determine_forest_type(
                forest_detections
            )
            
            # Analyze undergrowth
            forest_metrics['undergrowth_level'] = self._analyze_undergrowrowth(
                forest_detections
            )
            
            # Analyze light
            forest_metrics['forest_light_filtering'] = self._analyze_forest_lighting(
                forest_detections
            )
            
            # Analyze wildlife
            forest_metrics['wildlife_indicators'] = self._detect_wildlife_presence(
                forest_detections
            )

    except Exception as e:
        print(f"Error in forest analysis: {str(e)}")

    return forest_metrics

def _analyze_suburban_setting(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze suburban environment characteristics"""
    suburban_metrics = {
        'house_types': 0.0,
        'yard_presence': 0.0,
        'street_layout': 0.0,
        'community_features': 0.0,
        'suburban_activity_level': 0.0
    }

    try:
        suburban_detections = []
        for frame in video_frames:
            labels = frame.get('Labels', [])
            if self._is_suburban_setting(labels):
                suburban_elements = self._detect_suburban_elements(labels)
                suburban_detections.append(suburban_elements)

        if suburban_detections:
            # Analyze house types
            suburban_metrics['house_types'] = self._analyze_house_types(
                suburban_detections
            )
            
            # Analyze yards
            suburban_metrics['yard_presence'] = self._analyze_yard_characteristics(
                suburban_detections
            )
            
            # Analyze street layout
            suburban_metrics['street_layout'] = self._analyze_street_patterns(
                suburban_detections
            )
            
            # Analyze community features
            suburban_metrics['community_features'] = self._analyze_community_elements(
                suburban_detections
            )
            
            # Analyze activity level
            suburban_metrics['suburban_activity_level'] = self._analyze_suburban_activity(
                suburban_detections
            )

    except Exception as e:
        print(f"Error in suburban analysis: {str(e)}")

    return suburban_metrics

# Helper methods for setting detection
def _is_mountain_setting(self, labels: List[Dict]) -> bool:
    """Determine if the setting is mountainous"""
    mountain_keywords = ['mountain', 'peak', 'cliff', 'ridge', 'alpine']
    return any(
        any(keyword in label['Name'].lower() for keyword in mountain_keywords)
        for label in labels
    )

def _is_forest_setting(self, labels: List[Dict]) -> bool:
    """Determine if the setting is a forest"""
    forest_keywords = ['forest', 'woods', 'trees', 'woodland', 'grove']
    return any(
        any(keyword in label['Name'].lower() for keyword in forest_keywords)
        for label in labels
    )

def _is_suburban_setting(self, labels: List[Dict]) -> bool:
    """Determine if the setting is suburban"""
    suburban_keywords = ['suburb', 'residential', 'neighborhood', 'house', 'yard']
    return any(
        any(keyword in label['Name'].lower() for keyword in suburban_keywords)
        for label in labels
    )

# Helper methods for element detection
def _detect_mountain_elements(self, labels: List[Dict]) -> Dict[str, Any]:
    """Detect mountain-specific elements"""
    elements = {
        'peaks': [],
        'snow_coverage': 0.0,
        'rock_formations': [],
        'vegetation': [],
        'trails': []
    }
    
    for label in labels:
        name = label['Name'].lower()
        confidence = label['Confidence']
        
        if 'peak' in name or 'mountain' in name:
            elements['peaks'].append({
                'type': name,
                'confidence': confidence
            })
        elif 'snow' in name:
            elements['snow_coverage'] = max(elements['snow_coverage'], confidence)
        elif 'rock' in name or 'boulder' in name:
            elements['rock_formations'].append({
                'type': name,
                'confidence': confidence
            })
        elif 'tree' in name or 'plant' in name:
            elements['vegetation'].append({
                'type': name,
                'confidence': confidence
            })
        elif 'trail' in name or 'path' in name:
            elements['trails'].append({
                'type': name,
                'confidence': confidence
            })
    
    return elements
from typing import List, Dict, Any

def _detect_forest_elements(self, labels: List[Dict]) -> Dict[str, Any]:
    """Detect forest-specific elements"""
    elements = {
        'tree_types': [],
        'ground_cover': [],
        'wildlife': [],
        'light_conditions': 0.0,
        'density_indicators': []
    }
    
    for label in labels:
        name = label['Name'].lower()
        confidence = label['Confidence']
        
        if 'tree' in name or 'pine' in name or 'oak' in name:
            elements['tree_types'].append({
                'type': name,
                'confidence': confidence
            })
        elif 'moss' in name or 'fern' in name or 'undergrowth' in name:
            elements['ground_cover'].append({
                'type': name,
                'confidence': confidence
            })
        elif any(animal in name for animal in ['bird', 'deer', 'fox', 'animal']):
            elements['wildlife'].append({
                'type': name,
                'confidence': confidence
            })
            
    return elements

def _detect_suburban_elements(self, labels: List[Dict]) -> Dict[str, Any]:
    """Detect suburban-specific elements"""
    elements = {
        'houses': [],
        'yards': [],
        'streets': [],
        'community_amenities': [],
        'vehicles': []
    }
    
    for label in labels:
        name = label['Name'].lower()
        confidence = label['Confidence']
        
        if 'house' in name or 'home' in name:
            elements['houses'].append({
                'style': name,
                'confidence': confidence
            })
        elif 'yard' in name or 'lawn' in name or 'garden' in name:
            elements['yards'].append({
                'type': name,
                'confidence': confidence
            })
        elif 'street' in name or 'road' in name:
            elements['streets'].append({
                'type': name,
                'confidence': confidence
            })
            
    return elements

def _analyze_mountain_visibility(self, detections: List[Dict]) -> float:
    """Analyze visibility of mountain features"""
    visibility_score = 0.0
    peak_confidences = []
    
    for detection in detections:
        for peak in detection.get('peaks', []):
            peak_confidences.append(peak['confidence'])
    
    if peak_confidences:
        visibility_score = sum(peak_confidences) / len(peak_confidences)
    
    return visibility_score

def _analyze_elevation_indicators(self, detections: List[Dict]) -> float:
    """Analyze indicators of elevation"""
    elevation_score = 0.0
    indicators = {
        'snow': 0.8,
        'cloud': 0.6,
        'cliff': 0.7,
        'ridgedge': 0.5
    }
    
    for detection in detections:
        for peak in detection.get('peaks', []):
            for indicator, weight in indicators.items():
                if indicator in peak['type']:
                    elevation_score = max(elevation_score, peak['confidence'] * weight)
    
    return elevation_score

def _analyze_terrain_characteristics(self, detections: List[Dict]) -> Dict[str, float]:
    """Analyze terrain characteristics"""
    terrain_types = {
        'rocky': 0.0,
        'snowy': 0.0,
        'vegetated': 0.0,
        'steep': 0.0
    }
    
    for detection in detections:
        for formation in detection.get('rock_formations', []):
            terrain_types['rocky'] = max(terrain_types['rocky'], formation['confidence'])
        
        if detection.get('snow_coverage', 0) > terrain_types['snowy']:
            terrain_types['snowy'] = detection['snow_coverage']
            
        for vegetation in detection.get('vegetation', []):
            terrain_types['vegetated'] = max(terrain_types['vegetated'], vegetation['confidence'])
    
    return terrain_types

def _analyze_tree_density(self, detections: List[Dict]) -> float:
    """Analyze density of trees in forest"""
    density_score = 0.0
    tree_counts = []
    
    for detection in detections:
        tree_count = len(detection.get('tree_types', []))
        tree_counts.append(tree_count)
    
    if tree_counts:
        max_trees = max(tree_counts)
        density_score = min(100, (max_trees / 10) * 100)  # Normalize to 0-100
    
    return density_score

def _determine_forest_type(self, detections: List[Dict]) -> Dict[str, float]:
    """Determine the type of forest"""
    forest_types = {
        'deciduous': 0.0,
        'coniferous': 0.0,
        'mixed': 0.0,
        'tropical': 0.0
    }
    
    for detection in detections:
        for tree in detection.get('tree_types', []):
            tree_type = tree['type']
            confidence = tree['confidence']
            
            if any(t in tree_type for t in ['pine', 'spruce', 'fir']):
                forest_types['coniferous'] += confidence
            elif any(t in tree_type for t in ['oak', 'maple', 'birch']):
                forest_types['deciduous'] += confidence
            elif any(t in tree_type for t in ['palm', 'banana']):
                forest_types['tropical'] += confidence
    
    # Normalize scores
    total = sum(forest_types.values())
    if total > 0:
        for type_ in forest_types:
            forest_types[type_] = (forest_types[type_] / total) * 100
            
        # Check for mixed forest
        if forest_types['deciduous'] > 25 and forest_types['coniferous'] > 25:
            forest_types['mixed'] = (forest_types['deciduous'] + forest_types['coniferous']) / 2
    
    return forest_types

def _analyze_undergrowth(self, detections: List[Dict]) -> float:
    """Analyze forest undergrowth characteristics"""
    undergrowth_score = 0.0
    
    for detection in detections:
        ground_cover = detection.get('ground_cover', [])
        for cover in ground_cover:
            undergrowth_score = max(undergrowth_score, cover['confidence'])
    
    return undergrowth_score

def _analyze_forest_lighting(self, detections: List[Dict]) -> Dict[str, float]:
    """Analyze light conditions in forest"""
    lighting_metrics = {
        'brightness': 0.0,
        'shadow_patterns': 0.0,
        'light_penetration': 0.0
    }
    
    # Implementation would depend on specific Rekognition capabilities
    # This is a placeholder for the actual light analysis logic
    
    return lighting_metrics

def _analyze_house_types(self, detections: List[Dict]) -> Dict[str, float]:
    """Analyze types of houses in suburban setting"""
    house_types = {
        'single_family': 0.0,
        'townhouse': 0.0,
        'duplex': 0.0,
        'modern': 0.0,
        'traditional': 0.0
    }
    
    for detection in detections:
        for house in detection.get('houses', []):
            style = house['style']
            confidence = house['confidence']
            
            if 'single' in style or 'detached' in style:
                house_types['single_family'] += confidence
            elif 'town' in style:
                house_types['townhouse'] += confidence
            elif 'duplex' in style:
                house_types['duplex'] += confidence
            elif any(m in style for m in ['modern', 'contemporary']):
                house_types['modern'] += confidence
            elif any(t in style for t in ['traditional', 'colonial']):
                house_types['traditional'] += confidence
    
    # Normalize scores
    total = sum(house_types.values())
    if total > 0:
        for type_ in house_types:
            house_types[type_] = (house_types[type_] / total) * 100
    
    return house_types
from typing import List, Dict

def _analyze_yard_characteristics(self, detections: List[Dict]) -> Dict[str, float]:
    """Analyze yard characteristics in suburban setting"""
    yard_metrics = {
        'size': 0.0,
        'landscaping_quality': 0.0,
        'maintenance_level': 0.0,
        'features_present': [],
        'greenery_density': 0.0
    }
    
    for detection in detections:
        for yard in detection.get('yards', []):
            # Analyze yard size
            if 'large' in yard['type']:
                yard_metrics['size'] = max(yard_metrics['size'], 80.0)
            elif 'medium' in yard['type']:
                yard_metrics['size'] = max(yard_metrics['size'], 50.0)
            elif 'small' in yard['type']:
                yard_metrics['size'] = max(yard_metrics['size'], 30.0)
                
            # Track features
            features = []
            if 'garden' in yard['type']:
                features.append({'type': 'garden', 'confidence': yard['confidence']})
            if 'pool' in yard['type']:
                features.append({'type': 'pool', 'confidence': yard['confidence']})
            if 'fence' in yard['type']:
                features.append({'type': 'fence', 'confidence': yard['confidence']})
                
            yard_metrics['features_present'].extend(features)
            
            # Analyze landscaping and maintenance
            yard_metrics['landscaping_quality'] = self._evaluate_landscaping(yard)
            yard_metrics['maintenance_level'] = self._evaluate_maintenance(yard)
            yard_metrics['greenery_density'] = self._calculate_greenery_density(yard)
    
    return yard_metrics

def _analyze_street_patterns(self, detections: List[Dict]) -> Dict[str, float]:
    """Analyze street layout patterns in suburban areas"""
    street_metrics = {
        'layout_type': '',
        'connectivity': 0.0,
        'width': 0.0,
        'sidewalk_presence': 0.0,
        'traffic_patterns': []
    }
    
    street_layouts = {
        'grid': 0.0,
        'curved': 0.0,
        'cul_de_sac': 0.0,
        'mixed': 0.0
    }
    
    for detection in detections:
        for street in detection.get('streets', []):
            # Determine street layout type
            layout_score = self._determine_street_layout(street)
            for layout_type, score in layout_score.items():
                street_layouts[layout_type] += score
                
            # Analyze street characteristics
            street_metrics['width'] = max(
                street_metrics['width'],
                self._analyze_street_width(street)
            )
            street_metrics['sidewalk_presence'] += self._detect_sidewalks(street)
            
            # Analyze traffic patterns
            traffic_pattern = self._analyze_traffic_pattern(street)
            if traffic_pattern:
                street_metrics['traffic_patterns'].append(traffic_pattern)
    
    # Determine dominant layout type
    street_metrics['layout_type'] = max(
        street_layouts.items(),
        key=lambda x: x[1]
    )[0]
    
    return street_metrics

def _analyze_community_elements(self, detections: List[Dict]) -> Dict[str, float]:
    """Analyze community features in suburban setting"""
    community_metrics = {
        'parks_and_recreation': 0.0,
        'schools': 0.0,
        'commercial_areas': 0.0,
        'public_spaces': 0.0,
        'community_facilities': []
    }
    
    for detection in detections:
        # Analyze community amenities
        for amenity in detection.get('community_amenities', []):
            if 'park' in amenity['type'] or 'playground' in amenity['type']:
                community_metrics['parks_and_recreation'] = max(
                    community_metrics['parks_and_recreation'],
                    amenity['confidence']
                )
            elif 'school' in amenity['type']:
                community_metrics['schools'] = max(
                    community_metrics['schools'],
                    amenity['confidence']
                )
            elif 'store' in amenity['type'] or 'shop' in amenity['type']:
                community_metrics['commercial_areas'] = max(
                    community_metrics['commercial_areas'],
                    amenity['confidence']
                )
            
            community_metrics['community_facilities'].append({
                'type': amenity['type'],
                'confidence': amenity['confidence']
            })
    
    return community_metrics

def _analyze_suburban_activity(self, detections: List[Dict]) -> Dict[str, float]:
    """Analyze activity levels and types in suburban areas"""
    activity_metrics = {
        'pedestrian_activity': 0.0,
        'vehicular_traffic': 0.0,
        'recreational_activity': 0.0,
        'time_of_day_patterns': [],
        'community_interaction_level': 0.0
    }
    
    for detection in detections:
        # Analyze people presence
        pedestrians = self._detect_pedestrians(detection)
        activity_metrics['pedestrian_activity'] = max(
            activity_metrics['pedestrian_activity'],
            self._calculate_activity_level(pedestrians)
        )
        
        # Analyze vehicle presence
        vehicles = detection.get('vehicles', [])
        activity_metrics['vehicular_traffic'] = max(
            activity_metrics['vehicular_traffic'],
            self._calculate_traffic_level(vehicles)
        )
        
        # Analyze recreational activities
        rec_activities = self._detect_recreational_activities(detection)
        activity_metrics['recreational_activity'] = max(
            activity_metrics['recreational_activity'],
            self._calculate_recreation_level(rec_activities)
        )
        
        # Analyze time patterns
        time_pattern = self._analyze_time_of_day_activity(detection)
        if time_pattern:
            activity_metrics['time_of_day_patterns'].append(time_pattern)
        
        # Calculate community interaction
        activity_metrics['community_interaction_level'] = self._calculate_interaction_level(
            pedestrians, rec_activities
        )
    
    return activity_metrics

def _evaluate_landscaping(self, yard: Dict) -> float:
    """Evaluate landscaping quality"""
    quality_score = 0.0
    
    features = {
        'garden': 0.3,
        'trees': 0.2,
        'flowers': 0.2,
        'lawn': 0.2,
        'decorative': 0.1
    }
    
    for feature, weight in features.items():
        if feature in yard['type'].lower():
            quality_score += weight * yard['confidence']
    
    return min(100, quality_score * 100)

def _calculate_greenery_density(self, yard: Dict) -> float:
    """Calculate density of greenery in yard"""
    vegetation_keywords = ['grass', 'tree', 'bush', 'plant', 'flower']
    density_score = 0.0
    
    for keyword in vegetation_keywords:
        if keyword in yard['type'].lower():
            density_score += yard['confidence']
    
    return min(100, density_score)

def _determine_street_layout(self, street: Dict) -> Dict[str, float]:
    """Determine type of street layout"""
    layout_scores = {
        'grid': 0.0,
        'curved': 0.0,
        'cul_de_sac': 0.0,
        'mixed': 0.0
    }
    
    street_type = street['type'].lower()
    confidence = street['confidence']
    
    if 'straight' in street_type or 'grid' in street_type:
        layout_scores['grid'] = confidence
    elif 'curved' in street_type or 'winding' in street_type:
        layout_scores['curved'] = confidence
    elif 'cul' in street_type or 'dead end' in street_type:
        layout_scores['cul_de_sac'] = confidence
    else:
        layout_scores['mixed'] = confidence
    
    return layout_scores

from typing import List, Dict

def _detect_sidewalks(self, street: Dict) -> float:
    """Detect presence and quality of sidewalks"""
    if 'sidewalk' in street['type'].lower():
        return street['confidence']
    return 0.0

def _analyze_street_width(self, street: Dict) -> float:
    """Analyze width of street"""
    width_indicators = {
        'wide': 100.0,
        'medium': 60.0,
        'narrow': 30.0
    }
    
    for indicator, score in width_indicators.items():
        if indicator in street['type'].lower():
            return score * (street['confidence'] / 100)
    return 50.0  # default medium width

def _analyze_traffic_pattern(self, street: Dict) -> Dict[str, float]:
    """Analyze traffic patterns on street"""
    return {
        'volume': self._estimate_traffic_volume(street),
        'flow': self._analyze_traffic_flow(street),
        'time_of_day': self._detect_time_patterns(street),
        'vehicle_types': self._analyze_vehicle_types(street)
    }

def _detect_pedestrians(self, detection: Dict) -> List[Dict]:
    """Detect pedestrians in the scene"""
    pedestrians = []
    
    for label in detection.get('Labels', []):
        if 'person' in label['Name'].lower() or 'pedestrian' in label['Name'].lower():
            pedestrians.append({
                'confidence': label['Confidence'],
                'location': label.get('BoundingBox', {}),
                'activity': self._determine_pedestrian_activity(label)
            })
    
    return pedestrians

def _calculate_activity_level(self, pedestrians: List[Dict]) -> float:
    """Calculate overall activity ly level based on pedestrian presence"""
    if not pedestrians:
        return 0.0
        
    total_confidence = sum(ped['confidence'] for ped in pedestrians)
    activity_score = min(100, (len(pedestrians) * total_confidence) / 100)
    return activity_score

def _calculate_traffic_level(self, vehicles: List[Dict]) -> float:
    """Calculate traffic level based on vehicle presence"""
    if not vehicles:
        return 0.0
        
    vehicle_weights = {
        'car': 1.0,
        'truck': 1.5,
        'bus': 2.0,
        'motorcycle': 0.5,
        'bicycle': 0.3
    }
    
    weighted_sum = sum(
        vehicle_weights.get(v['type'], 1.0) * v['confidence']
        for v in vehicles
    )
    
    return min(100, weighted_sum / len(vehicles))

def _detect_recreational_activities(self, detection: Dict) -> List[Dict]:
    """Detect recreational activities in the scene"""
    activities = []
    recreation_keywords = [
        'playing', 'walking', 'jogging', 'biking', 'sports',
        'gardening', 'exercising', 'dog walking'
    ]
    
    for label in detection.get('Labels', []):
        for keyword in recreation_keywords:
            if keyword in label['Name'].lower():
                activities.append({
                    'type': keyword,
                    'confidence': label['Confidence']
                })
    
    return activities

def _analyze_time_of_day_activity(self, detection: Dict) -> Dict[str, float]:
    """Analyze activity patterns based on time of day"""
    return {
        'morning_activity': self._estimate_time_activity(detection, 'morning'),
        'afternoon_activity': self._estimate_time_activity(detection, 'afternoon'),
        'evening_activity': self._estimate_time_activity(detection, 'evening')
    }

def _calculate_interaction_level(self, pedestrians: List[Dict], activities: List[Dict]) -> float:
    """Calculate level of community interaction"""
    interaction_score = 0.0
    
    # Score based on group sizes
    if pedestrians:
        groups = self._identify_pedestrian_groups(pedestrians)
        interaction_score += len(groups) * 20  # 20 points per group
    
    # Score based on activities
    social_activities = [
        act for act in activities
        if any(keyword in act['type'] for keyword in ['playing', 'sports', 'walking', 'gathering'])
    ]
    interaction_score += len(social_activities) * 15  # 15 points per social activity
    
    return min(100, interaction_score)

def _estimate_time_activity(self, detection: Dict, time_period: str) -> float:
    """Estimate activity level for specific time period"""
    # Time-specific indicators
    time_indicators = {
        'morning': ['sunrise', 'breakfast', 'commute', 'school'],
        'afternoon': ['lunch', 'shopping', 'bright', 'sunny'],
        'evening': ['sunset', 'dinner', 'dusk', 'return']
    }
    
    activity_score = 0.0
    indicators = time_indicators.get(time_period, [])
    
    for label in detection.get('Labels', []):
        if any(indicator in label['Name'].lower() for indicator in indicators):
            activity_score += label['Confidence']
    
    return min(100, activity_score)

def _identify_pedestrian_groups(self, pedestrians: List[Dict]) -> List[List[Dict]]:
    """Identify groups of pedestrians based on proximity"""
    groups = []
    processed = set()
    
    for i, ped1 in enumerate(pedestrians):
        if i in processed:
            continue
            
        current_group = [ped1]
        processed.add(i)
        
        for j, ped2 in enumerate(pedestrians):
            if j in processed:
                continue
                
            if self._are_pedestrians_grouped(ped1, ped2):
                current_group.append(ped2)
                processed.add(j)
        
        if current_group:
            groups.append(current_group)
    
    return groups

def _are_pedestrians_grouped(self, ped1: Dict, ped2: Dict) -> bool:
    """Determine if two pedestrians are part of the same group"""
    # Calculate distance between pedestrians using bounding boxes
    box1 = ped1['location']
    box2 = ped2['location']
    
    if not box1 or not box2:
        return False
        
    # Calculate centers
    center1 = (box1['Left'] + box1['Width']/2, box1['Top'] + box1['Height']/2)
    center2 = (box2['Left'] + box2['Width']/2, box2['Top'] + box2['Height']/2)
    
    # Calculate distance
    distance = ((center1[0] - center2[0])**2 + (center1[1] - center2[1])**2)**0.5
    
    # Consider grouped if within threshold
    GROUP_THRESHOLD = 0.2  # 20% of image width/height
    return distance < GROUP_THRESHOLD

def _determine_pedestrian_activity(self, label: Dict) -> str:
    """Determine what activity a pedestrian is engaged in"""
    activity_keywords = {
        'walking': ['walking', 'strolling', 'moving'],
        'running': ['running', 'jogging', 'sprinting'],
        'standing': ['standing', 'waiting', 'stationary'],
        'sitting': ['sitting', 'seated', 'resting'],
        'playing': ['playing', 'sports', 'recreation']
    }
    
    for activity, keywords in activity_keywords.items():
        if any(keyword in label['Name'].lower() for keyword in keywords):
            return activity
    
    return 'unknown'
def _estimate_traffic_volume(self, street: Dict) -> float:
    """Estimate traffic volume on street"""
    volume_score = 0.0
    
    # Check for traffic indicators
    traffic_indicators = {
        'heavy_traffic': 100.0,
        'moderate_traffic': 60.0,
        'light_traffic': 30.0,
        'vehicle': 10.0
    }
    
    for indicator, score in traffic_indicators.items():
        if indicator in street['type'].lower():
            volume_score = max(volume_score, score * (street['confidence'] / 100))
    
    return volume_score

def _analyze_traffic_flow(self, street: Dict) -> Dict[str, float]:
    """Analyze traffic flow patterns"""
    return {
        'speed': self._estimate_traffic_speed(street),
        'congestion': self._estimate_congestion(street),
        'direction': self._determine_traffic_direction(street),
        'flow_consistency': self._analyze_flow_consistency(street)
    }

def _detect_time_patterns(self, street: Dict) -> Dict[str, float]:
    """Detect time-based traffic patterns"""
    return {
        'peak_hours': self._identify_peak_hours(street),
        'off_peak': self._identify_off_peak(street),
        'day_night_variation': self._analyze_day_night_patterns(street)
    }

def _analyze_vehicle_types(self, street: Dict) -> Dict[str, int]:
    """Analyze types of vehicles present"""
    vehicle_counts = {
        'cars': 0,
        'trucks': 0,
        'buses': 0,
        'bicycles': 0,
        'motorcycles': 0
    }
    
    for label in street.get('Labels', []):
        vehicle_type = label['Name'].lower()
        if 'car' in vehicle_type:
            vehicle_counts['cars'] += 1
        elif 'truck' in vehicle_type:
            vehicle_counts['trucks'] += 1
        elif 'bus' in vehicle_type:
            vehicle_counts['buses'] += 1
        elif 'bicycle' in vehicle_type:
            vehicle_counts['bicycles'] += 1
        elif 'motorcycle' in vehicle_type:
            vehicle_counts['motorcycles'] += 1
    
    return vehicle_counts

def _estimate_traffic_speed(self, street: Dict) -> float:
    """Estimate traffic speed based on visual cues"""
    speed_score = 50.0  # Default medium speed
    
    speed_indicators = {
        'stopped': 0.0,
        'slow': 25.0,
        'moderderate': 50.0,
        'fast': 75.0,
        'rapid': 100.0
    }
    
    for indicator, score in speed_indicators.items():
        if indicator in street['type'].lower():
            speed_score = score
            break
    
    return speed_score

def _estimate_congestion(self, street: Dict) -> float:
    """Estimate traffic congestion level"""
    congestion_score = 0.0
    
    # Check for congestion indicators
    congestion_indicators = {
        'congested': 100.0,
        'heavy': 75.0,
        'moderate': 50.0,
        'light': 25.0,
        'free_flowing': 0.0
    }
    
    for indicator, score in congestion_indicators.items():
        if indicator in street['type'].lower():
            congestion_score = score
            break
    
    return congestion_score

def _determine_traffic_direction(self, street: Dict) -> Dict[str, float]:
    """Determine traffic flow direction"""
    return {
        'northbound': self._estimate_directional_flow(street, 'north'),
        'southbound': self._estimate_directional_flow(street, 'south'),
        'eastbound': self._estimate_directional_flow(street, 'east'),
        'westbound': self._estimate_directional_flow(street, 'west')
    }

def _analyze_flow_consistency(self, street: Dict) -> float:
    """Analyze consistency of traffic flow"""
    consistency_score = 100.0
    
    # Reduce score based on flow disruption indicators
    disruption_indicators = {
        'stop': -20.0,
        'intersection': -15.0,
        'crossing': -10.0,
        'traffic_light': -5.0
    }
    
    for indicator, penalty in disruption_indicators.items():
        if indicator in street['type'].lower():
            consistency_score += penalty
    
    return max(0.0, consistency_score)

def _identify_peak_hours(self, street: Dict) -> List[Dict]:
    """Identify peak traffic hours"""
    peak_periods = []
    
    # Common peak period indicators
    peak_indicators = {
        'morning_rush': {
            'start_time': '07:00',
            'end_time': '09:00',
            'confidence': 0.0
        },
        'evening_rush': {
            'start_time': '16:00',
            'end_time': '18:00',
            'confidence': 0.0
        }
    }
    
    for period, data in peak_indicators.items():
        if self._detect_peak_period_indicators(street, period):
            peak_periods.append({
                'period': period,
                'start_time': data['start_time'],
                'end_time': data['end_time'],
                'confidence': street['confidence']
            })
    
    return peak_periods

def _detect_peak_period_indicators(self, street: Dict, period: str) -> bool:
    """Detect indicators of peak traffic periods"""
    period_indicators = {
        'morning_rush': ['commute', 'morning', 'rush_hour'],
        'evening_rush': ['evening', 'return', 'rush_hour']
    }
    
    return any(
        indicator in street['type'].lower()
        for indicator in period_indicators.get(period, [])
    )

def _estimate_directional_flow(self, street: Dict, direction: str) -> float:
    """Estimate traffic flow in a specific direction"""
    flow_score = 0.0
    
    direction_indicators = {
        'north': ['northbound', 'north'],
        'south': ['southbound', 'south'],
        'east': ['eastbound', 'east'],
        'west': ['westbound', 'west']
    }
    
    for indicator in direction_indicators.get(direction, []):
        if indicator in street['type'].lower():
            flow_score = street['confidence']
            break
    
    return flow_score

def _analyze_day_night_patterns(self, street: Dict) -> Dict[str, float]:
    """Analyze traffic patterns during day vs night"""
    return {
        'daytime_activity': self._estimate_daytime_traffic(street),
        'nighttime_activity': self._estimate_nighttime_traffic(street),
        'pattern_consistency': self._analyze_pattern_consistency(street)
    }

def _estimate_daytime_traffic(self, street: Dict) -> float:
    """Estimate daytime traffic levels"""
    if 'daytime' in street['type'].lower() or 'day' in street['type'].lower():
        return street['confidence']
    return 50.0  # Default medium traffic

def _estimate_nighttime_traffic(self, street: Dict) -> float:
    """Estimate nighttime traffic levels"""
    if 'nighttime' in street['type'].lower() or 'night' in street['type'].lower():
        return street['confidence']
    return 30.0  # Default lower traffic

def _analyze_pattern_consistency(self, street: Dict) -> float:
    """Analyze consistency of traffic patterns"""
    consistency_score = 100.0
    
    # Reduce score based on pattern variation indicators
    variation_indicators = {
        'variable': -30.0,
        'inconsistent': -40.0,
        'irregular': -20.0,
        'fluctuating': -25.0
    }
    
    for indicator, penalty in variation_indicators.items():
        if indicator in street['type'].lower():
            consistency_score += penalty
    
    return max(0.0, consistency_score)
from typing import List, Dict, Any

def analyze_content_structure(self, video_frames: List[Dict]) -> Dict[str, Any]:
    """Analyze overall content structure"""
    return {
        'pacing_and_rhythm': {
            'opening_sequence': self._analyze_opening_sequence(video_frames),
            'content_flow': self._analyze_content_flow(video_frames),
            'conclusion_and_call_to_action': self._analyze_conclusion_cta(video_frames)
        }
    }

def _analyze_opening_sequence(self, video_frames: List[Dict]) -> Dict[str, Any]:
    """Analyze opening sequence characteristics"""
    return {
        'hook_effectiveness': self._analyze_hook_effectiveness(video_frames),
        'introduction_pacing': self._analyze_introduction_pacing(video_frames)
    }

def _analyze_hook_effectiveness(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze effectiveness of content hook"""
    hook_metrics = {
        'attention_grab_speed': 0.0,
        'curiosity_trigger_strength': 0.0,
        'relevance_establishment': 0.0,
        'tone_setting': 0.0,
        'expectation_management': 0.0
    }

    try:
        # Analyze first few seconds (typical hook duration)
        hook_frames = video_frames[:int(len(video_frames) * 0.1)]  # First 10% of frames
        
        # Calculate attention grab speed
        hook_metrics['attention_grab_speed'] = self._calculate_attention_speed(hook_frames)
        
        # Analyze curiosity triggers
        hook_metrics['curiosity_trigger_strength'] = self._analyze_curiosity_elements(hook_frames)
        
        # Evaluate relevance
        hook_metrics['relevance_establishment'] = self._evaluate_relevance(hook_frames)
        
        # Analyze tone setting
        hook_metrics['tone_setting'] = self._analyze_tone_establishment(hook_frames)
        
        # Evaluate expectation setting
        hook_metrics['expectation_management'] = self._analyze_expectation_setting(hook_frames)

    except Exception as e:
        print(f"Error in hook analysis: {str(e)}")

    return hook_metrics

def _analyze_introduction_pacing(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze pacing of content introduction"""
    intro_metrics = {
        'information_density': 0.0,
        'personality_emergence_rate': 0.0,
        'context_provision_speed': 0.0,
        'viewer_orientation_efficiency': 0.0,
        'initial_engagement_sustainability': 0.0
    }

    try:
        # Analyze introduction section
        intro_frames = video_frames[int(len(video_frames) * 0.1):int(len(video_frames) * 0.25)]
        
        # Calculate information density
        intro_metrics['information_density'] = self._calculate_information_density(intro_frames)
        
        # Analyze personality emergence
        intro_metrics['personality_emergence_rate'] = self._analyze_personality_presence(intro_frames)
        
        # Evaluate context provision
        intro_metrics['context_provision_speed'] = self._evaluate_context_delivery(intro_frames)
        
        # Analyze viewer orientation
        intro_metrics['viewer_orientation_efficiency'] = self._analyze_viewer_guidance(intro_frames)
        
        # Evaluate engagement sustainability
        intro_metrics['initial_engagement_sustainability'] = self._analyze_engagement_maintenance(intro_frames)

    except Exception as e:
        print(f"Error in introduction pacing analysis: {str(e)}")

    return intro_metrics

def _analyze_content_flow(self, video_frames: List[Dict]) -> Dict[str, Any]:
    """Analyze overall content flow"""
    return {
        'segment_transitions': self._analyze_segment_transitions(video_frames),
        'energy_modulation': self._analyze_energy_modulation(video_frames),
        'information_density': self._analyze_info_density_distribution(video_frames)
    }

def _analyze_segment_transitions(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze transitions between content segments"""
    transition_metrics = {
        'transition_smoothness': 0.0,
        'thematic_connection_clarity': 0.0,
        'pace_variation': 0.0,
        'attention_retention_techniques': 0.0,
        'logical_progression': 0.0
    }

    try:
        # Detect segment boundaries
        segments = self._detect_content_segments(video_frames)
        
        if segments:
            # Analyze transition smoothness
            transition_metrics['transition_smoothness'] = self._evaluate_transition_smoothness(segments)
            
            # Analyze thematic connections
            transition_metrics['thematic_connection_clarity'] = self._analyze_thematic_connections(segments)
            
            # Evaluate pace variations
            transition_metrics['pace_variation'] = self._analyze_pace_variation(segments)
            
            # Analyze attention retention
            transition_metrics['attention_retention_techniques'] = self._analyze_retention_techniques(segments)
            
            # Evaluate logical progression
            transition_metrics['logical_progression'] = self._evaluate_progression_logic(segments)

    except Exception as e:
        print(f"Error in transition analysis: {str(e)}")

    return transition_metrics

def _analyze_energy_modulation(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze energy modulation throughout content"""
    energy_metrics = {
        'high_energy_segment_placement': 0.0,
        'calm_moment_integration': 0.0,
        'emotional_pacing': 0.0,
        'viewer_fatigue_consideration': 0.0,
        'climax_build_up': 0.0
    }

    try:
        # Analyze energy segments
        energy_segments = self._detect_energy_segments(video_frames)
        
        if energy_segments:
            # Evaluate high energy placement
            energy_metrics['high_energy_segment_placement'] = self._evaluate_energy_placement(energy_segments)
            
            # Analyze calm moments
            energy_metrics['calm_moment_integration'] = self._analyze_calm_periods(energy_segments)
            
            # Evaluate emotional pacing
            energy_metrics['emotional_pacing'] = self._analyze_emotional_flow(energy_segments)
            
            # Analyze viewer fatigue consideration
            energy_metrics['viewer_fatigue_consideration'] = self._analyze_fatigue_management(energy_segments)
            
            # Evaluate climax build-up
            energy_metrics['climax_build_up'] = self._analyze_climax_structure(energy_segments)

    except Exception as e:
        print(f"Error in energy modulation analysis: {str(e)}")

    return energy_metrics

from typing import List, Dict, Any

def _analyze_info_density_distribution(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze distribution of information density"""
    density_metrics = {
        'complex_info_chunk_size': 0.0,
        'simplification_techniques': 0.0,
        'repetition_strategy': 0.0,
        'key_point_emphasis': 0.0,
        'information_hierarchy': 0.0
    }

    try:
        # Analyze content segments for information density
        info_segments = self._detect_info_segments(video_frames)
        
        if info_segments:
            # Evaluate complex information chunks
            density_metrics['complex_info_chunk_size'] = self._analyze_info_chunk_sizes(info_segments)
            
            # Analyze simplification methods
            density_metrics['simplification_techniques'] = self._evaluate_simplification_methods(info_segments)
            
            # Evaluate repetition patterns
            density_metrics['repetition_strategy'] = self._analyze_repetition_patterns(info_segments)
            
            # Analyze key point emphasis
            density_metrics['key_point_emphasis'] = self._evaluate_key_point_emphasis(info_segments)
            
            # Evaluate information hierarchy
            density_metrics['information_hierarchy'] = self._analyze_info_hierarchy(info_segments)

    except Exception as e:
        print(f"Error in information density analysis: {str(e)}")

    return density_metrics

def _analyze_conclusion_cta(self, video_frames: List[Dict]) -> Dict[str, Any]:
    """Analyze conclusion and call to action"""
    return {
        'wrap_up_efficiency': self._analyze_wrap_up(video_frames),
        'cta_integration': self._analyze_cta(video_frames)
    }

def _analyze_wrap_up(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze content wrap-up effectiveness"""
    wrap_up_metrics = {
        'key_point_recapitulation': 0.0,
        'emotional_resolution': 0.0,
        'future_content_teasing': 0.0,
        'viewer_reflection_prompts': 0.0,
        'satisfaction_delivery': 0.0
    }

    try:
        # Analyze conclusion section (last 10-15% of content)
        conclusion_frames = video_frames[int(len(video_frames) * 0.85):]
        
        # Evaluate key point recap
        wrap_up_metrics['key_point_recapitulation'] = self._analyze_key_point_recap(conclusion_frames)
        
        # Analyze emotional resolution
        wrap_up_metrics['emotional_resolution'] = self._evaluate_emotional_resolution(conclusion_frames)
        
        # Analyze future content teasing
        wrap_up_metrics['future_content_teasing'] = self._analyze_future_content_hints(conclusion_frames)
        
        # Evaluate viewer reflection prompts
        wrap_up_metrics['viewer_reflection_prompts'] = self._analyze_reflection_prompts(conclusion_frames)
        
        # Analyze satisfaction delivery
        wrap_up_metrics['satisfaction_delivery'] = self._evaluate_satisfaction_delivery(conclusion_frames)

    except Exception as e:
        print(f"Error in wrap-up analysis: {str(e)}")

    return wrap_up_metrics

def _analyze_cta(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze call to action effectiveness"""
    cta_metrics = {
        'cta_natural_placement': 0.0,
        'value_proposition_clarity': 0.0,
        'action_step_simplicity': 0.0,
        'motivation_alignment': 0.0,
        'cta_variety': 0.0
    }

    try:
        # Detect CTAs in content
        cta_segments = self._detect_cta_segments(video_frames)
        
        if cta_segments:
            # Evaluate CTA placement
            cta_metrics['cta_natural_placement'] = self._evaluate_cta_placement(cta_segments)
            
            # Analyze value proposition
            cta_metrics['value_proposition_clarity'] = self._analyze_value_proposition(cta_segments)
            
            # Evaluate action step clarity
            cta_metrics['action_step_simplicity'] = self._evaluate_action_steps(cta_segments)
            
            # Analyze motivation alignment
            cta_metrics['motivation_alignment'] = self._analyze_motivation_alignment(cta_segments)
            
            # Evaluate CTA variety
            cta_metrics['cta_variety'] = self._analyze_cta_variety(cta_segments)

    except Exception as e:
        print(f"Error in CTA analysis: {str(e)}")

    return cta_metrics

def _detect_content_segments(self, video_frames: List[Dict]) -> List[Dict]:
    """Detect distinct content segments in video"""
    segments = []
    current_segment = None
    
    for i, frame in enumerate(video_frames):
        # Detect significant changes that indicate segment boundaries
        if self._is_segment_boundary(frame, video_frames[i-1] if i > 0 else None):
            if current_segment:
                current_segment['end_frame'] = i
                segments.append(current_segment)
            
            current_segment = {
                'start_frame': i,
                'type': self._determine_segment_type(frame),
                'characteristics': self._analyze_segment_characteristics(frame)
            }
    
    # Add final segment
    if current_segment:
        current_segment['end_frame'] = len(video_frames)
        segments.append(current_segment)
    
    return segments

def _detect_energy_segments(self, video_frames: List[Dict]) -> List[Dict]:
    """Detect segments with different energy levels"""
    energy_segments = []
    current_energy = None
    segment_start = 0
    
    for i, frame in enumerate(video_frames):
        # Calculate frame energy level
        frame_energy = self._calculate_frame_energy(frame)
        
        # Detect significant energy changes
        if current_energy is None or abs(frame_energy - current_energy) > 0.2:  # 20% change threshold
            if i > segment_start:
                energy_segments.append({
                    'start_frame': segment_start,
                    'end_frame': i,
                    'energy_level': current_energy,
                    'characteristics': self._analyze_energy_characteristics(video_frames[segment_start:i])
                })
            
            segment_start = i
            current_energy = frame_energy
    
    # Add final segment
    if segment_start < len(video_frames):
        energy_segments.append({
            'start_frame': segment_start,
            'end_frame': len(video_frames),
            'energy_level': current_energy,
            'characteristics': self._analyze_energy_characteristics(video_frames[segment_start:])
        })
    
    return energy_segments
from typing import List, Dict

def _calculate_attention_speed(self, frames: List[Dict]) -> float:
    """Calculate how quickly content grabs attention"""
    try:
        attention_indicators = {
            'motion_change': self._detect_motion_changes(frames[:10]),  # First 10 frames
            'visual_impact': self._analyze_visual_impact(frames[0]),       # First frame
            'audio_impact': self._analyze_audaudio_impact(frames[0]),       # First frame
            'scene_complexity': self._analyze_scene_complexity(frames[0])
        }
        
        weights = {
            'motion_change': 0.3,
            'visual_impact': 0.3,
            'audio_impact': 0.2,
            'scene_complexity': 0.2
        }
        
        attention_score = sum(score * weights[metric] 
                              for metric, score in attention_indicators.items())
        return min(100, attention_score)
    except Exception:
        return 0.0

def _analyze_curiosity_elements(self, frames: List[Dict]) -> float:
    """Analyze elements that trigger curiosity"""
    try:
        curiosity_indicators = {
            'mystery_elements': self._detect_mystery_elements(frames),
            'question_presentation': self._detect_question_elements(frames),
            'unexpected_elements': self._detect_unexpected_elements(frames),
            'intrigue_buildup': self._analyze_intrigue_progression(frames)
        }
        
        weights = {
            'mystery_elements': 0.3,
            'question_presentation': 0.3,
            'unexpected_elements': 0.2,
            'intrigue_buildup': 0.2
        }
        
        curiosity_score = sum(score * weights[metric] 
                              for metric, score in curiosity_indicators.items())
        return min(100, curiosity_score)
    except Exception:
        return 0.0

def _evaluate_relevance(self, frames: List[Dict]) -> float:
    """Evaluate how well content establishes relevance"""
    try:
        relevance_metrics = {
            'target_audience_signals': self._detect_audience_signals(frames),
            'problem_identification': self._detect_problem_presentation(frames),
            'value_proposition': self._detect_value_indicators(frames),
            'contextual_alignment': self._analyze_context_alignment(frames)
        }
        
        weights = {
            'target_audience_signals': 0.3,
            'problem_identification': 0.3,
            'value_proposition': 0.2,
            'contextual_alignment': 0.2
        }
        
        relevance_score = sum(score * weights[metric] 
                              for metric, score in relevance_metrics.items())
        return min(100, relevance_score)
    except Exception:
        return 0.0

def _analyze_tone_establishment(self, frames: List[Dict]) -> float:
    """Analyze how effectively tone is established"""
    try:
        tone_elements = {
            'visual_mood': self._analyze_visual_mood(frames),
            'audio_tone': self._analyze_audio_tone(frames),
            'pacing_mood': self._analyze_pacing_mood(frames),
            'presenter_tone': self._analyze_presenter_tone(frames)
        }
        
        weights = {
            'visual_mood': 0.3,
            'audio_tone': 0.3,
            'pacing_mood': 0.2,
            'presenter_tone': 0.2
        }
        
        tone_score = sum(score * weights[metric] 
                         for metric, score in tone_elements.items())
        return min(100, tone_score)
    except Exception:
        return 0.0

def _analyze_expectation_setting(self, frames: List[Dict]) -> float:
    """Analyze how well expectations are set"""
    try:
        expectation_elements = {
            'content_preview': self._analyze_preview_elements(frames),
            'benefit_indication': self._detect_benefit_signals(frames),
            'structure_signaling': self._detect_structure_hints(frames),
            'time_investment_clarity': self._analyze_time_indicators(frames)
        }
        
        weights = {
            'content_preview': 0.3,
            'benefit_indication': 0.3,
            'structure_signaling': 0.2,
            'time_investment_clarity': 0.2
        }
        
        expectation_score = sum(score * weights[metric] 
                                for metric, score in expectation_elements.items())
        return min(100, expectation_score)
    except Exception:
        return 0.0

def _calculate_information_density(self, frames: List[Dict]) -> float:
    """Calculate density of information presentation"""
    try:
        info_elements = {
            'text_density': self._analyze_text_density(frames),
            'visual_complexity': self._analyze_visual_complexity(frames),
            'concept_frequency': self._analyze_concept_frequency(frames),
            'information_layering': self._analyze_info_layering(frames)
        }
        
        weights = {
            'text_density': 0.3,
            'visual_complexity': 0.3,
            'concept_frequency': 0.2,
            'information_layering': 0.2
        }
        
        density_score = sum(score * weights[metric] 
                            for metric, score in info_elements.items())
        return min(100, density_score)
    except Exception:
        return 0.0

def _analyze_personality_presence(self, frames: List[Dict]) -> float:
    """Analyze presenter personality emergence"""
    try:
        personality_indicators = {
            'presenter_expressiveness': self._analyze_presenter_expression(frames),
            'speaking_style': self._analyze_speaking_style(frames),
            'personal_anecdotes': self._detect_personal_elements(frames),
            'authenticity_signals': self._analyze_authenticity(frames)
        }
        
        weights = {
            'presenter_expressiveness': 0.3,
            'speaking_style': 0.3,
            'personal_anecdotes': 0.2,
            'authenticity_signals': 0.2
        }
        
        personality_score = sum(score * weights[metric] 
                                for metric, score in personality_indicators.items())
        return min(100, personality_score)
    except Exception:
        return 0.0

def _evaluate_context_delivery(self, frames: List[Dict]) -> float:
    """Evaluate how context is provided"""
    try:
        context_elements = {
            'background_information': self._analyze_background_info(frames),
            'prerequisite_explanation': self._analyze_prerequisites(frames),
            'scope_definition': self._analyze_scope_setting(frames),
            'relevance_connection': self._analyze_relevance_links(frames)
        }
        
        weights = {
            'background_information': 0.3,
            'prerequisite_explanation': 0.3,
            'scope_definition': 0.2,
            'relevance_connection': 0.2
        }
        
        context_score = sum(score * weights[metric] 
                            for metric, score in context_elements.items())
        return min(100, context_score)
    except Exception:
        return 0.0

def _analyze_viewer_guidance(self, frames: List[Dict]) -> float:
    """Analyze how well viewers are guided through content"""
    try:
        guidance_elements = {
            'navigation_cues': self._analyze_navigation_elements(frames),
            'structural_clarity': self._analyze_structure_clarity(frames),
            'expectation_management': self._analyze_expectation_mgmt(frames),
            'orientation_aids': self._analyze_orientation_elements(frames)
        }
        
        weights = {
            'navigation_cues': 0.3,
            'structural_clarity': 0.3,
            'expectation_management': 0.2,
            'orientation_aids': 0.2
        }
        
        guidance_score = sum(score * weights[metric] 
                             for metric, score in guidance_elements.items())
        return min(100, guidance_score)
    except Exception:
        return 0.0
def _is_segment_boundary(self, current_frame: Dict, previous_frame: Dict) -> bool:
    """Determine if frame represents a segment boundary"""
    if not previous_frame:
        return False
        
    try:
        # Check for significant changes that indicate segment boundaries
        boundary_indicators = {
            'scene_change': self._detect_scene_change(current_frame, previous_frame),
            'audio_transition': self._detect_audio_transition(current_frame, previous_frame),
            'topic_change': self._detect_topic_change(current_frame, previous_frame),
            'visual_transition': self._detect_visual_transition(current_frame, previous_frame)
        }
        
        # If any indicator shows significant change, consider it a boundary
        return any(boundary_indicators.values())
        
    except Exception:
        return False

def _determine_segment_type(self, frame: Dict) -> str:
    """Determine the type of content segment"""
    try:
        segment_characteristics = {
            'introduction': self._check_introduction_indicators(frame),
            'main_content': self._check_main_content_indicators(frame),
            'transition': self._check_transition_indicators(frame),
            'conclusion': self._check_conclusion_indicators(frame),
            'cta': self._check_cta_indicators(frame)
        }
        
        # Return the segment type with highest confidence
        return max(segment_characteristics.items(), key=lambda x: x[1])[0]
        
    except Exception:
        return 'unknown'

def _analyze_segment_characteristics(self, frame: Dict) -> Dict[str, float]:
  
    """Analyze characteristics of content segment"""
    try:
        return {
            'energy_level': self._calculate_segment_energy(frame),
            'information_density': self._calculate_segment_info_density(frame),
            'engagement_level': self._calculate_segment_engagement(frame),
            'pace': self._calculate_segment_pace(frame),
            'complexity': self._calculate_segment_complexity(frame)
        }
    except Exception:
        return {
            'energy_level': 0.0,
            'information_density': 0.0,
            'engagement_level': 0.0,
            'pace': 0.0,
            'complexity': 0.0
        }

def _evaluate_transition_smoothness(self, segments: List[Dict]) -> float:
    """Evaluate smoothness of transitions between segments"""
    try:
        transition_scores = []
        
        for i in range(1, len(segments)):
            prev_segment = segments[i-1]
            curr_segment = segments[i]
            
            transition_characteristics = {
                'visual_smoothness': self._analyze_visual_transition(prev_segment, curcurr_segment),
                'audio_smoothness': self._analyze_audio_transition(prev_segment, curr_segment),
                'pacing_consistency': self._analyze_pacing_transition(prev_segment, curr_segment),
                'thematic_flow': self._analyze_thematic_transition(prev_segment, curr_segment)
            }
            
            weights = {
                'visual_smoothness': 0.3,
                'audio_smoothness': 0.3,
                'pacing_consistency': 0.2,
                'thematic_flow': 0.2
            }
            
            transition_score = sum(score * weights[metric] 
                                for metric, score in transition_characteristics.items())
            transition_scores.append(transition_score)
        
        return sum(transition_scores) / len(transition_scores) if transition_scores else 0.0
        
    except Exception:
        return 0.0

def _analyze_thematic_connections(self, segments: List[Dict]) -> float:
    """Analyze thematic connections between segments"""
    try:
        connection_scores = []
        
        for i in range(1, len(segments)):
            prev_segment = segments[i-1]
            curr_segment = segments[i]
            
            connection_metrics = {
                'topic_relevance': self._analyze_topic_connection(prev_segment, curr_segment),
                'narrative_continuity': self._analyze_narrative_connection(prev_segment, curr_segment),
                'logical_flow': self._analyze_logical_connection(prev_segment, curr_segment),
                'theme_consistency': self._analyze_theme_consistency(prev_segment, curr_segment)
            }
            
            weights = {
                'topic_relevance': 0.3,
                'narrative_continuity': 0.3,
                'logical_flow': 0.2,
                'theme_consistency': 0.2
            }
            
            connection_score = sum(score * weights[metric] 
                                for metric, score in connection_metrics.items())
            connection_scores.append(connection_score)
        
        return sum(connection_scores) / len(connection_scores) if connection_scores else 0.0
        
    except Exception:
        return 0.0

def _analyze_pace_variation(self, segments: List[Dict]) -> float:
    """Analyze variation in pacing across segments"""
    try:
        pace_metrics = {
            'pace_range': self._calculate_pace_range(segments),
            'pace_distribution': self._analyze_pace_distribution(segments),
            'transition_timing': self._analyze_transition_timing(segments),
            'rhythm_consistency': self._analyze_rhythm_consistency(segments)
        }
        
        weights = {
            'pace_range': 0.3,
            'pace_distribution': 0.3,
            'transition_timing': 0.2,
            'rhythm_consistency': 0.2
        }
        
        variation_score = sum(score * weights[metric] 
                            for metric, score in pace_metrics.items())
        return min(100, variation_score)
        
    except Exception:
        return 0.0

def _calculate_frame_energy(self, frame: Dict) -> float:
    """Calculate energy level of a single frame"""
    try:
        energy_components = {
            'visual_energy': self._calculate_visual_energy(frame),
            'audio_energy': self._calculate_audio_energy(frame),
            'motion_energy': self._calculate_motion_energy(frame),
            'emotional_energy': self._calculate_emotional_energy(frame)
        }
        
        weights = {
            'visual_energy': 0.3,
            'audio_energy': 0.3,
            'motion_energy': 0.2,
            'emotional_energy': 0.2
        }
        
        energy_score = sum(score * weights[metric] 
                         for metric, score in energy_components.items())
        return min(100, energy_score)
        
    except Exception:
        return 0.0
# Energy Analysis Helper Methods

def _analyze_energy_characteristics(self, frames: List[Dict]) -> Dict[str, float]:
    """Analyze detailed energy characteristics of frame sequence"""
    try:
        energy_metrics = {
            'intensity': self._calculate_intensity_level(frames),
            'variability': self._calculate_energy_variability(frames),
            'sustainability': self._analyze_energy_sustainability(frames),
            'peak_distribution': self._analyze_energy_peaks(frames),
            'recovery_periods': self._identify_recovery_periods(frames)
        }
        
        return energy_metrics
    except Exception:
        return {key: 0.0 for key in ['intensity', 'variability', 'sustainability', 
                                    'peak_distribution', 'recovery_periods']}

def _evaluate_energy_placement(self, segments: List[Dict]) -> float:
    """Evaluate strategic placement of high-energy segments"""
    try:
        placement_scores = []
        total_duration = sum(seg['end_frame'] - seg['start_frame'] for seg in segments)
        
        for segment in segments:
            # Calculate relative position in content
            position = segment['start_frame'] / total_duration
            energy_level = segment['energy_level']
            
            # Score based on strategic placement
            placement_score = self._score_energy_placement(position, energy_level)
            placement_scores.append(placement_score)
        
        return sum(placement_scores) / len(placement_scores) if placement_scores else 0.0
    except Exception:
        return 0.0

def _analyze_calm_periods(self, segments: List[Dict]) -> float:
    """Analyze effectiveness of calm/low-energy periods"""
    try:
        calm_metrics = {
            'distribution': self._analyze_calm_distribution(segments),
            'duration': self._analyze_calm_duration(segments),
            'transition': self._analyze_calm_transitions(segments),
            'purpose': self._analyze_calm_purpose(segments)
        }
        
        weights = {
            'distribution': 0.3,
            'duration': 0.3,
            'transition': 0.2,
            'purpose': 0.2
        }
        
        return sum(score * weights[metric] for metric, score in calm_metrics.items())
    except Exception:
        return 0.0

def _analyze_emotional_flow(self, segments: List[Dict]) -> float:
    """Analyze emotional flow through energy variations"""
    try:
        flow_metrics = {
            'emotional_arc': self._analyze_emotional_arc(segments),
            'tension_release': self._analyze_tension_release(segments),
            'engagement_sustain': self._analyze_engagement_maintenance(segments),
            'emotional_contrast': self._analyze_emotional_contrast(segments)
        }
        
        weights = {
            'emotional_arc': 0.3,
            'tension_release': 0.3,
            'engagement_sustain': 0.2,
            'emotional_contrast': 0.2
        }
        
        return sum(score * weights[metric] for metric, score in flow_metrics.items())
    except Exception:
        return 0.0

# Information Density Helper Methods

def _detect_info_segments(self, frames: List[Dict]) -> List[Dict]:
    """Detect segments with distinct information density"""
    try:
        info_segments = []
        current_segment = None
        
        for i, frame in enumerate(frames):
            info_density = self._calculate_frame_info_density(frame)
            
            # Check for significant change in information density
            if (current_segment is None or 
                abs(info_density - current_segment['density']) > 0.2):  # 20% threshold
                
                if current_segment:
                    current_segment['end_frame'] = i
                    info_segments.append(current_segment)
                
                current_segment = {
                    'start_frame': i,
                    'density': info_density,
                    'type': self._determine_info_type(frame),
                    'complexity': self._calculate_info_complexity(frame)
                }
        
        # Add final segment
        if current_segment:
            current_segment['end_frame'] = len(frames)
            info_segments.append(current_segment)
        
        return info_segments
    except Exception:
        return []

def _analyze_info_chunk_sizes(self, segments: List[Dict]) -> float:
    """Analyze size and distribution of information chunks"""
    try:
        chunk_metrics = {
            'size_appropriateness': self._evaluate_chunk_sizes(segments),
            'complexity_distribution': self._analyze_complexity_distribution(segments),
            'spacing_effectiveness': self._analyze_chunk_spacing(segments),
            'cognitive_load_management': self._evaluate_cognitive_load(segments)
        }
        
        weights = {
            'size_appropriateness': 0.3,
            'complexity_distribution': 0.3,
            'spacing_effectiveness': 0.2,
            'cognitive_load_management': 0.2
        }
        
        return sum(score * weights[metric] for metric, score in chunk_metrics.items())
    except Exception:
        return 0.0

def _evaluate_simplification_methods(self, segments: List[Dict]) -> float:
    """Evaluate effectiveness of information simplification methods"""
    try:
        simplification_metrics = {
            'visual_aids': self._analyze_visual_simplification(segments),
            'explanation_clarity': self._analyze_explanation_methods(segments),
            'concept_breakdown': self._analyze_concept_simplification(segments),
            'abstraction_levels': self._analyze_abstraction_usage(segments)
        }
        
        weights = {
            'visual_aids': 0.3,
            'explanation_clarity': 0.3,
            'concept_breakdown': 0.2,
            'abstraction_levels': 0.2
        }
        
        return sum(score * weights[metric] for metric, score in simplification_metrics.items())
    except Exception:
        return 0.0

def _analyze_repetition_patterns(self, segments: List[Dict]) -> float:
    """Analyze patterns of information repetition"""
    try:
        repetition_metrics = {
            'key_point_reinforcement': self._analyze_key_point_repetition(segments),
            'spaced_repetition': self._analyze_repetition_spacing(segments),
            'variation_in_presentation': self._analyze_repetition_variation(segments),
            'retention_support': self._evaluate_retention_effectiveness(segments)
        }
        
        weights = {
            'key_point_reinforcement': 0.3,
            'spaced_repetition': 0.3,
            'variation_in_presentation': 0.2,
            'retention_support': 0.2
        }
        
        return sum(score * weights[metric] for metric, score in repetition_metrics.items())
    except Exception:
        return 0.0

def _analyze_info_hierarchy(self, segments: List[Dict]) -> float:
    """Analyze hierarchy and organization of information"""
    try:
        hierarchy_metrics = {
            'structural_clarity': self._analyze_info_structure(segments),
            'priority_ordering': self._analyze_info_prioritization(segments),
            'relationship_clarity': self._analyze_info_relationships(segments),
            'progressive_complexity': self._analyze_complexity_progression(segments)
        }
        
        weights = {
            'structural_clarity': 0.3,
            'priority_ordering': 0.3,
            'relationship_clarity': 0.2,
            'progressive_complexity': 0.2
        }
        
        return sum(score * weights[metric] for metric, score in hierarchy_metrics.items())
    except Exception:
        return 0.0
def _detect_cta_segments(self, frames: List[Dict]) -> List[Dict]:
    """Detect segments containing calls to action"""
    try:
        cta_segments = []
        current_cta = None
        
        for i, frame in enumerate(frames):
            if self._is_cta_element(frame):
                if not current_cta:
                    current_cta = {
                        'start_frame': i,
                        'type': self._determine_cta_type(frame),
                        'characteristics': self._analyze_cta_characteristics(frame)
                    }
            elif current_cta:
                current_cta['end_frame'] = i
                cta_segments.append(current_cta)
                current_cta = None
        
        # Handle final CTA segment
        if current_cta:
            current_cta['end_frame'] = len(frames)
            cta_segments.append(current_cta)
        
        return cta_segments
    except Exception:
        return []

def _evaluate_cta_placement(self, segments: List[Dict]) -> float:
    """Evaluate strategic placement of CTAs"""
    try:
        placement_metrics = {
            'timing_effectiveness': self._analyze_cta_timing(segments),
            'context_alignment': self._analyze_cta_context(segments),
            'momentum_utilizatiotion': self._analyze_momentum_alignment(segments),
            'audience_readiness': self._evaluate_audience_preparation(segments)
        }
        
        weights = {
            'timing_effectiveness': 0.3,
            'context_alignment': 0.3,
            'momentum_utilization': 0.2,
            'audience_readiness': 0.2
        }
        
        # Fixed the syntax error by removing "re" from "for metric, score re in ..."
        return sum(score * weights[metric] for metric, score in placement_metrics.items())
    except Exception:
        return 0.0

def _analyze_value_proposition(self, segments: List[Dict]) -> float:
    """Analyze clarity and effectiveness of value proposition in CTAs"""
    try:
        value_metrics = {
            'benefit_clarity': self._analyze_benefit_presentation(segments),
            'relevance_demonstration': self._analyze_relevance_clarity(segments),
            'pain_point_connection': self._analyze_pain_point_alignment(segments),
            'value_uniqueness': self._analyze_unique_value(segments)
        }
        
        weights = {
            'benefit_clarity': 0.3,
            'relevance_demonstration': 0.3,
            'pain_point_connection': 0.2,
            'value_uniqueness': 0.2
        }
        
        return sum(score * weights[metric] for metric, score in value_metrics.items())
    except Exception:
        return 0.0

def _evaluate_action_steps(self, segments: List[Dict]) -> float:
    """Evaluate clarity and simplicity of action steps"""
    try:
        action_metrics = {
            'step_clarity': self._analyze_step_clarity(segments),
            'barrier_reduction': self._analyze_barrier_removal(segments),
            'urgency_creation': self._analyze_urgency_elements(segments),
            'follow_through_support': self._analyze_follow_through(segments)
        }
        
        weights = {
            'step_clarity': 0.3,
            'barrier_reduction': 0.3,
            'urgency_creation': 0.2,
            'follow_through_support': 0.2
        }
        
        return sum(score * weights[metric] for metric, score in action_metrics.items())
    except Exception:
        return 0.0

def _analyze_motivation_alignment(self, segments: List[Dict]) -> float:
    """Analyze alignment with viewer motivation"""
    try:
        motivation_metrics = {
            'desire_connection': self._analyze_desire_alignment(segments),
            'emotional_resonance': self._analyze_emotional_alignment(segments),
            'goal_alignment': self._analyze_goal_alignment(segments),
            'value_match': self._analyze_value_alignment(segments)
        }
        
        weights = {
            'desire_connection': 0.3,
            'emotional_resonance': 0.3,
            'goal_alignment': 0.2,
            'value_match': 0.2
        }
        
        return sum(score * weights[metric] for metric, score in motivation_metrics.items())
    except Exception:
        return 0.0

def _analyze_cta_variety(self, segments: List[Dict]) -> float:
    """Analyze variety and complementary nature of CTAs"""
    try:
        variety_metrics = {
            'type_diversity': self._analyze_cta_types(segments),
            'format_variation': self._analyze_format_variety(segments),
            'platform_coverage': self._analyze_platform_diversity(segments),
            'engagement_levels': self._analyze_engagement_options(segments)
        }
        
        weights = {
            'type_diversity': 0.3,
            'format_variation': 0.3,
            'platform_coverage': 0.2,
            'engagement_levels': 0.2
        }
        
        return sum(score * weights[metric] for metric, score in variety_metrics.items())
    except Exception:
        return 0.0

# Additional helper methods for CTA analysis

def _is_cta_element(self, frame: Dict) -> bool:
    """Detect if frame contains CTA elements"""
    try:
        cta_indicators = {
            'visual_cues': self._detect_visual_cta_indicators(frame),
            'text_cues': self._detect_text_cta_indicators(frame),
            'audio_cues': self._detect_audio_cta_indicators(frame),
            'behavioral_cues': self._detect_behavioral_cta_indicators(frame)
        }
        
        return any(cta_indicators.values())
    except Exception:
        return False

def _determine_cta_type(self, frame: Dict) -> str:
    """Determine type of CTA present in frame"""
    try:
        cta_types = {
            'subscribe': self._check_subscribe_cta(frame),
            'purchase': self._check_purchase_cta(frame),
            'follow': self._check_follow_cta(frame),
            'share': self._check_share_cta(frame),
            'learn_more': self._check_learn_more_cta(frame)
        }
        
        # Return type with highest confidence
        return max(cta_types.items(), key=lambda x: x[1])[0]
    except Exception:
        return 'unknown'

def _analyze_cta_characteristics(self, frame: Dict) -> Dict[str, float]:
    """Analyze detailed characteristics of CTA"""
    try:
        return {
            'visibility': self._analyze_cta_visibility(frame),
            'clarity': self._analyze_cta_clarity(frame),
            'urgency': self._analyze_cta_urgency(frame),
            'appeal': self._analyze_cta_appeal(frame),
            'accessibility': self._analyze_cta_accessibility(frame)
        }
    except Exception:
        return {key: 0.0 for key in ['visibility', 'clarity', 'urgency', 'appeal', 'accessibility']}
def _detect_visual_cta_indicators(self, frame: Dict) -> bool:
    """Detect visual indicators of CTA"""
    try:
        visual_indicators = [
            self._detect_buttons(frame),
            self._detect_arrows(frame),
            self._detect_highlighted_areas(frame),
            self._detect_action_graphics(frame)
        ]
        # Corrected: iterate over visual_indicators properly
        return any(visual_indicators)
    except Exception:
        return False

def _detect_text_cta_indicators(self, frame: Dict) -> bool:
    """Detect text-based CTA indicators"""
    try:
        cta_phrases = [
            'subscribe', 'follow', 'buy now', 'learn more',
            'sign up', 'click here', 'get started', 'join now'
        ]
        text_detected = self._extract_text_from_frame(frame)
        return any(phrase in text_detected.lower() for phrase in cta_phrases)
    except Exception:
        return False

def _detect_audio_cta_indicators(self, frame: Dict) -> bool:
    """Detect audio-based CTA indicators"""
    try:
        audio_data = frame.get('Audio', {})
        return self._analyze_audio_commands(audio_data)
    except Exception:
        return False

def _analyze_cta_visibility(self, frame: Dict) -> float:
    """Analyze visibility and prominence of Cf CTA"""
    try:
        visibility_factors = {
            'size': self._analyze_cta_size(frame),
            'contrast': self._analyze_cta_contrast(frame),
            'position': self._analyze_cta_position(frame),
            'duration': self._analyze_cta_duration(frame)
        }
        return sum(visibility_factors.values()) / len(visibility_factors)
    except Exception:
        return 0.0

def _analyze_cta_timing(self, segments: List[Dict]) -> float:
    """Analyze timing effectiveness of CTAs"""
    try:
        timing_scores = []
        content_duration = segments[-1]['end_frame']
        
        for segment in segments:
            # Corrected key access for start_frame
            relative_position = segment['start_frame'] / content_duration
            timing_score = self._evaluate_timing_effectiveness(
                relative_position,
                segment['characteristics']
            )
            timing_scores.append(timing_score)
        
        return sum(timing_scores) / len(timing_scores) if timing_scores else 0.0
    except Exception:
        return 0.0

def _analyze_benefit_presentation(self, segments: List[Dict]) -> float:
    """Analyze how benefits are presented in CTAs"""
    try:
        benefit_scores = []
        for segment in segments:
            benefit_metrics = {
                'clarity': self._evaluate_benefit_clarity(segment),
                'specificity': self._evaluate_benefit_specificity(segment),
                'relevance': self._evaluate_benefit_relevance(segment),
                'credibility': self._evaluate_benefit_credibility(segment)
            }
            benefit_scores.append(sum(benefit_metrics.values()) / len(benefit_metrics))
        
        return sum(benefit_scores) / len(benefit_scores) if benefit_scores else 0.0
    except Exception:
        return 0.0

def _analyze_barrier_removal(self, segments: List[Dict]) -> float:
    """Analyze how effectively barriers to action are removed"""
    try:
        barrier_metrics = {
            'complexity_reduction': self._evaluate_complexity_reduction(segments),
            'risk_mitigation': self._evaluate_risk_mitigation(segments),
            'accessibility_improvement': self._evaluate_accessibility(segments),
            'friction_reduction': self._evaluate_friction_points(segments)
        }
        return sum(barrier_metrics.values()) / len(barrier_metrics)
    except Exception:
        return 0.0

def _analyze_urgency_elements(self, segments: List[Dict]) -> float:
    """Analyze urgency creation in CTAs"""
    try:
        urgency_metrics = {
            'scarcity_indicators': self._detect_scarcity_elements(segments),
            'time_limitation': self._detect_time_limits(segments),
            'opportunity_cost': self._analyze_opportunity_cost(segments),
            'momentum_building': self._analyze_momentum_creation(segments)
        }
        return sum(urgency_metrics.values()) / len(urgency_metrics)
    except Exception:
        return 0.0

def _analyze_cta_types(self, segments: List[Dict]) -> float:
    """Analyze variety in CTA types"""
    try:
        cta_types = set()
        for segment in segments:
            cta_types.add(segment['type'])
        
        # Score based on variety and complementary nature
        variety_score = len(cta_types) / 5  # Normalize to 0-1 range assuming 5 main CTA types
        return min(100, variety_score * 100)
    except Exception:
        return 0.0

def _analyze_format_variety(self, segments: List[Dict]) -> float:
    """Analyze variety in CTA formats"""
    try:
        format_types = {
            'visual': self._detect_visual_formats(segments),
            'audio': self._detect_audio_formats(segments),
            'text': self._detect_text_formats(segments),
            'interactive': self._detect_interactive_formats(segments)
        }
        
        # Calculate variety score
        format_count = sum(1 for present in format_types.values() if present)
        variety_score = format_count / len(format_types)
        return min(100, variety_score * 100)
    except Exception:
        return 0.0

def analyze_food_content(self, video_frames: List[Dict]) -> Dict[str, Any]:
    """Analyze food-related content characteristics"""
    return {
        'cooking_preparation': {
            'ingredient_showcase': self._analyze_ingredient_showcase(video_frames),
            'origin_and_quality': self._analyze_origin_and_quality(video_frames),
            'alternative_options': self._analyze_alternative_options(video_frames)
        },
        'plating_and_presentation': {
            'composition': self._analyze_plating_composition(video_frames),
            'plating_techniques': self._analyze_plating_techniques(video_frames),
            'eating_experience': self._analyze_eating_experience(video_frames)
        }
    }

def _analyze_ingredient_showcase(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze ingredient showcase characteristics"""
    showcase_metrics = {
        'ingredient_freshness_display': 0.0,
        'color_variety': 0.0,
        'texture_visibility': 0.0,
        'size_comparison': 0.0,
        'quantity_representation': 0.0
    }

    try:
        ingredient_segments = self._detect_ingredient_segments(video_frames)
        
        if ingredient_segments:
            # Analyze freshness display
            showcase_metrics['ingredient_freshness_display'] = self._analyze_freshness_indicators(
                ingredient_segments
            )
            
            # Analyze color variety
            showcase_metrics['color_variety'] = self._analyze_ingredient_colors(
                ingredient_segments
            )
            
            # Analyze texture visibility
            showcase_metrics['texture_visibility'] = self._analyze_texture_visibility(
                ingredient_segments
            )
            
            # Analyze size comparisons
            showcase_metrics['size_comparison'] = self._analyze_size_demonstrations(
                ingredient_segments
            )
            
            # Analyze quantity representation
            showcase_metrics['quantity_representation'] = self._analyze_quantity_display(
                ingredient_segments
            )

    except Exception as e:
        print(f"Error in ingredient showcase analysis: {str(e)}")

    return showcase_metrics

def _analyze_origin_and_quality(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze ingredient origin and quality indicators"""
    quality_metrics = {
        'sourcing_information': 0.0,
        'organic_vs_conventional': 0.0,
        'brand_showcase': 0.0,
        'quality_grade_indication': 0.0,
        'local_vs_imported': 0.0
    }

    try:
        # Detect segments with origin/quality information
        info_segments = self._detect_quality_info_segments(video_frames)
        
        if info_segments:
            # Analyze sourcing information
            quality_metrics['sourcing_information'] = self._analyze_sourcing_details(
                info_segments
            )
            
            # Analyze organic vs conventional indicators
            quality_metrics['organic_vs_conventional'] = self._analyze_farming_method(
                info_segments
            )
            
            # Analyze brand presence
            quality_metrics['brand_showcase'] = self._analyze_brand_presentation(
                info_segments
            )
            
            # Analyze quality grade indicators
            quality_metrics['quality_grade_indication'] = self._analyze_quality_indicators(
                info_segments
            )
            
            # Analyze local vs imported indicators
            quality_metrics['local_vs_imported'] = self._analyze_origin_indicators(
                info_segments
            )

    except Exception as e:
        print(f"Error in origin and quality analysis: {str(e)}")

    return quality_metrics

def _analyze_alternative_options(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze presentation of alternative ingredient options"""
    alternative_metrics = {
        'substitution_suggestions': 0.0,
        'dietary_restriction_options': 0.0,
        'seasonal_variations': 0.0,
        'budget_friendly_alternatives': 0.0,
        'ethnic_ingredient_explanations': 0.0
    }

    try:
        # Detect segments with alternative options
        alternative_segments = self._detect_alternative_segments(video_frames)
        
        if alternative_segments:
            # Analyze substitution suggestions
            alternative_metrics['substitution_suggestions'] = self._analyze_substitutions(
                alternative_segments
            )
            
            # Analyze dietary options
            alternative_metrics['dietary_restriction_options'] = self._analyze_dietary_options(
                alternative_segments
            )
            
            # Analyze seasonal variations
            alternative_metrics['seasonal_variations'] = self._analyze_seasonal_alternatives(
                alternative_segments
            )
            
            # Analyze budget alternatives
            alternative_metrics['budget_friendly_alternatives'] = self._analyze_budget_options(
                alternative_segments
            )
            
            # Analyze ethnic ingredient explanations
            alternative_metrics['ethnic_ingredient_explanations'] = self._analyze_ethnic_ingredients(
                alternative_segments
            )

    except Exception as e:
        print(f"Error in alternative options analysis: {str(e)}")

    return alternative_metrics
def _analyze_plating_composition(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze plating composition characteristics"""
    composition_metrics = {
        'color_balance': 0.0,
        'texture_variety': 0.0,
        'height_and_dimension': 0.0,
        'portion_mounding': 0.0,
        '3D_presentation_techniques': 0.0
    }

    try:
        plating_segments = self._detect_plating_segments(video_frames)
        
        if plating_segments:
            # Analyze color balance
            composition_metrics['color_balance'] = self._analyze_color_balance(
                plating_segments
            )
            
            # Analyze texture variety
            composition_metrics['texture_variety'] = self._analyze_texture_variety(
                plating_segments
            )
            
            # Analyze height and dimension
            composition_metrics['height_and_dimension'] = self._analyze_plate_dimensions(
                plating_segments
            )
            
            # Analyze portion mounding
            composition_metrics['portion_mounding'] = self._analyze_mounding_technique(
                plating_segments
            )
            
            # Analyze 3D presentation
            composition_metrics['3D_presentation_techniques'] = self._analyze_3d_techniques(
                   plating_segments
            )

    except Exception as e:
        print(f"Error in plating composition analysis: {str(e)}")

    return composition_metrics

def _analyze_plating_techniques(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze specific plating techniques"""
    technique_metrics = {
        'sauce_application': {
            'technique_variety': 0.0,
            'precision': 0.0,
            'artistic_quality': 0.0,
            'functionality': 0.0
        },
        'portion_control': {
            'consistency': 0.0,
            'balance': 0.0,
            'scale': 0.0,
            'arrangement': 0.0
        },
        'garnish_placement': {
            'positioning': 0.0,
            'relevance': 0.0,
            'visual_impact': 0.0,
            'integration': 0.0
        }
    }

    try:
        # Analyze sauce application techniques
        sauce_segments = self._detect_sauce_segments(video_frames)
        if sauce_segments:
            technique_metrics['sauce_application'] = self._analyze_sauce_techniques(
                sauce_segments
            )
        
        # Analyze portion control
        portion_segments = self._detect_portion_segments(video_frames)
        if portion_segments:
            technique_metrics['portion_control'] = self._analyze_portion_control(
                portion_segments
            )
        
        # Analyze garnish placement
        garnish_segments = self._detect_garnish_segments(video_frames)
        if garnish_segments:
            technique_metrics['garnish_placement'] = self._analyze_garnish_placement(
                garnish_segments
            )

    except Exception as e:
        print(f"Error in plating techniques analysis: {str(e)}")

    return technique_metrics

def _analyze_eating_experience(self, video_frames: List[Dict]) -> Dict[str, Any]:
    """Analyze eating experience presentation"""
    experience_metrics = {
        'first_bite': self._analyze_first_bite(video_frames),
        'multi_course_progression': self._analyze_course_progression(video_frames)
    }

    return experience_metrics

def _analyze_first_bite(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze first bite presentation"""
    first_bite_metrics = {
        'anticipation_building': 0.0,
        'texture_revelation': 0.0,
        'aroma_capture': 0.0,
        'visual_appeal': 0.0,
        'reaction_capture': 0.0
    }

    try:
        first_bite_segments = self._detect_first_bite_segments(video_frames)
        
        if first_bite_segments:
            # Analyze anticipation building
            first_bite_metrics['anticipation_building'] = self._analyze_anticipation(
                first_bite_segments
            )
            
            # Analyze texture revelation
            first_bite_metrics['texture_revelation'] = self._analyze_texture_reveal(
                first_bite_segments
            )
            
            # Analyze aroma capture
            first_bite_metrics['aroma_capture'] = self._analyze_aroma_presentation(
                first_bite_segments
            )
            
            # Analyze visual appeal
            first_bite_metrics['visual_appeal'] = self._analyze_bite_visuals(
                first_bite_segments
            )
            
            # Analyze reaction capture
            first_bite_metrics['reaction_capture'] = self._analyze_bite_reaction(
                first_bite_segments
            )

    except Exception as e:
        print(f"Error in first bite analysis: {str(e)}")

    return first_bite_metrics

def _analyze_course_progression(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze multi-course progression"""
    progression_metrics = {
        'course_transitions': 0.0,
        'pacing': 0.0,
        'flavor_journey': 0.0,
        'portion_progression': 0.0,
        'dining_rhythm': 0.0
    }

    try:
        course_segments = self._detect_course_segments(video_frames)
        
        if course_segments:
            # Analyze course transitions
            progression_metrics['course_transitions'] = self._analyze_course_transitions(
                course_segments
            )
            
            # Analyze pacing
            progression_metrics['pacing'] = self._analyze_course_pacing(
                course_segments
            )
            
            # Analyze flavor journey
            progression_metrics['flavor_journey'] = self._analyze_flavor_progression(
                course_segments
            )
            
            # Analyze portion progression
            progression_metrics['portion_progression'] = self._analyze_portion_progression(
                course_segments
            )
            
            # Analyze dining rhythm
            progression_metrics['dining_rhythm'] = self._analyze_dining_rhythm(
                course_segments
            )

    except Exception as e:
        print(f"Error in course progression analysis: {str(e)}")

    return progression_metrics
def _detect_ingredient_segments(self, video_frames: List[Dict]) -> List[Dict]:
    """Detect segments focusing on ingredient presentation"""
    ingredient_segments = []
    current_segment = None
    
    try:
        for i, frame in enumerate(video_frames):
            # Check for ingredient-focused frames
            if self._is_ingredient_focus(frame):
                if not current_segment:
                    current_segment = {
                        'start_frame': i,
                        'ingredients': [],
                        'characteristics': {}
                    }
                
                # Add ingredient details
                ingredients = self._identify_ingredients(frame)
                current_segment['ingredients'].extend(ingredients)
                
            elif current_segment:
                current_segment['end_frame'] = i
                current_segment['characteristics'] = self._analyze_ingredredient_characteristics(
                    video_frames[current_segment['start_frame']:i]
                )
                ingredient_segments.append(current_segment)
                current_segment = None
        
        # Handle final segment
        if current_segment:
            current_segment['end_frame'] = len(video_frames)
            current_segment['characteristics'] = self._analyze_ingredient_characteristics(
                video_frames[current_segment['start_frameame']:]
            )
            ingredient_segments.append(current_segment)
            
        return ingredient_segments
    except Exception:
        return []

def _analyze_freshness_indicators(self, segments: List[Dict]) -> float:
    """Analyze visual indicators of ingredient freshness"""
    freshness_scores = []
    
    try:
        for segment in segments:
            freshness_indicators = {
                'color_vibrancy': self._analyze_color_vibrancy(segment),
                'texture_quality': self._analyze_texture_quality(segment),
                'moisture_level': self._analyze_moisture_indicators(segment),
                'structural_integrity': self._analyze_structural_integrity(segment)
            }
            
            weights = {
                'color_vibrancy': 0.3,
                'texture_quality': 0.3,
                'moisture_level': 0.2,
                'structural_integrity': 0.2
            }
            
            score = sum(indicator * weights[metric] 
                       for metric, indicator in freshness_indicators.items())
            freshness_scores.append(score)
        
        return sum(freshness_scores) / len(freshnshness_scores) if freshness_scores else 0.0
    except Exception:
        return 0.0

def _analyze_ingredient_colors(self, segments: List[Dict]) -> float:
    """Analyze color variety in ingredients"""
    try:
        color_metrics = {
            'color_count': self._count_distinct_colors(segments),
            'color_contrast': self._analyze_color_contrast(segments),
            'color_harmony': self._analyze_color_harmony(segments),
            'color_distribution': self._analyze_color_distribution(segments)
        }
        
        weights = {
            'color_count': 0.3,
            'color_contrast': 0.3,
            'color_harmony': 0.2,
            'color_distribution': 0.2
        }
        
        return sum(metric * weights[key] for key, metric in color_metrics.items())
    except Exception:
        return 0.0

def _analyze_texture_visibility(self, segments: List[Dict]) -> float:
    """Analyze visibility and clarity of ingredient textures"""
    try:
        texture_metrics = {
            'surface_detail': self._analyze_surface_detail(segments),
            'lighting_quality': self._analyze_texture_lighting(segments),
            'focus_quality': self._analyze_texture_focus(segments),
            'angle_optimization': self._analyze_viewing_angles(segments)
        }
        
        weights = {
            'surface_detail': 0.3,
            'lighting_quality': 0.3,
            'focus_quality': 0.2,
            'angle_optimization': 0.2
        }
        
        return sum(metric * weights[key] for key, metric in texture_metrics.items())
    except Exception:
        return 0.0

def _analyze_size_demonstrations(self, segments: List[Dict]) -> float:
    """Analyze effectiveness of size comparisons"""
    try:
        size_metrics = {
            'reference_objects': self._detect_size_references(segments),
            'scale_clarity': self._analyze_scale_presentation(segments),
            'perspective_usage': self._analyze_size_perspective(segments),
            'measurement_indication': self._detect_measurements(segments)
        }
        
        weights = {
            'reference_objects': 0.3,
            'scale_clarity': 0.3,
            'perspective_usage': 0.2,
            'measurement_indication': 0.2
        }
        
        return sum(metric * weights[key] for key, metric in size_metrics.items())
    except Exception:
        return 0.0

def _detect_plating_segments(self, video_frames: List[Dict]) -> List[Dict]:
    """Detect segments focusing on plating presentation"""
    plating_segments = []
    current_segment = None
    
    try:
        for i, frame in enumerate(video_frames):
            if self._is_plating_action(frame):
                if not current_segment:
                    current_segment = {
                        'start_frame': i,
                        'plating_type': self._identify_plating_technique(frame),
                        'characteristics': {}
                    }
            elif current_segment:
                current_segment['end_frame'] = i
                current_segment['characteristics'] = self._analyze_plating_characteristics(
                    video_frames[current_segment['start_frame']:i]
                )
                plating_segments.append(current_segment)
                current_segment = None
        
        # Handle final segment
        if current_segment:
            current_segment['end_frame'] = len(video_frames)
            current_segment['characteristics'] = self._analyze_plating_characteristics(
                video_frames[current_segment['start_frame']:]
            )
            plating_segments.append(current_segment)
        
        return plating_segments
    except Exception:
        return []

def _analyze_color_balance(self, segments: List[Dict]) -> float:
    """Analyze color balance in plated presentation"""
    try:
        color_metrics = {
            'color_harmony': self._analyze_plate_color_harmony(segments),
            'contrast_levels': self._analyze_plate_contrast(segments),
            'color_distribution': self._analyze_plate_color_distribution(segments),
            'visual_appeal': self._analyze_color_appeal(segments)
        }
        
        weights = {
            'color_harmony': 0.3,
            'contrast_levels': 0.3,
            'color_distribution': 0.2,
            'visual_appeal': 0.2
        }
        
        return sum(metric * weights[key] for key, metric in color_metrics.items())
    except Exception:
        return 0.0
def _analyze_texture_variety(self, segments: List[Dict]) -> float:
    """Analyze variety of textures in plated presentation"""
    try:
        texture_metrics = {
            'texture_contrast': self._analyze_texture_contrasts(segments),
            'texture_layering': self._analyze_texture_layers(segments),
            'texture_distribution': self._analyze_texture_spread(segments),
            'textural_interest': self._analyze_texture_interest(segments)
        }
        
        weights = {
            'texture_contrast': 0.3,
            'texture_layering': 0.3,
            'texture_distribution': 0.2,
            'textural_interest': 0.2
        }
        
        return sum(metric * weights[key] for key, metric in texture_metrics.items())
    except Exception:
        return 0.0

def _analyze_plate_dimensions(self, segments: List[Dict]) -> float:
    """Analyze height and dimensional aspects of plating"""
    try:
        dimension_metrics = {
            'vertical_composition': self._analyze_vertical_build(segments),
            'layering_technique': self._analyze_layer_structure(segments),
            'height_variation': self._analyze_height_dynamics(segments),
            'spatial_balance': self._analyze_spatial_distribution(segments)
        }
        
        weights = {
            'vertical_composition': 0.3,
            'layering_technique': 0.3,
            'height_variation': 0.2,
            'spatial_balance': 0.2
        }
        
        return sum(metric * weights[key] for key, metric in dimension_metrics.items())
    except Exception:
        return 0.0

def _analyze_sauce_techniques(self, segments: List[Dict]) -> Dict[str, float]:
    """Analyze sauce application techniques"""
    try:
        return {
            'technique_variety': self._analyze_sauce_variety(segments),
            'precision': self._analyze_sauce_precision(segments),
            'artistic_quality': self._analyze_sauce_artistry(segments),
            'functionality': self._analyze_sauce_functionality(segments)
        }
    except Exception:
        return {
            'technique_variety': 0.0,
            'precision': 0.0,
            'artistic_quality': 0.0,
            'functionality': 0.0
        }

def _analyze_garnish_placement(self, segmentents: List[Dict]) -> Dict[str, float]:
    """Analyze garnish placement techniques"""
    try:
        return {
            'positioning': self._analyze_garnish_position(segments),
            'relevance': self._analyze_garnish_relevance(segments),
            'visual_impact': self._analyze_garnish_impact(segments),
            'integration': self._analyze_garnish_integration(segments)
        }
    except Exception:
        return {
            'positioning': 0.0,
            'relevance': 0.0,
            'visual_impact': 0.0,
            'integration': 0.0
        }

def _detect_first_bite_se_segments(self, video_frames: List[Dict]) -> List[Dict]:
    """Detect segments showing first bite presentation"""
    bite_segments = []
    current_segment = None
    
    try:
        for i, frame in enumerate(video_frames):
            if self._is_first_bite_moment(frame):
                if not current_segment:
                    current_segment = {
                        'start_frame': i,
                        'bite_type': self._identify_bite_type(frame),
                        'characteristics': {}
                    }
            elif current_segment:
                current_segment['end_frame'] = i
                current_segment['characteristics'] = self._analyze_bite_characteristics(
                    video_frames[current_segment['start_frame']:i]
                )
                bite_segments.append(current_segment)
                current_segment = None
        
        if current_segment:
            current_segment['end_frame'] = len(video_frames)
            current_segment['characteristics'] = self._analyze_bite_characteristics(
                video_frames[current_segment['start_frame']:]
            )
            bite_segments.append(current_segment)
        
        return bite_segments
    except Exception:
        return []

def _analyze_anticipation(self, segments: List[Dict]) -> float:
    """Analyze anticipation building in first bite presentation"""
    try:
        anticipation_metrics = {
            'visual_buildup': self._analyze_visual_anticipation(segments),
            'timing_pacing': self._analyze_anticipation_pacing(segments),
            'reaction_setup': self._analyze_reaction_setup(segments),
            'sensory_cues': self._analyze_sensory_buildup(segments)
        }
        
        weights = {
            'visual_buildup': 0.3,
            'timing_pacing': 0.3,
            'reaction_setup': 0.2,
            'sensory_cues': 0.2
        }
        
        return sum(metric * weights[key] for key, metric in anticipation_metrics.items())
    except Exception:
        return 0.0

def _analyze_course_progression(self, segments: List[Dict]) -> Dict[str, float]:
    """Analyze multi-course progression characteristics"""
    try:
        return {
            'course_flow': self._analyze_course_flow(segments),
            'timing_balance': self._analyze_course_timing(segments),
            'flavor_sequencing': self._analyze_flavor_sequence(segments),
            'portion_scaling': self._analyze_portion_scaling(segments),
            'presentation_evolution': self._analyze_presentation_progression(segments)
        }
    except Exception:
        return {
            'course_flow': 0.0,
            'timing_balance': 0.0,
            'flavor_sequencing': 0.0,
            'portion_scaling': 0.0,
            'presentation_evolution': 0.0
        }

def _analyze_dining_rhythm(self, segments: List[Dict]) -> float:
    """Analyze rhythm and pacing of dining experience"""
    try:
        rhythm_metrics = {
            'course_spacing': self._analyze_course_spacing(segments),
            'pace_variation': self._analyze_dining_pace(segments),
            'transition_smoothness': self._analyze_course_transitions(segments),
            'experience_flow': self._analyze_dining_flow(segments)
        }
        
        weights = {
            'course_spacing': 0.3,
            'pace_variation': 0.3,
            'transition_smoothness': 0.2,
            'experience_flow': 0.2
        }
        
        return sum(metric * weights[key] for key, metric in rhythm_metrics.items())
    except Exception:
        return 0.0

def _analyze_flavor_progression(self, segments: List[Dict]) -> float:
    """Analyze progression of flavors across courses"""
    try:
        progression_metrics = {
            'flavor_complexity': self._analyze_flavor_complexity(segments),
            'taste_balance': self._analyze_taste_balance(segments),
            'intensity_progression': self._analyze_intensity_progression(segments),
            'palate_development': self._analyze_palate_journey(segments)
        }
        
        weights = {
            'flavor_complexity': 0.3,
            'taste_balance': 0.3,
            'intensity_progression': 0.2,
            'palate_development': 0.2
        }
        
        return sum(metric * weights[key] for key, metric in progression_metrics.items())
    except Exception:
        return 0.0
def analyze_visual_storytelling(self, video_frames: List[Dict]) -> Dict[str, Any]:
    """Analyze visual storytelling elements"""
    return {
        'shot_composition': self._analyze_shot_composition(video_frames),
        'camera_movement': self._analyze_camera_movement(video_frames)
    }

def _analyze_shot_composition(self, video_frames: List[Dict]) -> Dict[str, Any]:
    """Analyze shot composition techniques"""
    return {
        'framing_techniques': self._analyze_framing_techniques(video_frames),
        'subject_positioning': self._analyze_subject_positioning(video_frames),
        'color_composition': self._analyze_color_composition(video_frames)
    }

def _analyze_framing_techniques(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze framing techniques used in shots"""
    framing_metrics = {
        'rule_of_thirds_application': 0.0,
        'leading_lines_usage': 0.0,
        'depth_creation': 0.0,
        'symmetry_vs_asymmetry': 0.0,
        'negative_space_utilization': 0.0
    }

    try:
        for frame in video_frames:
            # Analyze rule of thirds
            thirds_score = self._analyze_rule_of_thirds(frame)
            framing_metrics['rule_of_thirds_application'] = max(
                framing_metrics['rule_of_thirds_application'],
                thirds_score
            )
            
            # Analyze leading lines
            lines_score = self._detect_leading_lines(frame)
            framing_metrics['leading_lines_usageage'] = max(
                framing_metrics['leading_lines_usage'],
                lines_score
            )
            
            # Analyze depth
            depth_score = self._analyze_depth_elements(frame)
            framing_metrics['depth_creation'] = max(
                framing_metrics['depth_creation'],
                depth_score
            )
            
            # Analyze symmetry
            symmetry_score = self._analyze_frame_symmetry(frame)
            framiaming_metrics['symmetry_vs_asymmetry'] = max(
                framing_metrics['symmetry_vs_asymmetry'],
                symmetry_score
            )
            
            # Analyze negative space
            space_score = self._analyze_negative_space(frame)
            framing_metrics['negative_space_utilization'] = max(
                framing_metrics['negative_space_utilization'],
                space_score
            )

    except Exception as e:
        print(f"Error in framing analysis: {str(e)}")

    return framing_metrics

def _analyze_camera_movement(self, video_frames: List[Dict]) -> Dict[str, Any]:
    """Analyze camera movement techniques"""
    return {
        'static_shots': self._analyze_static_shots(video_frames),
        'dynamic_movement': self._analyze_dynamic_movement(video_frames),
        'transition_techniques': self._analyze_transition_techniques(video_frames)
    }

def _analyze_static_shots(self, video_frames: List[Dict]) -> Dict[str, float]:
    """AnaAnalyze static shot characteristics"""
    static_metrics = {
        'tripod_stability': 0.0,
        'framing_precision': 0.0,
        'duratiation_appropriateness': 0.0,
        'subject_movement_within_frame': 0.0,
        'intentional_stillness': 0.0
    }

    try:
        static_segments = self._identify_static_segments(video_frames)
        
        for segment in static_segments:
            # Analyze stability
            stability_score = self._analyze_frame_stability(segment)
            static_metrics['tripod_stability'] = max(
                static_metrics['tripod_stability'],
                stability_score
            )
            
            # Analyze framing precision
            precision_score = self._analyze_framing_precision(segment)
            static_metrics['framing_precision'] = max(
                static_metrics['framing_precision'],
                precision_score
            )
            
            # Analyze duration
            duration_score = self._analyze_shot_duration(segment)
            static_metrics['duration_appropriateness'] = max(
                static_metrics['duration_appropriateness'],
                duration_score
            )
            
            # Analyze subject movement
            movement_score = self._analyze_subject_movement(segment)
            static_metrics['subject_movement_within_frame'] = max(
                static_metrics['subject_movement_within_frame'],
                movement_score
            )
            
            # Analyze intentional stillness
            stillness_score = self._analyze_stillness_quality(segment)
            static_metrics['intentional_stillness'] = max(
                static_metrics['intentional_stillness'],
                stillness_score
            )

    except Exception as e:
        print(f"Error in static shot analysis: {str(e)}")

    return static_metrics

def _analyze_dynamic_movement(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze dynamic camera movement"""
    movement_metrics = {
        'pan_smoothness': 0.0,
        'tilt_purpose': 0.0,
        'dolly_movement_impact': 0.0,
        'handheld_shake_intentionality': 0.0,
        'gimbal_fluidity': 0.0
    }

    try:
        movement_segments = self._identify_movement_segments(video_frames)
        
        for segment in movement_segments:
            # Analyze pan movements
            pan_score = self._analyze_pan_movement(segment)
            movement_metrics['pan_smoothness'] = max(
                movement_metrics['pan_smoothness'],
                pan_score
            )
            
            # Analyze tilt movements
            tilt_score = self._analyze_tilt_movement(segment)
            movement_metrics['tilt_purpose'] = max(
                movement_metrics['tilt_purpose'],
                tilt_score
            )
            
            # Analyze dolly movements
            dolly_score = self._analyze_dolly_movement(segment)
            movement_metrics['dolly_movement_impact'] = max(
                movement_metrics['dolly_movement_impact'],
                dolly_score
            )
            
            # Analyze handheld characteristics
            handheld_score = self._analyze_handheld_quality(segment)
            movement_metrics['handheld_shake_intentionality'] = max(
                movement_metrics['handheld_shake_intentionality'],
                handheld_score
            )
            
            # Analyze gimbal movements
            gimbal_score = self._analyze_gimbal_movement(segment)
            movement_metrics['gimbal_fluidity'] = max(
                movement_metrics['gimbal_fluidity'],
                gimbal_score
            )

    except Exception as e:
        print(f"Error in dynamic movement analysis: {str(e)}")

    return movement_metrics
def _analyze_rule_of_thirds(self, frame: Dict) -> float:
    """Analyze adherence to rule of thirds"""
    try:
        # Get frame dimensions
        height = frame.get('Height', 1)
        width = frame.get('Width', 1)
        
        # Calculate third lines
        h_thirds = [height/3, 2*height/3]
        v_thirds = [width/3, 2*width/3]
        
        # Get subject bounding boxes
        subjects = self._detect_main_subjects(frame)
        
        thirds_score = 0.0
        for subject in subjects:
            # Calculate subject center
            center_x = subject['BoundingBox']['Left'] + subject['BoundingBox']['Width'] / 2
            center_y = subject['BoundingBox']['Top'] + subject['BoundingBox']['Height'] / 2
            
            # Check proximity to third lines
            h_proximity = min(abs(center_y - third) for third in h_thirds)
            v_proximity = min(abs(center_x - third) for third in v_thirds)
            
            # Score based on proximity to intersections
            intersection_score = self._calculate_intersection_proximity(
                center_x, center_y, h_thirds, v_thirds
            )
            
            thirds_score = max(thirds_score, intersection_score)
        
        return thirds_score
    except Exception:
        return 0.0

def _detect_leading_lines(self, frame: Dict) -> float:
    """Detect and analyze leading lines in frame"""
    try:
        # Detect edges in frame
        edges = self._detect_edges(frame)
        
        # Analyze line characteristics
        line_metrics = {
            'convergence': self._analyze_line_convergence(edges),
            'direction': self._analyze_line_direction(edges),
            'strength': self._analyze_line_strength(edges),
            'subject_connection': self._analyze_line_subject_connection(edges, frame)
        }
        
        weights = {
            'convergence': 0.3,
            'direction': 0.3,
            'strength': 0.2,
            'subject_connection': 0.2
        }
        
        return sum(score * weights[metric] for metric, score in line_metrics.items())
    except Exception:
        return 0.0

def _analyze_depth_elements(self, frame: Dict) -> float:
    """Analyze depth creation in frame"""
    try:
        depth_metrics = {
            'foreground_presence': self._analyze_foreground(frame),
            'midground_presence': self._analyze_midground(frame),
            'background_presence': self._analyze_background(frame),
            'depth_separation': self._analyze_depth_separation(frame),
            'perspective_lines': self._analyze_perspective_lines(frame)
        }
        
        weights = {  # Renamed from weightshts to weights for consistency
            'foreground_presence': 0.2,
            'midground_presence': 0.3,
            'background_presence': 0.2,
            'depth_separation': 0.15,
            'perspective_lines': 0.15
        }
        
        return sum(score * weights[metric] for metric, score in depth_metrics.items())
    except Exception:
        return 0.0

def _analyze_frame_symmetry(self, frame: Dict) -> float:
    """Analyze symmetry vs asymmetry in frame"""
    try:
        symmetry_metrics = {
            'vertical_symmetry': self._analyze_vertical_symmetry(frame),
            'horizontal_symmetry': self._analyze_horizontal_symmetry(frame),
            'radial_symmetry': self._analyze_radial_symmetry(frame),
            'intentional_asymmetry': self._analyze_intentional_asymmetry(frame)
        }
        
        return max(symmetry_metrics.values())
    except Exception:
        return 0.0

def _analyze_negative_space(self, frame: Dict) -> float:
    """Analyze use of negative space"""
    try:
        # Detect subject areas
        subjects = self._detect_main_subjects(frame)
        
        # Calculate total subject area
        total_subject_area = sum(
            subject['BoundingBox']['Width'] * subject['BoundingBox']['Height']
            for subject in subjects
        )
        
        # Calculate frame area
        frame_area = frame.get('Width', 1) * frame.get('Height', 1)
        
        # Calculate negative space ratio
        negative_space_ratio = 1 - (total_subject_area / frame_area)
        
        # Analyze negative space quality
        space_metrics = {
            'ratio': negative_space_ratio * 100,
            'balance': self._analyze_space_balance(frame, subjects),
            'intentionality': self._analyze_space_intentionality(frame, subjects),
            'effectiveness': self._analyze_space_effectiveness(frame, subjects)
        }
        
        weights = {
            'ratio': 0.3,
            'balance': 0.3,
            'intentionality': 0.2,
            'effectiveness': 0.2
        }
        
        return sum(score * weights[metric] for metric, score in space_metrics.items())
    except Exception:
        return 0.0

def _identify_static_segments(self, video_frames: List[Dict]) -> List[Dict]:
    """Identify segments with static camera work"""
    static_segments = []
    current_segment = None
    
    try:
        for i, frame in enumerate(video_frames):
            movement = self._calculate_frame_movement(
                frame, video_frames[i-1] if i > 0 else None
            )
            
            if movement < 0.1:  # Threshold for static shot
                if not current_segment:
                    current_segment = {
                        'start_frame': i,
                        'frames': []
                    }
                current_segment['frames'].append(frame)
            elif current_segment:
                current_segment['end_frame'] = i
                static_segments.append(current_segment)
                current_segment = None
        
        # Handle final segment
        if current_segment:
            current_segment['end_frame'] = len(video_frames)
            static_segments.append(current_segment)
        
        return static_segments
    except Exception:
        return []

def _analyze_frame_stability(self, segment: Dict) -> float:
    """Analyze stability of static shots"""
    try:
        stability_metrics = {
            'motion_consistency': self._analyze_motion_consistency(segment),
            'edge_stability': self._analyze_edge_stability(segment),
            'feature_tracking': self._analyze_feature_stability(segment),
            'horizon_level': self._analyze_horizon_stability(segment)
        }
        
        weights = {
            'motion_consistency': 0.3,
            'edge_stability': 0.3,
            'feature_tracking': 0.2,
            'horizon_level': 0.2
        }
        
        return sum(score * weights[metric] for metric, score in stability_metrics.items())
    except Exception:
        return 0.0

def _analyze_framing_precision(self, segment: Dict) -> float:
    """Analyze precision of frame composition"""
    try:
        precision_metrics = {
            'subject_placement': self._analyze_subject_placement(segment),
            'frame_balance': self._analyze_frame_balance(segment),
            'border_management': self._analyze_border_management(segment),
            'composition_maintenance': self._analyze_composition_maintenance(segment)
        }
        
        weights = {
            'subject_placement': 0.3,
            'frame_balance': 0.3,
            'border_management': 0.2,
            'composition_maintenance': 0.2
        }
        
        return sum(score * weights[metric] for metric, score in precision_metrics.items())
    except Exception:
        return 0.0

def _identify_movement_segments(self, video_frames: List[Dict]) -> List[Dict]:
    """Identify segments with intentional camera movement"""
    movement_segments = []
    current_segment = None
    
    try:
        for i, frame in enumerate(video_frames):
            movement = self._calculate_frame_movement(frame, video_frames[i-1] if i > 0 else None)
            movement_type = self._classify_movement_type(movement, frame)
            
            if movement_type != 'static':
                if not current_segment:
                    current_segment = {
                        'start_frame': i,
                        'movement_type': movement_type,
                        'frames': []
                    }
                current_segment['frames'].append(frame)
            elif current_segment:
                current_segment['end_frame'] = i
                movement_segments.append(current_segment)
                current_segment = None
        
        if current_segment:
            current_segment['end_frame'] = len(video_frames)
            movement_segments.append(current_segment)
        
        return movement_segments
    except Exception:
        return []

def _analyze_pan_movement(self, segment: Dict) -> float:
    """Analyze horizontal pan movement quality"""
    try:
        pan_metrics = {
            'smoothness': self._analyze_pan_smoothness(segment),
            'speed_consistency': self._analyze_pan_speed(segment),  # Fixed: removed extra "ed("
            'direction_stability': self._analyze_pan_direction(segment),
            'start_end_control': self._analyze_pan_control(segment)
        }
        
        weights = {
            'smoothness': 0.3,
            'speed_consistency': 0.3,
            'direction_stability': 0.2,
            'start_end_control': 0.2
        }
        
        return sum(score * weights[metric] for metric, score in pan_metrics.items())
    except Exception:
        return 0.0

def _analyze_tilt_movement(self, segment: Dict) -> float:
    """Analyze vertical tilt movement quality"""
    try:
        tilt_metrics = {
            'smoothness': self._analyze_tilt_smoothness(segment),
            'speed_consistency': self._analyze_tilt_speed(segment),
            'angle_maintenance': self._analyze_tilt_angle(segment),
            'purpose_clarity': self._analyze_tilt_purpose(segment)
        }
        
        weights = {
            'smoothness': 0.3,
            'speed_consistency': 0.3,
            'angle_maintenance': 0.2,
            'purpose_clarity': 0.2
        }
        
        return sum(score * weights[metric] for metric, score in tilt_metrics.items())
    except Exception:
        return 0.0

def _analyze_dolly_movement(self, segment: Dict) -> float:
    """Analyze dolly (in/out) movement quality"""
    try:
        dolly_metrics = {
            'motion_smoothness': self._analyze_dolly_smoothness(segment),  # Fixed key name
            'speed_control': self._analyze_dolly_speed(segment),
            'subject_tracking': self._analyze_dolly_tracking(segment),
            'depth_impact': self._analyze_dolly_depth_effect(segment)
        }
        
        weights = {
            'motion_smoothness': 0.3,
            'speed_control': 0.3,
            'subject_tracking': 0.2,
            'depth_impact': 0.2
        }
  
        return sum(score * weights[metric] for metric, score in dolly_metrics.items())
    except Exception:
        return 0.0

def _analyze_handheld_quality(self, segment: Dict) -> float:
    """Analyze handheld camera movement quality"""
    try:
        handheld_metrics = {
            'shake_intentionality': self._analyze_shake_purpose(segment),
            'stability_control': self._analyze_stability_control(segment),
            'movement_naturalness': self._analyze_movement_naturalness(segment),
            'energy_contribution': self._analyze_movement_energy(segment)
        }
        
        weights = {
            'shake_intentionality': 0.3,
            'stability_control': 0.3,
            'movement_naturalness': 0.2,
            'energy_contribution': 0.2
        }
        
        return sum(score * weights[metric] for metric, score in handheld_metrics.items())
    except Exception:
        return 0.0

def _analyze_gimbal_movement(self, segment: Dict) -> float:
    """Analyze gimbal-stabilized movement quality"""
    try:
        gimbal_metrics = {
            'stabilization_quality': self._analyze_gimbal_stability(segment),
            'movement_fluidity': self._analyze_movement_fluidity(segment),
            'rotation_smoothness': self._analyze_rotation_smoothness(segment),
            'transition_quality': self._analyze_movement_transitions(segment)
        }
        
        weights = {
            'stabilization_quality': 0.3,
            'movement_fluidity': 0.3,
            'rotation_smoothness': 0.2,
            'transition_quality': 0.2
        }
        
        return sum(score * weights[metric] for metric, score in gimbal_metrics.items())
    except Exception:
        return 0.0

def _analyze_border_management(self, segment: Dict) -> float:
    """Analyze management of frame borders"""
    try:
        border_metrics = {
            'edge_clearance': self._analyze_edge_clearance(segment),
            'border_activity': self._analyze_border_activity(segment),
            'frame_containment': self._analyze_frame_containment(segment),
            'edge_tension': self._analyze_edge_tension(segment)
        }
        
        weights = {
            'edge_clearance': 0.3,
            'border_activity': 0.2,
            'frame_containment': 0.3,
            'edge_tension': 0.2
        }
        
        return sum(score * weights[metric] for metric, score in border_metrics.items())
    except Exception:
        return 0.0

def _analyze_composition_maintenance(self, segment: Dict) -> float:
    """Analyze maintenance of composition over time"""
    try:
        maintenance_metrics = {
            'subject_tracking': self._analyze_subject_tracking(segment),
            'composition_stability': self._analyze_composition_stability(segment),
            'framing_consistency': self._analyze_framing_consistency(segment),
            'dynamic_recomposition': self._analyze_dynamic_recomposition(segment)
        }
        
        weights = {
            'subject_tracking': 0.3,
            'composition_stability': 0.3,
            'framing_consistency': 0.2,
            'dynamic_recomposition': 0.2
        }
        
        return sum(score * weights[metric] for metric, score in maintenance_metrics.items())
    except Exception:
        return 0.0

def _analyze_motion_consistency(self, segment: Dict) -> float:
    """Analyze consistency of motion in static shots"""
    try:
        frames = segment.get('frames', [])
        if len(frames) < 2:
            return 0.0
            
        motion_variations = []
        for i in range(1, len(frames)):
            motion = self._calculate_frame_movement(frames[i], frames[i-1])
            motion_variations.append(motion)
            
        if motion_variations:
            avg_motion = sum(motion_variations) / len(motion_variations)
            variance = sum((m - avg_motion)**2 for m in motion_variations) / len(motion_variations)
            
            # Convert variance to consistency score (inverse relationship)
            consistency = 1 / (1 + variance)
            return consistency * 100
        return 0.0
    except Exception:
        return 0.0

def _analyze_edge_stability(self, segment: Dict) -> float:
    """Analyze stability of frame edges"""
    try:
        edge_metrics = {
            'horizontal_stability': self._analyze_horizontal_edges(segment),
            'vertical_stability': self._analyze_vertical_edges(segment),
            'corner_stability': self._analyze_corner_stability(segment),
            'edge_parallelism': self._analyze_edge_parallelism(segment)
        }
        
        weights = {
            'horizontal_stability': 0.3,
            'vertical_stability': 0.3,
            'corner_stability': 0.2,
            'edge_parallelism': 0.2
        }
        
        return sum(score * weights[metric] for metric, score in edge_metrics.items())
    except Exception:
        return 0.0

def _analyze_feature_stability(self, segment: Dict) -> float:
    """Analyze stability of tracked features"""
    try:
        feature_metrics = {
            'point_stability': self._analyze_point_stability(segment),
            'feature_drift': self._analyze_feature_drift(segment),
            'tracking_consistency': self._analyze_tracking_consistency(segment),
            'feature_retention': self._analyze_feature_retention(segment)
        }
        
        weights = {
            'point_stability': 0.3,
            'feature_drift': 0.3,
            'tracking_consistency': 0.2,
            'feature_retention': 0.2
        }
        
        return sum(score * weights[metric] for metric, score in feature_metrics.items())
    except Exception:
        return 0.0

def _analyze_horizon_stability(self, segment: Dict) -> float:
    """Analyze stability of horizon line"""
    try:
        horizon_metrics = {
            'angle_stability': self._analyze_horizon_angle(segment),
            'level_consistency': self._analyze_level_consistency(segment),
            'roll_variation': self._analyze_roll_variation(segment),
            'horizon_detection': self._analyze_horizon_detection(segment)
        }
        
        weights = {
            'angle_stability': 0.3,
            'level_consistency': 0.3,
            'roll_variation': 0.2,
            'horizon_detection': 0.2
        }
        
        return sum(score * weights[metric] for metric, score in horizon_metrics.items())
    except Exception:
        return 0.0

def _extract_feature_points(self, frame: Dict) -> List[Dict]:
    """Extract trackable feature points from frame"""
    try:
        feature_points = []
        
        # Extract facial landmarks
        for face in frame.get('FaceDetails', []):
            for landmark in face.get('Landmarks', []):
                feature_points.append({
                    'x': landmark['X'],
                    'y': landmark['Y'],
                    'type': 'facial_landmark',
                    'confidence': face.get('Confidence', 0)
                })
        
        # Extract object keypoints
        for label in frame.get('Labels', []):
            for instance in label.get('Instances', []):
                if 'KeyPoints' in instance:
                    for keypoint in instance['KeyPoints']:
                        feature_points.append({
                            'x': keypoint['X'],
                            'y': keypoint['Y'],
                            'type': 'object_keypoint',
                            'confidence': instance.get('Confidence', 0)
                        })
        
        return [pt for pt in feature_points if pt['confidence'] > self.confidence_threshold]
    except Exception:
        return []

def _classify_movement_type(self, movement: float, frame: Dict) -> str:
    """Classify type of camera movement"""
    try:
        if movement < 0.1:
            return 'static'
        
        # Analyze movement direction and characteristics
        movement_characteristics = self._analyze_movement_characteristics(frame)
        
        if movement_characteristics['horizontal_movement'] > movement_characteristics['vertical_movement']:
            return 'pan'
        elif movement_characteristics['vertical_movement'] > movement_characteristics['horizontal_movement']:
            return 'tilt'
        elif movement_characteristics['depth_movement'] > max(
            movement_characteristics['horizontal_movement'],
            movement_characteristics['vertical_movement']
        ):
            return 'dolly'
        elif movement_characteristics['stabilized']:
            return 'gimbal'
        else:
            return 'handheld'
    except Exception:
        return 'unknown'
def analyze_restaurant_showcase(self, video_frames: List[Dict]) -> Dict[str, Any]:
    """Analyze restaurant showcase features"""
    return {
        'ambiance_and_setting': {
            'exterior_presentation': self._analyze_exterior_presentation(video_frames),
            'interior_design': self._analyze_interior_design(video_frames)
        },
        'food_interaction': self._analyze_food_interaction(video_frames)
    }

def _analyze_exterior_presentation(self, video_frames: List[Dict]) -> Dict[str, Any]:
    """Analyze restaurant exterior presentation"""
    exterior_metrics = {
        'facade_appeal': self._analyze_facade_appeal(video_frames),
        'surrounding_area': self._analyze_surrounding_area(video_frames)
    }
    
    return exterior_metrics

def _analyze_facade_appeal(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze restaurant facade appeal"""
    try:
        facade_metrics = {
            'architectural_style': self._analyze_architectural_style(video_frames),
            'signage_visibility': self._analyze_signage_visibility(video_frames),
            'lighting_design': self._analyze_exterior_lighting(video_frames),
            'outdoor_seating': self._analyze_outdoor_seating(video_frames),
            'entrance_accessibility': self._analyze_entrance_accessibility(video_frames)
        }
        
        # Detect exterior shots
        exterior_shots = self._detect_exterior_shots(video_frames)
        if exterior_shots:
            for metric in facade_metrics:
                facade_metrics[metric] = self._calculate_metric_score(
                    exterior_shots,
                    metric
                )
                
        return facade_metrics
    except Exception as e:
        print(f"Error in facade analysis: {str(e)}")
        return {key: 0.0 for key in ['architectural_style', 'signage_visibility', 
                                    'lighting_design', 'outdoor_seating', 
                                    'entrance_accessibility']}

def _analyze_surrounding_area(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze restaurant surroundings"""
    try:
        surrounding_metrics = {
            'neighborhood_character': self._analyze_neighborhood(video_frames),
            'parking_availability': self._analyze_parking_area(video_fo_frames),
            'pedestrian_traffic': self._analyze_pedestrian_flow(video_frames),
            'complementary_businesses': self._analyze_nearby_businesses(video_frames),
            'scenic_views': self._analyze_scenic_quality(video_frames)
        }
        
        return surrounding_metrics
    except Exception as e:
        print(f"Error in surroundings analysis: {str(e)}")
        return {key: 0.0 for key in ['neighborhood_character', 'parking_availability',
                                    'pedestrian_traffic', 'complementary_businesses',
                                    'scenic_views']}

def _analyze_interior_design(self, video_frames: List[Dict]) -> Dict[str, Any]:
    """Analyze restaurant interior design"""
    return {
        'seating_arrangement': self._analyze_seating_arrangement(video_frames),
        'lighting_scheme': self._analyze_lighting_scheme(video_frames),
        'decor_elements': self._analyze_decor_elemenments(video_frames)
    }

def _analyze_seating_arrangement(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze restaurant seating arrangement"""
    try:
        seating_metrics = {
            'table_spacing': self._analyze_table_spacing(video_frames),
            'booth_vs_table_ratio': self._analyze_seating_ratio(video_frames),
            'bar_seating_design': self._analyze_bar_seating(video_frames),
            'private_dining_options': self._analyze_private_dining(video_frames),
            'communal_table_presence': self._analyze_communal_seating(video_frames)
        }
        
        return seating_metrics
    except Exception as e:
        print(f"Error in seating analysis: {str(e)}")
        return {key: 0.0 for key in ['table_spacing', 'booth_vs_table_ratio',
                                    'bar_seating_design', 'private_dining_options',
                                    'communal_table_presence']}

def _analyze_lighting_scheme(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze restaurant lighting scheme"""
    try:
        lighting_metrics = {
            'ambient_lighting_level': self._analyze_ambient_lighting(video_frames),
            'accent_lighting_usage': self._analyze_accent_lighting(video_frames),
            'natural_light_integration': self._analyze_natural_light(video_frames),
            'mood_setting_through_light': self._analyze_lighting_mood(video_frames),
            'table_lighting': self._analyze_table_lighting(video_frames)
        }
        
        return lighting_metrics
    except Exception as e:
        print(f"Error in lighting analysis: {str(e)}")
        return {key: 0.0 for key in ['ambient_lighting_level', 'accent_lighting_usage',
                                    'natural_light_integration', 'mood_setting_through_light',
                                    'table_lighting']}

def _analyze_decor_elements(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze restaurant decor elements"""
    try:
        decor_metrics = {
            'color_scheme': self._analyze_interior_colors(video_frames),
            'artwork_selection': self._analyze_artwork(video_frames),
            'plant_life_integration': self._analyze_plant_presence(video_frames),
            'thematic_consistency': self._analyze_theme_consistency(video_frames),
            'local_culture_representation': self._analyze_cultural_elements(video_frames)
        }
        
        return decor_metrics
    except Exception as e:
        print(f"Error in decor analysis: {str(e)}")
        return {key: 0.0 for key in ['color_scheme', 'artwork_selection',
                                    'plant_life_integration', 'thematic_consistency',
                                    'local_culture_representation']}

def _detect_exterior_shots(self, video_frames: List[Dict]) -> List[Dict]:
    """Detect frames showing restaurant exterior"""
    exterior_frames = []
    
    try:
        for frame in video_frames:
            # Check for outdoor scene indicators
            labels = frame.get('Labels', [])
            outdoor_indicators = ['building', 'architecture', 'street', 'outdoor']
            
            if any(label['Name'].lower() in outdoor_indicators 
                  for label in labels 
                  if label['Confidence'] > self.confidence_threshold):
                exterior_frames.append(frame)
        
        return exterior_frames
    except Exception:
        return []
def _analyze_architectural_style(self, video_frames: List[Dict]) -> float:
    """Analyze architectural style of restaurant facade"""
    try:
        style_indicators = {
            'modern': self._detect_modern_elements(video_frames),
            'traditional': self._detect_traditional_elements(video_frames),
            'rustic': self._detect_rustic_elements(video_frames),
            'industrial': self._detect_industrial_elements(video_frames),
            'contemporary': self._detect_contemporary_elements(video_frames)
        }
        
        # Calculate dominant style score
        return max(style_indicators.values())
    except Exception:
        return 0.0

def _analyze_signage_visibility(self, video_frames: List[Dict]) -> float:
    """Analyze visibility and effectiveness of restaurant signage"""
    try:
        signage_metrics = {
            'size_appropriateness': self._analyze_sign_size(video_frames),
            'lighting_quality': self._analyze_sign_lighting(video_frames),
            'readability': self._analyze_sign_readability(video_frames),
            'placement_effectiveness': self._analyze_sign_placement(video_frames),
            'brand_consistency': self._analyze_sign_branding(video_frames)
        }
        
        weights = {
            'size_appropriateness': 0.25,
            'lighting_quality': 0.2,
            'readability': 0.25,
            'placement_effectiveness': 0.15,
            'brand_consistency': 0.15
        }
        
        return sum(score * weights[metric] for metric, score in signage_metrics.items())
    except Exception:
        return 0.0

def _analyze_exterior_lighting(self, video_frames: List[Dict]) -> float:
    """Analyze exterior lighting design"""
    try:
        lighting_metrics = {
            'facade_illumination': self._analyze_facade_lighting(video_frames),
            'entrance_highlighting': self._analyze_entrance_lighting(video_frames),
            'signage_illumination': self._analyze_signage_lighting(video_frames),
            'ambient_lighting': self._analyze_ambient_exterior_light(video_frames),
            'decorative_lighting': self._analyze_decorative_lights(video_frames)
        }
        
        weights = {
            'facade_illumination': 0.25,
            'entrance_highlighting': 0.2,
            'signage_illumination': 0.25,
            'ambient_lighting': 0.15,
            'decorative_lighting': 0.15
        }
        
        return sum(score * weights[metric] for metric, score in lighting_metrics.items())
    except Exception:
        return 0.0

def _analyze_outdoor_seating(self, video_frames: List[Dict]) -> float:
    """Analyze outdoor seating arrangement"""
    try:
        seating_metrics = {
            'layout_functionality': self._analyze_outdoor_layout(video_frames),
            'comfort_elements': self._analyze_seating_comfort(video_frames),
            'weather_protection': self._analyze_weather_accommodation(video_frames),
            'ambiance_quality': self._analyze_outdoor_ambiance(video_frames),
            'integration_with_facade': self._analyze_seating_integration(video_frames)
        }
        
        weights = {
            'layout_functionality': 0.25,
            'comfort_elements': 0.2,
            'weather_protection': 0.2,
            'ambiance_quality': 0.2,
            'integration_with_facade': 0.15
        }
        
        return sum(score * weights[metric] for metric, score in seating_metrics.items())
    except Exception:
        return 0.0

def _analyze_entrance_accessibiibility(self, video_frames: List[Dict]) -> float:
    """Analyze accessibility of restaurant entrance"""
    try:
        accessibility_metrics = {
            'physical_accessibility': self._analyze_physical_access(video_frames),
            'visibility_from_street': self._analyze_entrance_visibility(video_frames),
            'traffic_flow': self._analyze_entrance_traffic(video_frames),
            'signage_clarity': self._analyze_entrance_signage(video_frames),
            'safety_features': self._analyze_entrance_safety(video_frames)
        }
        
        weights = {
            'physical_accessibility': 0.3,
            'visibility_from_street': 0.2,
            'traffic_flow': 0.2,
            'signage_clarity': 0.15,
            'safety_features': 0.15
        }
        
        return sum(score * weights[metric] for metric, score in accessibility_metrics.items())
    except Exception:
        return 0.0

def _analyze_neighborhood(self, video_frames: List[Dict]) -> float:
    """Analyze neighborhood characteristics"""
    try:
        neighborhood_metrics = {
            'area_type': self._analyze_area_type(video_frames),
            'foot_traffic': self._analyze_foot_traffic(video_frames),
            'surrounding_businesses': self._analyze_business_mix(video_frames),
            'cleanliness': self._analyze_area_cleanliness(video_frames),
            'safety_indicators': self._analyze_safety_factors(video_frames)
        }
        
        weights = {
            'area_type': 0.25,
            'foot_traffic': 0.2,
            'surrounding_businesses': 0.2,
            'cleanliness': 0.2,
            'safety_indicators': 0.15
        }
        
        return sum(score * weights[metric] for metric, score in neighborhood_metrics.items())
    except Exception:
        return 0.0

def _analyze_table_spacing(self, video_frames: List[Dict]) -> float:
    """Analyze spacing between tables"""
    try:
        spacing_metrics = {
            'distance_between_tables': self._calculate_table_distances(video_frames),
            'traffic_flow_space': self._analyze_traffic_paths(video_frames),
            'privacy_considerations': self._analyze_table_privacy(video_frames),
            'seating_density': self._calculate_seating_density(video_frames),
            'service_accessibility': self._analyze_service_access(video_frames)
        }
        
        weights = {
            'distance_between_tables': 0.3,
            'traffic_flow_space': 0.2,
            'privacy_considerations': 0.2,
            'seating_density': 0.15,
            'service_accessibility': 0.15
        }
        
        return sum(score * weights[metric] for metric, score in spacing_metrics.items())
    except Exception:
        return 0.0

def _analyze_ambient_lighting(self, video_frames: List[Dict]) -> float:
    """Analyze ambient lighting in restaurant"""
    try:
        lighting_metrics = {
            'overall_brightness': self._analyze_brightness_level(video_frames),
            'light_distribution': self._analyze_light_distribution(video_frames),
            'color_temperature': self._analyze_light_temperature(video_frames),
            'shadow_management': self._analyze_shadow_patterns(video_frames),
            'mood_creation': self._analyze_lighting_mood(video_frames)
        }
        
        weights = {
            'overall_brightness': 0.25,
            'light_distribution': 0.2,
            'color_temperature': 0.2,
            'shadow_management': 0.15,
            'mood_creation': 0.2
        }
        
        return sum(score * weights[metric] for metric, score in lighting_metrics.items())
    except Exception:
        return 0.0

def _analyze_interior_colors(self, video_frames: List[Dict]) -> float:
    """Analyze interior color scheme"""
    try:
        color_metrics = {
            'color_palette': self._analyze_color_palette(video_frames),
            'color_harmony': self._analyze_color_harmony(video_frames),
            'brand_alignment': self._analyze_brand_color_alignment(video_frames),
            'mood_impact': self._analyze_color_mood_impact(video_frames),
            'cultural_relevance': self._analyze_color_cultural_fit(video_frames)
        }
        
        weights = {
            'color_palette': 0.25,
            'color_harmony': 0.2,
            'brand_alignment': 0.2,
            'mood_impact': 0.2,
            'cultural_relevance': 0.15
        }
        
        return sum(score * weights[metric] for metric, score in color_metrics.items())
    except Exception:
        return 0.0

def _detect_modern_elements(self, video_frames: List[Dict]) -> float:
    """Detect modern architectural elements"""
    try:
        modern_indicators = {
            'clean_lines': self._detect_clean_lines(video_frames),
            'glass_usage': self._detect_glass_elements(video_frames),
            'minimalist_design': self._detect_minimalism(video_frames),
            'metal_elements': self._detect_metal_materials(video_frames),
            'contemporary_lightinging': self._detect_modern_lighting(video_frames)
        }
        
        return sum(score for score in modern_indicators.values()) / len(modern_indicators)
    except Exception:
        return 0.0

def _analyze_sign_size(self, video_frames: List[Dict]) -> float:
    """Analyze appropriateness of signage size"""
    try:
        # Detect signage in frames
        signs = self._detect_signage(video_frames)
        if not signs:
            return 0.0
            
        size_metrics = {
            'relative_size': self._calculate_relative_size(signs),
            'visibility_distance': self._estimate_visibility_distance(signs),
            'proportion_to_facade': self._calculate_facade_proportion(signs),
            'readability_factor': self._calculate_readability_factor(signs)
        }
        
        return sum(score for score in size_metrics.values()) / len(size_metrics)
    except Exception:
        return 0.0

def _analyze_facade_lighting(self, video_frames: List[Dict]) -> float:
    """Analyze facade lighting quality"""
    try:
        lighting_aspects = {
            'coverage': self._analyze_light_coverage(video_frames),
            'intensity': self._analyze_light_intensity(video_frames),
            'uniformity': self._analyze_light_uniformity(video_frames),
            'highlighting': self._analyze_architectural_highlighting(video_frames),
            'shadow_control': self._analyze_shadow_control(video_frames)
        }
        
        weights = {
            'coverage': 0.25,
            'intensity': 0.2,
            'uniformity': 0.2,
            'highlighting': 0.2,
            'shadow_control': 0.15
        }
        
        return sum(score * weights[metric] for metric, score in lighting_aspects.items())
    except Exception:
        return 0.0

def _analyze_outoutdoor_layout(self, video_frames: List[Dict]) -> float:
    """Analyze functionality of outdoor seating layout"""
    try:
        layout_aspects = {
            'space_efficiency': self._analyze_space_usage(video_frames),
            'traffic_flow': self._analyze_customer_flow(video_frames),
            'accessibility': self._analyze_seating_accessibility(video_frames),
            'privacy': self._analyze_seating_privacy(video_frames),
            'view_optimization': self._analyze_view_ew_quality(video_frames)
        }
        
        return sum(score for score in layout_aspects.values()) / len(layout_aspects)
    except Exception:
        return 0.0

def _analyze_physical_access(self, video_frames: List[Dict]) -> float:
    """Analyze physical accessibility features"""
    try:
        access_features = {
            'ramp_presence': self._detect_ramps(video_frames),
            'door_width': self._analyze_door_width(video_frames),
            'step_height': self._analyze_step_height(video_frames),
            'landing_space': self._analyze_landing_area(video_frames),
            'handrail_presence': self._detect_handrails(video_frames)
        }
        
        weights = {
            'ramp_presence': 0.3,
            'door_width': 0.25,
            'step_height': 0.15,
            'landing_space': 0.15,
            'handrail_presence': 0.15
        }
        
        return sum(score * weights[metric] for metric, score in access_features.items())
    except Exception:
        return 0.0

def _analyze_area_type(self, video_frames: List[Dict]) -> float:
    """Analyze type and character of surrounding area"""
    try:
        area_characteristics = {
            'urban_density': self._analyze_urban_density(video_frames),
            'commercial_activity': self._analyze_commercial_presence(video_frames),
            'residential_mix': self._analyze_residential_presence(video_frames),
            'pedestrian_friendliness': self._analyze_walkability(video_frames),
            'area_maintenance': self._analyze_area_upkeep(video_frames)
        }
        
        return sum(score for score in area_characteristics.values()) / len(area_characteristics)
    except Exception:
        return 0.0

def _calculate_table_distances(self, video_frames: List[Dict]) -> float:
    """Calculate and analyze distances between tables"""
    try:
        # Detect tables in frames
        tables = self._detect_tables(video_frames)
        if not tables:
            return 0.0
            
        distance_metrics = {
            'minimum_spacing': self._calculate_minimum_spacing(tables),
            'average_spacing': self._calculate_average_spacing(tables),
            'circulation_space': self._analyze_circulation_space(tables),
            'emergency_access': self._analyze_emergency_routes(tables)
        }
        
        weights = {
            'minimum_spacing': 0.3,
            'average_spacing': 0.3,
            'circulation_space': 0.2,
            'emergency_access': 0.2
        }
        
        return sum(score * weights[metric] for metric, score in distance_metrics.items())
    except Exception:
        return 0.0

def _analyze_brightness_level(self, video_frames: List[Dict]) -> float:
    """Analyze overall brightness levels in space"""
    try:
        brightness_metrics = {
            'average_luminosity': self._calculate_average_luminosity(video_frames),
            'brightness_distribution': self._analyze_brightness_distribution(video_frames),
            'contrast_levels': self._analyze_contrast_levels(video_frames),
            'glare_control': self._analyze_glare_presence(video_frames),
            'dark_spot_management': self._analyze_dark_spots(video_frames)
        }
        
        weights = {
            'average_luminosity': 0.25,
            'brightness_distribution': 0.25,
            'contrast_levels': 0.2,
            'glare_control': 0.15,
            'dark_spot_management': 0.15
        }
        
        return sum(score * weights[metric] for metric, score in brightness_metrics.items())
    except Exception:
        return 0.0

def _analyze_color_palette(self, video_frames: List[Dict]) -> float:
    """Analyze interior color palette"""
    try:
        palette_metrics = {
            'color_scheme_coherence': self._analyze_color_coherence(video_frames),
            'dominant_colors': self._identify_dominant_colors(video_frames),
            'accent_colors': self._analyze_accent_colors(video_frames),
            'color_balance': self._analyze_color_balance(video_frames),
            'psychological_impact': self._analyze_color_psychology(video_frames)
        }
        
        weights = {
            'color_scheme_coherence': 0.25,
            'dominant_colors': 0.2,
            'accent_colors': 0.2,
            'color_balance': 0.2,
            'psychological_impact': 0.15
        }
        
        return sum(score * weights[metric] for metric, score in palette_metrics.items())
    except Exception:
        return 0.0
from typing import List, Dict, Tuple
import numpy as np
import cv2
from collections import defaultdict
from sklearn.cluster import KMeans

def _detect_glass_elements(self, video_frames: List[Dict]) -> float:
    """Detect and analyze glass architectural elements"""
    try:
        glass_features = {
            'windows': self._detect_windows(video_frames),
            'glass_doors': self._detect_glass_doors(video_frames),
            'glass_walls': self._detect_glass_walls(video_frames),
            'transparency': self._analyze_transparency(video_frames),
            'reflectivity': self._analyze_reflectivity(video_frames)
        }
        
        return sum(score for score in glass_features.values()) / len(glass_features)
    except Exception:
        return 0.0

def _detect_signage(self, video_frames: List[Dict]) -> List[Dict]:
    """Detect and extract signage elements"""
    try:
        signage_elements = []
        for frame in video_frames:
            # Detect text
            text_detections = frame.get('TextDetections', [])
            # Detect logos
            label_detections = frame.get('Labels', [])
            
            for text in text_detections:
                if text['Confidence'] > self.confidence_threshold:
                    signage_elements.append({
                        'type': 'text',
                        'content': text['DetectedText'],
                        'location': text['Geometry']['BoundingBox'],
                        'confidence': text['Confidence']
                    })
                    
            for label in label_detections:
                if 'sign' in label['Name'].lower() and label['Confidence'] > self.confidence_threshold:
                    signage_elements.append({
                        'type': 'sign',
                        'content': label['Name'],
                        'location': label.get('Instances', [{}])[0].get('BoundingBox', {}),
                        'confidence': label['Confidence']
                    })
                    
        return signage_elements
    except Exception:
        return []

def _analyze_light_coverage(self, video_frames: List[Dict]) -> float:
    """Analyze coverage of lighting across facade"""
    try:
        coverage_metrics = {
            'spatial_distribution': self._analyze_light_distribution(video_frames),
            'dark_spots': self._identify_dark_spots(video_frames),
            'highlight_areas': self._identify_highlights(video_frames),
            'uniformity': self._calculate_lighting_uniformity(video_frames)
        }
        
        return sum(score for score in coverage_metrics.values()) / len(coverage_metrics)
    except Exception:
        return 0.0

def _detect_tables(self, video_frames: List[Dict]) -> List[Dict]:
    """Detect tables in restaurant space"""
    try:
        tables = []
        for frame in video_frames:
            labels = frame.get('Labels', [])
            for label in labels:
                if label['Name'].lower() in ['table', 'dining table', 'furfurniture']:
                    for instance in label.get('Instances', []):
                        if instance['Confidence'] > self.confidence_threshold:
                            tables.append({
                                'location': instance['BoundingBox'],
                                'confidence': instance['Confidence'],
                                'frame_index': frame.get('FrameIndex', 0)
                            })
        return tables
    except Exception:
        return []

def _calculate_average_luminosity(self, video_frames: List[Dict]) -> float:
    """Calculate average luminosity of space"""
    try:
        luminosity_values = []
        for frame in video_frames:
            # Extract brightness channel
            brightness = self._extract_brightness_channel(frame)
            if brightness is not None:
                luminosity_values.append(np.mean(brightness))
        
        return np.mean(luminosity_values) if luminosity_values else 0.0
    except Exception:
        return 0.0

def _identify_dominant_colors(self, video_frames: List[Dict]) -> List[Dict]:
    """Identify dominant colors in interior space"""
    try:
        color_counts = defaultdict(int)
        total_pixels = 0
        
        for frame in video_frames:
            # Extract colors
            colors = self._extract_color_palette(frame)
            for color, count in colors.items():
                color_counts[color] += count
                total_pixels += count
        
        # Calculate color percentages
        color_percentages = {
            color: (count / total_pixels) * 100
            for color, count in color_counts.items()
        }
        
        # Return top colors
        return sorted(
            [{'color': k, 'percentage': v} for k, v in color_percentages.items()],
            key=lambda x: x['percentage'],
            reverse=True
        )[:5]
    except Exception:
        return []

def _extract_brightness_channel(self, frame: Dict) -> np.ndarray:
    """Extract brightness channel from frame"""
    try:
        # Convert frame to HSV color space
        hsv_frame = cv2.cvtColor(frame['Image'], cv2.COLOR_BGR2HSV)
        # Return value (brightness) channel
        return hsv_frame[:,:,2]
    except Exception:
        return None

def _extract_color_palette(self, frame: Dict) -> Dict[str, int]:
    """Extract color palette from frame"""
    try:
        # Convert frame to RGB color space
        rgb_frame = cv2.cvtColor(frame['Image'], cv2.COLOR_BGR2RGB)
        
        # Reshape image
        pixels = rgb_frame.reshape(-1, 3)
        
        # Use K-means to find dominant colors
        kmeans = KMeans(n_clusters=5, random_state=42)
        kmeans.fit(pixels)
        
        # Count pixels for each color
        colors = {}
        for color in kmeans.cluster_centers_:
            color_hex = '#{:02x}{:02x}{:02x}'.format(
                int(color[0]), int(color[1]), int(color[2])
            )
            colors[color_hex] = np.sum(kmeans.labels_ == len(colors))
            
        return colors
    except Exception:
        return {}

def _calculate_minimum_spacing(self, tables: List[Dict]) -> float:
    """Calculate minimum spacing between tables"""
    try:
        min_spacing = float('inf')
        for i, table1 in enumerate(tables):
            for j, table2 in enumerate(tables[i+1:], i+1):
                spacing = self._calculate_table_distance(table1, table2)
                min_spacing = min(min_spacing, spacing)
        
        # Normalize to 0-100 scale
        return min(100, max(0, (min_spacing / 2.0) * 100))
    except Exception:
        return 0.0

def _calculate_table_distance(self, table1: Dict, table2: Dict) -> float:
    """Calculate distance between two tables"""
    try:
        # Calculate center points
        center1 = self._calculate_center(table1['location'])
        center2 = self._calculate_center(table2['location'])
        
        # Calculate Euclidean distance
        return ((center1[0] - center2[0])**2 + (center1[1] - center2[1])**2)**0.5
    except Exception:
        return float('inf')

def _calculate_center(self, bbox: Dict) -> Tuple[float, float]:
    """Calculate center point of bounding box"""
    return (
        bbox['Left'] + bbox['Width']/2,
        bbox['Top'] + bbox['Height']/2
    )
def _analyze_food_interaction(self, video_frames: List[Dict]) -> Dict[str, Any]:
    """Analyze food interaction characteristics"""
    return {
        'consumption_technique': {
            'bite_mechanics': self._analyze_bite_mechanics(video_frames),
            'utensil_usage': self._analyze_utensil_usage(video_frames)
        },
        'multi_sensory_experience': self._analyze_multi_sensory_experience(video_frames)
    }

def _analyze_bite_mechanics(self, video_frames: List[Dict]) -> Dict[str, Any]:
    """Analyze bite mechanics and techniques"""
    return {
        'first_bite_approach': {
            'bite_size_selection': self._analyze_bite_size_selection(video_frames),
            'bite_angle': self._analyze_bite_angle(video_frames),
            'pre_bite_preparation': self._analyze_pre_bite_preparation(video_frames)
        },
        'bite_execution': {
            'pressure_control': self._analyze_bite_pressure(video_frames),
            'cleanliness_management': self._analyze_cleanliness_management(video_frames),
            'multi_component_handling': self._analyze_component_handling(video_frames)
        }
    }

def _analyze_utensil_usage(self, video_frames: List[Dict]) -> Dict[str, Any]:
    """Analyze utensil usage techniques"""
    return {
        'fork_techniques': self._analyze_fork_usage(video_frames),
        'chopstick_mastery': self._analyze_chopstick_usage(video_frames),
        'spoon_utilization': self._analyze_spoon_usage(video_frames)
    }

def _analyze_multi_sensory_experience(self, video_frames: List[Dict]) -> Dict[str, Any]:
    """Analyze multi-sensory eating experience"""
    return {
        'visual_appreciation': self._analyze_visual_appreciation(video_frames),
        'aroma_experience': self._analyze_aroma_experience(video_frames),
        'texture_engagement': self._analyze_texture_engagement(video_frames),
        'flavor_analysis': self._analyze_flavor_analysis(video_frames)
    }

def _analyze_bite_size_selection(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze bite size selection patterns"""
    try:
        bite_metrics = {
            'small_testing_bite': self._detect_testing_bites(video_frames),
            'confident_full_bite': self._detect_full_bites(video_frames),
            'careful_edge_bite': self._detect_edge_bites(video_frames),
            'center_targeted_bite': self._detect_center_bites(video_frames),
            'structured_layered_bite': self._detect_layered_bites(video_frames)
        }
        
        return bite_metrics
    except Exception:
        return {key: 0.0 for key in ['small_testing_bite', 'confident_full_bite', 
                                    'careful_edge_bite', 'center_targeted_bite', 
                                    'structured_layered_bite']}

def _analyze_bite_angle(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze bite angle approaches"""
    try:
        angle_metrics = {
            'direct_front_approach': self._analyze_front_approach(video_frames),
            'side_angle_bite': self._analyze_side_approach(video_frames),
            '45_degree_approach': self._analyze_45_degree_approach(video_frames),
            'top_down_bite': self._analyze_top_down_approach(video_frames),
            'bottom_up_technique': self._analyze_bottom_up_approach(video_frames)
        }
        
        return angle_metrics
    except Exception:
        return {key: 0.0 for key in ['direct_front_approach', 'side_angle_bite',
                                    '45_degree_approach', 'top_down_bite',
                                    'bottom_up_technique']}

def _analyze_pre_bite_preparation(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze pre-bite preparation techniques"""
    try:
        prep_metrics = {
            'food_item_rotation': self._analyze_food_rotation(video_frames),
            'optimal_angle_finding': self._analyze_angle_adjustment(video_frames),
            'component_adjustment': self._analyze_component_adjustment(video_frames),
            'sauce_distribution_check': self._analyze_sauce_check(video_frames),
            'structural_integrity_assessment': self._analyze_structure_check(video_frames)
        }
        
        return prep_metrics
    except Exception:
        return {key: 0.0 for key in ['food_item_rotation', 'optimal_angle_finding',
                                    'component_adjustment', 'sauce_distribution_check',
                                    'structural_integrity_assessment']}

def _analyze_bite_pressure(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze bite pressure control"""
    try:
        pressure_metrics = {
            'gentle_initial_pressure': self._analyze_initial_pressure(video_frames),
            'firm_decisive_bite': self._analyze_bite_decisiveness(video_frames),
            'progressive_pressure_increase': self._analyze_pressure_progression(video_frames),
            'texture_adapted_force': self._analyze_texture_adaptation(video_frames),
            'bite_depth_control': self._analyze_bite_depth(video_frames)
        }
        
        return pressure_metrics
    except Exception:
        return {key: 0.0 for key in ['gentle_initial_pressure', 'firm_decisive_bite',
                                    'progressive_pressure_increase', 'texture_adapted_force',
                                    'bite_depth_control']}

def _analyze_cleanliness_management(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze cleanliness management during eating"""
    try:
        cleanliness_metrics = {
            'drip_prevention': self._analyze_drip_prevention(video_frames),
            'sauce_control': self._analyze_sauce_control(video_frames),
            'filling_containment': self._analyze_filling_containment(video_frames),
            'neat_separation': self._analyze_bite_separation(video_frames),
            'mess_minimization': self._analyze_mess_management(video_frames)
        }
        
        return cleanliness_metrics
    except Exception:
        return {key: 0.0 for key in ['drip_prevention', 'sauce_control',
                                    'filling_containment', 'neat_separation',
                                    'mess_minimization']}
def _analyze_fork_usage(self, video_frames: List[Dict]) -> Dict[str, Any]:
    """Analyze fork usage techniques"""
    try:
        return {
            'grip_style': {
                'formal_dining_grip': self._analyze_formal_grip(video_frames),
                'casual_hold': self._analyze_casual_grip(video_frames),
                'specialized_technique_grip': self._analyze_specialized_grip(video_frames),
                'adaptable_position': self._analyze_grip_adaptation(video_frames),
                'cultural_specific_method': self._analyze_cultural_grip(video_frames)
            },
            'food_capture': {
                'precision_spearing': self._analyze_spearing_technique(video_frames),
                'scooping_technique': self._analyze_scooping_method(video_frames),
                'twirling_method': self._analyze_twirling_technique(video_frames),
                'gathering_approach': self._analyze_gathering_technique(video_frames),
                'separating_motion': self._analyze_separation_technique(video_frames)
            }
        }
    except Exception:
        return {}

def _analyze_chopstick_usage(self, video_frames: List[Dict]) -> Dict[str, Any]:
    """Analyze chopstick usage techniques"""
    try:
        return {
            'grip_fundamentals': {
                'finger_position': self._analyze_finger_positioning(video_frames),
                'pressure_control': self._analyze_chopstick_pressure(video_frames),
                'stability_maintenance': self._analyze_chopstick_stability(video_frames),
                'flexibility_allowance': self._analyze_grip_flexibility(video_frames),
                'comfort_optimization': self._analyze_grip_comfort(video_frames)
            },
            'picking_techniques': {
                'precision_grasp': self._analyze_precision_picking(video_frames),
                'delicate_item_handling': self._analyze_delicate_handling(video_frames),
                'larger_item_managnagement': self._analyze_large_item_handling(video_frames),
                'slippery_food_control': self._analyze_slippery_food_handling(video_frames),
                'texture_specific_approach': self._analyze_texture_specific_technique(video_frames)
            }
        }
    except Exception:
        return {}

def _analyze_spoon_usage(self, video_frames: List[Dict]) -> Dict[str, Any]:
    """Analyze spoon usage techniques"""
    try:
        return {
            'soup_consumption': {
                'surface_skimming': self._analyze_skimming_technique(video_frames),
                'temperature_testing': self._analyze_temperature_testing(video_frames),
                'ingredient_selection': self._analyze_ingredient_selection(video_frames),
                'broth_balance': self._analyze_broth_balance(video_frames),
                'spill_prevention': self._analyze_spill_prevention(video_frames)
            },
            'dessert_enjoyment': {
                'portion_sizing': self._analyze_portion_control(video_frames),
                'texture_preservation': self._analyze_texture_preservation(video_frames),
                'layered_sampling': self._analyze_layer_sampling(video_frames),
                'sauce_incorporation': self._analyze_sauce_incorporation(video_frames),
                'melting_management': self._analyze_melting_management(video_frames)
            }
        }
    except Exception:
        return {}

def _analyze_visual_appreciation(self, video_frames: List[Dict]) -> Dict[str, Any]:
    """Analyze visual appreciation of food"""
    try:
        return {
            'color_analysis': {
                'color_vibrancy': self._analyze_color_vibrancy(video_frames),
                'contrast_appreciation': self._analyze_color_contrast(video_frames),
                'color_combination': self._analyze_color_combination(video_frames),
                'garnish_impact': self._analyze_garnish_visual_impact(video_frames),
                'temperature_cues': self._analyze_visual_temperature_cues(video_frames)
            },
            'plating_evaluation': {
                'composition': self._analyze_plate_composition(video_frames),
                'negative_space': self._analyze_negative_space_usage(video_frames),
                'height_variation': self._analyze_height_variation(video_frames),
                'symmetry': self._analyze_plating_symmetry(video_frames),
                'portion_visual': self._analyze_portion_visuals(video_frames)
            }
        }
    except Exception:
        return {}

def _analyze_aroma_experience(self, video_frames: List[Dict]) -> Dict[str, Any]:
    """Analyze aroma experience aspects"""
    try:
        return {
            'initial_scent': {
                'first_impression': self._analyze_initial_aroma(video_frames),
                'intensity_level': self._analyze_aroma_intensity(video_frames),
                'complexity': self._analyze_aroma_complexity(video_frames),
                'familiarity': self._analyze_aroma_familiarity(video_frames),
                'emotional_response': self._analyze_aroma_emotion(video_frames)
            },
            'layered_fragrances': {
                'primary_aroma': self._analyze_primary_aroma(video_frames),
                'secondary_notes': self._analyze_secondary_aromas(video_frames),
                'spice_recognition': self._analyze_spice_aromas(video_frames),
                'herb_fragrance': self._analyze_herb_aromas(video_frames),
                'cooking_method': self._analyze_cooking_aromas(video_frames)
            }
        }
    except Exception:
        return {}

def _analyze_texture_engagement(self, video_frames: List[Dict]) -> Dict[str, Any]:
    """Analyze texture engagement aspects"""
    try:
        return {
            'surface_texture': {
                'initial_touch': self._analyze_initial_texture(video_frames),
                'pattern_recognition': self._analyze_texture_patterns(video_frames),
                'temperature_contrast': self._analyze_temperature_contrast(video_frames),
                'moisture_level': self._analyze_moisture_assessment(video_frames),
                'textural_complexity': self._analyze_texture_complexity(video_frames)
            },
            'internal_texture': {
                'bite_resistance': self._analyze_bite_resistance(video_frames),
                'internal_structure': self._analyze_internal_structure(video_frames),
                'moisture_content': self._analyze_internal_moisture(video_frames),
                'texture_transitions': self._analyze_texture_transitions(video_frames),
                'consistency': self._analyze_texture_consistency(video_frames)
            }
        }
    except Exception:
        return {}
def _analyze_flavor_analysis(self, video_frames: List[Dict]) -> Dict[str, Any]:
    """Analyze flavor experience and reactions"""
    try:
        return {
            'initial_taste': {
                'first_flavor_impact': self._analyze_first_flavor_impact(video_frames),
                'immediate_intensity': self._analyze_flavor_intensity(video_frames),
                'flavor_location': self._analyze_flavor_location(video_frames),
                'taste_bud_activation': self._analyze_taste_activation(video_frames),
                'initial_complexity': self._analyze_initial_complexity(video_frames)
            },
            'flavor_development': {
                'taste_evolution': self._analyze_taste_evolution(video_frames),
                'flavor_layering': self._analyze_flavor_layers(video_frames),
                'spice_development': self._analyze_spice_development(video_frames),
                'umami_recognition': self._analyze_umami_presence(video_frames),
                'aftertaste_progression': self._analyze_aftertaste(video_frames)
            },
            'temperature_impact': {
                'hot_food_perception': self._analyze_hot_food_taste(video_frames),
                'cold_element_contrast': selfelf._analyze_temperature_taste_contrast(video_frames),
                'temperature_flavor_relationship': self._analyze_temp_flavor_relation(video_frames),
                'optimal_temperature': self._analyze_optimal_temp_range(video_frames),
                'cooling_effect': self._analyze_cooling_impact(video_frames)
            }
        }
    except Exception:
        return {}

# Helper methods for flavor analysis
def _analyze_first_flavor_impact(self, video_frames: List[Dict]) -> float:
    """Analyze initial flavor impact reaction"""
    try:
        impact_indicators = {
            'facial_response': self._analyze_initial_facial_response(video_frames),
            'immediate_reaction': self._analyze_immediate_reaction(video_frames),
            'expression_change': self._analyze_expression_change(video_frames),
            'body_language': self._analyze_initial_body_language(video_frames),
            'verbal_response': self._analyze_initial_verbal_response(video_frames)
        }
        
        weights = {
            'facial_response': 0.3,
            'immediate_reaction': 0.25,
            'expression_change': 0.2,
            'body_language': 0.15,
            'verbal_response': 0.1
        }
        
        return sum(score * weights[metric] for metric, score in impact_indicators.items())
    except Exception:
        return 0.0

def _analyze_flavor_intensity(self, video_frames: List[Dict]) -> float:
    """Analyze perceived flavor intensity"""
    try:
        intensity_indicators = {
            'reaction_strength': self._analyze_reaction_strength(video_frames),
            'consumption_pace': self._analyze_consumption_pace(video_frames),
            'pause_frequency': self._analyze_pause_frequency(video_frames),
            'secondary_reactions': self._analyze_secondary_reactions(video_frames),
            'recovery_time': self._analyze_recovery_time(video_frames)
        }
        
        weights = {
            'reaction_strength': 0.3,
            'consumption_pace': 0.2,
            'pause_frequency': 0.2,
            'secondary_reactions': 0.15,
            'recovery_time': 0.15
        }
        
        return sum(score * weights[metric] for metric, score in intensity_indicators.items())
    except Exception:
        return 0.0

def _analyze_taste_evolution(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze evolution of taste over time"""
    try:
        evolution_metrics = {
            'initial_phase': self._analyze_initial_taste_phase(video_frames),
            'development_phase': self._analyze_development_phase(video_frames),
            'peak_phase': self._analyze_peak_phase(video_frames),
            'fade_phase': self._analyze_fade_phase(video_frames),
            'finish_characteristics': self._analyze_finish_characteristics(video_frames)
        }
        
        return evolution_metrics
    except Exception:
        return {key: 0.0 for key in ['initial_phase', 'development_phase', 
                                    'peak_phase', 'fade_phase', 
                                    'finish_characteristics']}

def _analyze_spice_development(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze development of spice flavors"""
    try:
        spice_metrics = {
            'initial_heat': self._analyze_initial_heat(video_frames),
            'heat_buildup': self._analyze_heat_buildup(video_frames),
            'spice_complexity': self._analyze_spice_complexity(video_frames),
            'heat_distribution': self._analyze_heat_distribution(video_frames),
            'lingering_effect': self._analyze_lingering_heat(video_frames)
        }
        
        return spice_metrics
    except Exception:
        return {key: 0.0 for key in ['initial_heat', 'heat_buildup',
                                    'spice_complexity', 'heat_distribution',
                                    'lingering_effect']}

def _analyze_temperature_taste_contrast(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze temperature and taste contrast effects"""
    try:
        contrast_metrics = {
            'temperature_range': self._analyze_temp_range(video_frames),
            'flavor_variation': self._analyze_temp_flavor_variation(video_frames),
            'texture_impact': self._analyze_temp_texture_impact(video_frames),
            'taste_intensity': self._analyze_temp_taste_intensity(video_frames),
            'mouthfeel_changes': self._analyze_temp_mouthfeel_changes(video_frames)
        }
        
        return contrast_metrics
    except Exception:
        return {key: 0.0 for key in ['temperature_range', 'flavor_variation',
                                    'texture_impact', 'taste_intensity',
                                    'mouthfeel_changes']}

def _analyze_facial_response_sequence(self, video_frames: List[Dict]) -> List[Dict]:
    """Analyze sequence of facial responses during tasting"""
    try:
        response_sequence = []
        for i, frame in enumerate(video_frames):
            face_analysis = {
                'timestamp': frame.get('Timestamp', i),
                'expressions': self._detect_expressions(frame),
                'micro_expressions': self._detect_micro_expressions(frame),
                'muscle_movements': self._analyze_facial_muscles(frame),
                'eye_reactions': self._analyze_eye_reactions(frame)
            }
            response_sequence.append(face_analysis)
        return response_sequence
    except Exception:
        return []

def _analyze_consumption_behavior(self, video_frames: List[Dict]) -> Dict[str, Any]:
    """Analyze overall consumption behavior patterns"""
    try:
        behavior_metrics = {
            'pace': self._analyze_eating_pace(video_frames),
            'thoroughness': self._analyze_eating_thoroughness(video_frames),
            'attention': self._analyze_food_attention(video_frames),
            'enjoyment': self._analyze_enjoyment_indicators(video_frames),
            'social_interaction': self._analyze_social_eating_behavior(video_frames)
        }
        
        return behavior_metrics
    except Exception:
        return {key: 0.0 for key in ['pace', 'thoroughness', 'attention',
                                    'enjoyment', 'social_interaction']}
# Facial and Expression Analysis Helper Methods
def _detect_expressions(self, frame: Dict) -> Dict[str, float]:
    """Detect facial expressions during food interaction"""
    try:
        face_data = frame.get('FaceDetails', [{}])[0]
        expressions = {
            'enjoyment': self._detect_enjoyment_expression(face_data),
            'surprise': self._detect_surprise_expression(face_data),
            'contemplation': self._detect_contemplation_expression(face_data),
            'discomfort': self._detect_discomfort_expression(face_data),
            'satisfaction': self._detect_satisfaction_expression(face_data)
        }
        
        return expressions
    except Exception:
        return {key: 0.0 for key in ['enjoyment', 'surprise', 'contemplation', 
                                    'discomfort', 'satisfaction']}

def _detect_micro_expressions(self, frame: Dict) -> Dict[str, float]:
    """Detect subtle micro-expressions during tasting"""
    try:
        face_data = frame.get('FaceDetails', [{}])[0]
        micro_expressions = {
            'initial_reaction': self._analyze_initial_micro_reaction(face_data),
            'flavor_processing': self._analyze_processing_expressions(face_data),
            'temperature_response': self._analyze_temperature_reaction(face_data),
            'texture_response': self._analyze_texture_reaction(face_data),
            'aftertaste_reaction': self._analyze_aftertaste_reaction(face_data)
        }
        
        return micro_expressions
    except Exception:
        return {key: 0.0 for key in ['initial_reaction', 'flavor_processing',
                                    'temperature_response', 'texture_response',
                                    'aftertaste_reaction']}

def _analyze_facial_muscles(self, frame: Dict) -> Dict[str, float]:
    """Analyze facial muscle movements during tasting"""
    try:
        face_data = frame.get('FaceDetails', [{}])[0]
        muscle_movements = {
            'jaw_movement': self._analyze_jaw_action(face_data),
            'cheek_movement': self._analyze_cheek_action(face_data),
            'lip_movement': self._analyze_lip_action(face_data),
            'eye_squinting': self._analyze_eye_squinting(face_data),
            'brow_movement': self._analyze_brow_action(face_data)
        }
        
        return muscle_movements
    except Exception:
        return {key: 0.0 for key in ['jaw_movement', 'cheek_movement',
                                    'lip_movement', 'eye_squinting',
                                    'brow_movement']}

# Temperature and Taste Analysis Helper Methods
def _analyze_temp_range(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze temperature range and its effects"""
    try:
        temp_metrics = {
            'initial_temperature': self._detect_initial_temperature(video_frames),
            'temperature_change': self._track_temperature_change(video_frames),
            'optimal_range': self._identify_optimal_temp(video_frames),
            'cooling_pattern': self._analyze_cooling_pattern(video_frames),
            'temperature_consistency': self._analyze_temp_consistency(video_frames)
        }
        
        return temp_metrics
    except Exception:
        return {key: 0.0 for key in ['initial_temperature', 'temperature_change',
                                    'optimal_range', 'cooling_pattern',
                                    'temperature_consistency']}

def _analyze_temp_flavor_variation(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze how temperature affects flavor perception"""
    try:
        variation_metrics = {
            'hot_flavor_profile': self._analyze_hot_flavor(video_frames),
            'warm_flavor_profile': self._analyze_warm_flavor(video_frames),
               'room_temp_flavor': self._analyze_room_temp_flavor(video_frames),
            'cold_flavor_profile': self._analyze_cold_flavor(video_frames),
            'temperature_transitions': self._analyze_flavor_transitions(video_frames)
        }
        
        return variation_metrics
    except Exception:
        return {key: 0.0 for key in ['hot_flavor_profile', 'warm_flavor_profile',
                                    'room_temp_flavor', 'cold_flavor_profile',
                                    'temperature_transitions']}

# Reaction AnaAnalysis Helper Methods
def _analyze_reaction_strength(self, video_frames: List[Dict]) -> float:
    """Analyze strength of reaction to food"""
    try:
        reaction_indicators = {
            'expression_intensity': self._measure_expression_intensity(video_frames),
            'physical_response': self._measure_physical_response(video_frames),
            'verbal_response': self._measure_verbal_response(video_frames),
            'immediate_reaction': self._measure_immediate_reaction(video_frames),
            'sustained_response': self._measure_sustained_response(video_frames)
        }
        
        weights = {
            'expression_intensity': 0.3,
            'physical_response': 0.2,
            'verbal_response': 0.2,
            'immediate_reaction': 0.15,
            'sustained_response': 0.15
        }
        
        return sum(score * weights[metric] for metric, score in reaction_indicators.items())
    except Exception:
        return 0.0

def _analyze_consumption_pace(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze pace of food consumption"""
    try:
        pace_metrics = {
            'bite_frequency': self._calculate_bite_frequency(video_frames),
            'chewing_duration': self._calculate_chewing_duration(video_frames),
            'pause_patterns': self._analyze_pause_patterns(video_frames),
            'consistency': self._analyze_pace_consistency(video_frames),
            'adjustment_patterns': self._analyze_pace_adjustments(video_frames)
        }
        
        return pace_metrics
    except Exception:
        return {key: 0.0 for key in ['bite_frequency', 'chewing_duration',
                                    'pause_patterns', 'consistency',
                                    'adjustment_patterns']}

# Taste Phase Analysis Helper Methods
def _analyze_initial_taste_phase(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze initial taste phase"""
    try:
        initial_metrics = {
            'first_impression': self._analyze_first_impression(video_frames),
            'immediate_notes': self._analyze_immediate_notes(video_frames),
            'initial_complexity': self._analyze_initial_complexity(video_frames),
            'onset_speed': self._analyze_taste_onset(video_frames),
            'initial_balance': self._analyze_initial_balance(video_frames)
        }
        
        return initial_metrics
    except Exception:
        return {key: 0.0 for key in ['first_impression', 'immediate_notes',
                                    'initial_complexity', 'onset_speed',
                                    'initial_balance']}

def _analyze_development_phase(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze taste development phase"""
    try:
        development_metrics = {
            'flavor_evolution': self._analyze_flavor_evolution(video_frames),
            'complexity_building': self._analyze_complexity_building(video_frames),
            'taste_layering': self._analyze_taste_layering(video_frames),
            'flavor_transitions': self._analyze_flavor_transitions(video_frames),
            'texture_development': self._analyze_texture_development(video_frames)
        }
        
        return development_metrics
    except Exception:
        return {key: 0.0 for key in ['flavor_evolution', 'complexity_building',
                                    'taste_layering', 'flavor_transitions',
                                    'texture_development']}

def _analyze_peak_phase(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze peak taste phase"""
    try:
        peak_metrics = {
            'maximum_intensity': self._analyze_max_intensity(video_frames),
            'flavor_complexity': self._analyze_peak_complexity(video_frames),
            'balance_point': self._analyze_flavor_balance(video_frames),
            'peak_duration': self._analyze_peak_duration(video_frames),
            'satisfaction_level': self._analyze_satisfaction_level(video_frames)
        }
        
        return peak_metrics
    except Exception:
        return {key: 0.0 for key in ['maximum_intensity', 'flavor_complexity',
                                    'balance_point', 'peak_duration',
                                    'satisfaction_level']}

def _analyze_fade_phase(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze taste fade phase"""
    try:
        fade_metrics = {
            'fade_onset': self._analyze_fade_beginning(video_frames),
            'fade_progression': self._analyze_fade_progression(video_frames),
            'lingering_notes': self._analyze_lingering_flavors(video_frames),
            'fade_duration': self._analyze_fade_duration(video_frames),
            'finish_quality': self._analyze_finish_quality(video_frames)
        }
        
        return fade_metrics
    except Exception:
        return {key: 0.0 for key in ['fade_onset', 'fade_progression', 
                                    'lingering_notes', 'fadeade_duration', 
                                    'finish_quality']}

def _analyze_finish_characteristics(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze characteristics of taste finish"""
    try:
        finish_metrics = {
            'aftertaste_duration': self._analyze_aftertaste_duration(video_frames),
            'flavor_persistence': self._analyze_flavor_persistence(video_frames),
            'finish_complexity': self._analyze_finish_complexity(video_frames),
            'palate_cleansinging': self._analyze_palate_cleansing(video_frames),
            'residual_effects': self._analyze_residual_effects(video_frames)
        }
        
        return finish_metrics
    except Exception:
        return {key: 0.0 for key in ['aftertaste_duration', 'flavor_persistence',
                                    'finish_complexity', 'palate_cleansing',
                                    'residual_effects']}

def _analyze_pause_frequency(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze frequency and nature of pauses during consumption"""
    try:
        pause_metrics = {
            'pause_count': self._count_pauses(video_frames),
            'pause_duration': self._analyze_pause_duration(video_frames),
            'pause_timing': self._analyze_pause_timing(video_frames),
            'pause_purpose': self._analyze_pause_purpose(video_frames),
            'pause_patterns': self._analyze_pause_patterns(video_frames)
        }
        
        return pause_metrics
    except Exception:
        return {key: 0.0 for key in ['pause_count', 'pause_duration',
                                    'pause_timing', 'pause_purpose',
                                    'pause_patterns']}

def _analyze_secondary_reactions(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze secondary reactions to taste"""
    try:
        reaction_metrics = {
            'delayed_respesponse': self._analyze_delayed_response(video_frames),
            'subsequent_impressions': self._analyze_subsequent_imprmpressions(video_frames),
            'reaction_evolution': self._analyze_reaction_evolution(video_frames),
            'reflection_indicators': self._analyze_reflection_indicators(video_frames),
            'comparative_responses': self._analyze_comparative_responses(video_frames)
        }
        
        return reaction_metrics
    except Exception:
        return {key: 0.0 for key in ['delayed_response', 'subsequent_impressions',
                                    'reaction_evolution', 'reflection_indicators',
                                    'comparative_responses']}

def _analyze_recovery_time(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze recovery time between taste experiences"""
    try:
        recovery_metrics = {
            'palate_reset': self._analyze_palate_reset(video_frames),
            'sensitivity_return': self._analyze_sensitivity_return(video_frames),
            'recovery_duration': self._analyze_recovery_duration(video_frames),
            'readiness_indicators': self._analyze_readiness_indicators(video_frames),
            'recovery_completeness': self._analyze_recovery_completeness(video_frames)
        }
        
        return recovery_metrics
    except Exception:
        return {key: 0.0 for key in ['palate_reset', 'sensitivity_return',
                                    'recovery_duration', 'readiness_indicators',
                                    'recovery_completeness']}

def _analyze_enjoyment_expression(self, face_data: Dict) -> float:
    """Analyze facial expressions indicating enjoyment"""
    try:
        enjoyment_indicators = {
            'smile_presence': self._detect_smile(face_data),
            'eye_expression': self._analyze_eye_pleasure(face_data),
            'relaxed_features': self._analyze_facial_relaxation(face_data),
            'positive_micro_expressions': self._detect_positive_micro_expressions(face_data),
            'engagement_level': self._analyze_engagement(face_data)
        }
        
        weights = {
            'smile_presence': 0.3,
            'eye_expression': 0.2,
            'relaxed_features': 0.2,
            'positive_micro_expressions': 0.15,
            'engagement_level': 0.15
        }
        
        return sum(score * weights[metric] for metric, score in enjoyment_indicators.items())
    except Exception:
        return 0.0

def _analyze_temperature_reaction(self, face_data: Dict) -> float:
    """Analyze facial reactions to food temperature"""
    try:
        temp_reaction_indicators = {
            'initial_surprise': self._detect_temperature_surprise(face_data),
            'comfort_level': self._analyze_temperature_comfort(face_data),
            'adjustment_behavior': self._analyze_temperature_adjustment(face_data),
            'protective_response': self._detect_protective_response(face_data),
            'temperature_appreciation': self._analyze_temperature_appreciation(face_data)
        }
        
        weights = {
            'initial_surprise': 0.25,
            'comfort_level': 0.25,
            'adjustment_behavior': 0.2,
            'protective_response': 0.15,
            'temperature_appreciation': 0.15
        }
        
        return sum(score * weights[metric] for metric, score in temp_reaction_indicators.items())
    except Exception:
        return 0.0

def _analyze_texture_reaction(self, face_data: Dict) -> float:
    """Analyze facial reactions to food texture"""
    try:
        texture_reaction_indicators = {
            'chewing_adjustment': self._analyze_chewing_adaptation(face_data),
            'texture_surprise': self._detect_texture_surprise(face_data),
            'comfort_with_texture': self._analyze_texture_comfort(face_data),
            'mastication_effort': self._analyze_mastication_effort(face_data),
            'texture_appreciation': self._analyze_texture_appreciation(face_data)
        }
        
        weights = {
            'chewing_adjustment': 0.25,
            'texture_surprise': 0.2,
            'comfort_with_texture': 0.2,
            'mastication_effort': 0.2,
            'texture_appreciation': 0.15
        }
        
        return sum(score * weights[metric] for metric, score in texture_reaction_indicators.items())
    except Exception:
        return 0.0
# Flavor Analysis Helper Methods
def _analyze_flavor_persistence(self, video_frames: List[Dict]) -> float:
    """Analyze persistence of flavor after consumption"""
    try:
        persistence_indicators = {
            'duration': self._measure_flavor_duration(video_frames),
            'intensity_retention': self._measure_intensity_retention(video_frames),
            'flavor_evolution': self._track_flavor_changes(video_frames),
            'recurring_notes': self._detect_recurring_flavors(video_frames),
            'persistence_pattern': self._analyze_persistence_pattern(video_frames)
        }
        
        weights = {
            'duration': 0.3,
            'intensity_retention': 0.25,
            'flavor_evolution': 0.2,
            'recurring_no_notes': 0.15,
            'persistence_pattern': 0.1
        }
        
        return sum(score * weights[metric] for metric, score in persistence_indicators.items())
    except Exception:
        return 0.0

def _analyze_finish_complexity(self, video_frames: List[Dict]) -> float:
    """Analyze complexity of flavor finish"""
    try:
        complexity_indicators = {
            'layer_count': self._count_flavor_layers(video_frames),
            'flavor_interactions': self._analyze_flavor_interactions(video_frames),
            'complexity_development': self._track_complexity_development(video_frames),
            'finish_balance': self._analyze_finish_balance(video_frames),
            'complexity_duration': self._measure_complexity_duration(video_frames)
        }
        
        weights = {
            'layer_count': 0.25,
            'flavor_interactions': 0.25,
            'complexity_development': 0.2,
            'finish_balance': 0.15,
            'complexity_duration': 0.15
        }
        
        return sum(score * weights[metric] for metric, score in complexity_indicators.items())
    except Exception:
        return 0.0

def _analyze_palate_cleansing(self, video_frames: List[Dict]) -> float:
    """Analyze palate cleansing process"""
    try:
        cleansing_metrics = {
            'reset_effectiveness': self._measure_palate_reset(video_frames),
            'cleansing_time': self._measure_cleansing_time(video_frames),
            'residual_clearance': self._analyze_residual_clearance(video_frames),
            'sensitivity_recovery': self._measure_sensitivity_recovery(video_frames),
            'cleansing_completeness': self._analyze_cleansing_completeness(video_frames)
        }
        
        weights = {
            'reset_effectiveness': 0.3,
            'cleansing_time': 0.2,
            'residual_clearance': 0.2,
            'sensitivity_recovery': 0.15,
            'cleansing_completeness': 0.15
        }
        
        return sum(score * weights[metric] for metric, score in cleansing_metrics.items())
    except Exception:
        return 0.0

# Pause Analysis Helper Methods
def _count_pauses(self, video_frames: List[Dict]) -> Dict[str, int]:
    """Count and categorize pauses during consumption"""
    try:
        pause_counts = {
            'reflection_pauses': 0,
            'appreciation_pauses': 0,
            'recovery_pauses': 0,
            'palate_cleansing_pauses': 0,
            'social_interaction_pauses': 0
        }
        
        current_pause = None
        for i, frame in enumerate(video_frames):
            if self._is_pause_moment(frame):
                if not current_pause:
                    current_pause = self._categorize_pause(frame)
            elif current_pause:
                pause_counts[current_pause] += 1
                current_pause = None
        
        return pause_counts
    except Exception:
        return {key: 0 for key in ['reflection_pauses', 'appreciation_pauses',
                                  'recovery_pauses', 'palate_cleansing_pauses',
                                  'social_interaction_pauses']}

def _analyze_pause_duration(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze duration of di different types of pauses"""
    try:
        pause_durations = {
            'average_duration': self._calculate_average_pause_duration(video_frames),
            'pause_variation': self._calculate_pause_variation(video_frames),
            'pause_distribution': self._analyze_pause_distribution(video_frames),
            'type_specificfic_duration': self._analyze_pause_types_duration(video_frames),
            'context_impact': self._analyze_context_impact_on_duration(video_frames)
        }
        
        return pause_durations
    except Exception:
        return {key: 0.0 for key in ['average_duration', 'pause_variation',
                                    'pause_distribution', 'type_specific_duration',
                                    'context_impact']}

def _analyze_pause_timing(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze timing of pauses during consumption"""
    try:
        timing_metrics = {
            'interval_consistency': self._analyze_pause_intervals(video_frames),
            'timing_appropriateness': self._analyze_pause_appropriateness(video_frames),
            'sequence_patterns': self._analyze_pause_sequences(video_frames),
            'contextual_timing': self._analyze_contextual_timing(video_frames),
            'rhythm_impact': self._analyze_pause_rhythm_impact(video_frames)
        }
        
        return timing_metrics
    except Exception:
        return {key: 0.0 for key in ['interval_consistency', 'timing_appropriateness',
                                    'sequence_patterns', 'contextual_timing',
                                    'rhythm_impact']}

# Facial Analysis Helper Methods
def _detect_smile(self, face_data: Dict) -> float:
    """Detect and analyze smile characteristics"""
    try:
        smile_characteristics = {
            'smile_intensity': face_data.get('Smile', {}).get('Confidence', 0.0),
            'smile_genuineness': self._analyze_smile_genuineness(face_data),
            'smile_duration': self._analyze_smile_duration(face_data),
            'smile_context': self._analyze_smile_context(face_data),
            'emotional_congruence': self._analyze_emotional_congruence(face_data)
        }
        
        weights = {
            'smile_intensity': 0.3,
            'smile_genuineness': 0.25,
            'smile_duration': 0.2,
            'smile_context': 0.15,
            'emotional_congruence': 0.1
        }
        
        return sum(score * weights[metric] for metric, score in smile_characteristics.items())
    except Exception:
        return 0.0

def _analyze_eye_pleasure(self, face_data: Dict) -> float:
    """Analyze eye expressions indicating pleasure"""
    try:
        eye_indicators = {
            'eye_crinkle': self._detect_eye_crinkle(face_data),
            'pupil_dilation': self._analyze_pupil_dilation(face_data),
            'gaze_focus': self._analyze_gaze_focus(face_data),
            'blink_pattern': self._analyze_blink_pattern(face_data),
            'eye_brightness': self._analyze_eye_brightness(face_data)
        }
        
        weights = {
            'eye_crinkle': 0.3,
            'pupil_dilation': 0.25,
            'gaze_focus': 0.2,
            'blink_pattern': 0.15,
            'eye_brightness': 0.1
        }
        
        return sum(score * weights[metric] for metric, score in eye_indicators.items())
    except Exception:
        return 0.0

def _analyze_facial_relaxation(self, face_data: Dict) -> float:
    """Analyze facial muscle relaxation"""
    try:
        relaxation_indicators = {
            'muscle_tension': self._analyze_muscle_tension(face_data),
            'expression_ease': self._analyze_expression_ease(face_data),
            'forehead_relaxation': self._analyze_forehead_relaxation(face_data),
            'jaw_relaxation': self._analyze_jaw_relaxation(face_data),
            'overall_comfort': self._analyze_facial_comfort(face_data)
        }
        
        weights = {
            'muscle_tension': 0.25,
            'expression_ease': 0.25,
            'forehead_relaxation': 0.2,
            'jaw_relaxation': 0.15,
            'overall_comfort': 0.15
        }
        
        return sum(score * weights[metric] for metric, score in relaxation_indicators.items())
    except Exception:
        return 0.0

def _detect_temperature_surprise(self, face_data: Dict) -> float:
    """Detect surprise reactions to temperature"""
    try:
        surprise_indicators = {
            'initial_reaction': self._analyze_initial_temp_reaction(face_data),
            'reflex_response': self._analyze_temp_reflex(face_data),
            'adjustment_behavior': self._analyze_temp_adjustment(face_data),
            'comfort_indicators': self._analyze_temp_comfort_signs(face_data),
            'recovery_response': self._analyze_temp_recovery(face_data)
        }
        
        weights = {
            'initial_reaction': 0.3,
            'reflex_response': 0.25,
            'adjustment_behavior': 0.2,
            'comfort_indicators': 0.15,
            'recovery_response': 0.1
        }
        
        return sum(score * weights[metric] for metric, score in surprise_indicators.items())
    except Exception:
        return 0.0

# Flavor Analysis Helper Methods
def _analyze_flavor_persistence(self, video_frames: List[Dict]) -> float:
    """Analyze persistence of flavor after consumption"""
    try:
        persistence_indicators = {
            'duration': self._measure_flavor_duration(video_frames),
            'intensity_retention': self._measure_intensity_retention(video_frames),
            'flavor_evolution': self._track_flavor_changes(video_frames),
            'recurring_notes': self._detect_recurring_flavors(video_frames),
            'persistence_pattern': self._analyze_persistence_pattern(video_frames)
        }
        
        weights = {
            'duration': 0.3,
            'intensity_retention': 0.25,
            'flavor_evolution': 0.2,
            'recurring_no_notes': 0.15,
            'persistence_pattern': 0.1
        }
        
        return sum(score * weights[metric] for metric, score in persistence_indicators.items())
    except Exception:
        return 0.0

def _analyze_finish_complexity(self, video_frames: List[Dict]) -> float:
    """Analyze complexity of flavor finish"""
    try:
        complexity_indicators = {
            'layer_count': self._count_flavor_layers(video_frames),
            'flavor_interactions': self._analyze_flavor_interactions(video_frames),
            'complexity_development': self._track_complexity_development(video_frames),
            'finish_balance': self._analyze_finish_balance(video_frames),
            'complexity_duration': self._measure_complexity_duration(video_frames)
        }
        
        weights = {
            'layer_count': 0.25,
            'flavor_interactions': 0.25,
            'complexity_development': 0.2,
            'finish_balance': 0.15,
            'complexity_duration': 0.15
        }
        
        return sum(score * weights[metric] for metric, score in complexity_indicators.items())
    except Exception:
        return 0.0

def _analyze_palate_cleansing(self, video_frames: List[Dict]) -> float:
    """Analyze palate cleansing process"""
    try:
        cleansing_metrics = {
            'reset_effectiveness': self._measure_palate_reset(video_frames),
            'cleansing_time': self._measure_cleansing_time(video_frames),
            'residual_clearance': self._analyze_residual_clearance(video_frames),
            'sensitivity_recovery': self._measure_sensitivity_recovery(video_frames),
            'cleansing_completeness': self._analyze_cleansing_completeness(video_frames)
        }
        
        weights = {
            'reset_effectiveness': 0.3,
            'cleansing_time': 0.2,
            'residual_clearance': 0.2,
            'sensitivity_recovery': 0.15,
            'cleansing_completeness': 0.15
        }
        
        return sum(score * weights[metric] for metric, score in cleansing_metrics.items())
    except Exception:
        return 0.0

# Pause Analysis Helper Methods
def _count_pauses(self, video_frames: List[Dict]) -> Dict[str, int]:
    """Count and categorize pauses during consumption"""
    try:
        pause_counts = {
            'reflection_pauses': 0,
            'appreciation_pauses': 0,
            'recovery_pauses': 0,
            'palate_cleansing_pauses': 0,
            'social_interaction_pauses': 0
        }
        
        current_pause = None
        for i, frame in enumerate(video_frames):
            if self._is_pause_moment(frame):
                if not current_pause:
                    current_pause = self._categorize_pause(frame)
            elif current_pause:
                pause_counts[current_pause] += 1
                current_pause = None
        
        return pause_counts
    except Exception:
        return {key: 0 for key in ['reflection_pauses', 'appreciation_pauses',
                                  'recovery_pauses', 'palate_cleansing_pauses',
                                  'social_interaction_pauses']}

def _analyze_pause_duration(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze duration of di different types of pauses"""
    try:
        pause_durations = {
            'average_duration': self._calculate_average_pause_duration(video_frames),
            'pause_variation': self._calculate_pause_variation(video_frames),
            'pause_distribution': self._analyze_pause_distribution(video_frames),
            'type_specificfic_duration': self._analyze_pause_types_duration(video_frames),
            'context_impact': self._analyze_context_impact_on_duration(video_frames)
        }
        
        return pause_durations
    except Exception:
        return {key: 0.0 for key in ['average_duration', 'pause_variation',
                                    'pause_distribution', 'type_specific_duration',
                                    'context_impact']}

def _analyze_pause_timing(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze timing of pauses during consumption"""
    try:
        timing_metrics = {
            'interval_consistency': self._analyze_pause_intervals(video_frames),
            'timing_appropriateness': self._analyze_pause_appropriateness(video_frames),
            'sequence_patterns': self._analyze_pause_sequences(video_frames),
            'contextual_timing': self._analyze_contextual_timing(video_frames),
            'rhythm_impact': self._analyze_pause_rhythm_impact(video_frames)
        }
        
        return timing_metrics
    except Exception:
        return {key: 0.0 for key in ['interval_consistency', 'timing_appropriateness',
                                    'sequence_patterns', 'contextual_timing',
                                    'rhythm_impact']}

# Facial Analysis Helper Methods
def _detect_smile(self, face_data: Dict) -> float:
    """Detect and analyze smile characteristics"""
    try:
        smile_characteristics = {
            'smile_intensity': face_data.get('Smile', {}).get('Confidence', 0.0),
            'smile_genuineness': self._analyze_smile_genuineness(face_data),
            'smile_duration': self._analyze_smile_duration(face_data),
            'smile_context': self._analyze_smile_context(face_data),
            'emotional_congruence': self._analyze_emotional_congruence(face_data)
        }
        
        weights = {
            'smile_intensity': 0.3,
            'smile_genuineness': 0.25,
            'smile_duration': 0.2,
            'smile_context': 0.15,
            'emotional_congruence': 0.1
        }
        
        return sum(score * weights[metric] for metric, score in smile_characteristics.items())
    except Exception:
        return 0.0

def _analyze_eye_pleasure(self, face_data: Dict) -> float:
    """Analyze eye expressions indicating pleasure"""
    try:
        eye_indicators = {
            'eye_crinkle': self._detect_eye_crinkle(face_data),
            'pupil_dilation': self._analyze_pupil_dilation(face_data),
            'gaze_focus': self._analyze_gaze_focus(face_data),
            'blink_pattern': self._analyze_blink_pattern(face_data),
            'eye_brightness': self._analyze_eye_brightness(face_data)
        }
        
        weights = {
            'eye_crinkle': 0.3,
            'pupil_dilation': 0.25,
            'gaze_focus': 0.2,
            'blink_pattern': 0.15,
            'eye_brightness': 0.1
        }
        
        return sum(score * weights[metric] for metric, score in eye_indicators.items())
    except Exception:
        return 0.0

def _analyze_facial_relaxation(self, face_data: Dict) -> float:
    """Analyze facial muscle relaxation"""
    try:
        relaxation_indicators = {
            'muscle_tension': self._analyze_muscle_tension(face_data),
            'expression_ease': self._analyze_expression_ease(face_data),
            'forehead_relaxation': self._analyze_forehead_relaxation(face_data),
            'jaw_relaxation': self._analyze_jaw_relaxation(face_data),
            'overall_comfort': self._analyze_facial_comfort(face_data)
        }
        
        weights = {
            'muscle_tension': 0.25,
            'expression_ease': 0.25,
            'forehead_relaxation': 0.2,
            'jaw_relaxation': 0.15,
            'overall_comfort': 0.15
        }
        
        return sum(score * weights[metric] for metric, score in relaxation_indicators.items())
    except Exception:
        return 0.0

def _detect_temperature_surprise(self, face_data: Dict) -> float:
    """Detect surprise reactions to temperature"""
    try:
        surprise_indicators = {
            'initial_reaction': self._analyze_initial_temp_reaction(face_data),
            'reflex_response': self._analyze_temp_reflex(face_data),
            'adjustment_behavior': self._analyze_temp_adjustment(face_data),
            'comfort_indicators': self._analyze_temp_comfort_signs(face_data),
            'recovery_response': self._analyze_temp_recovery(face_data)
        }
        
        weights = {
            'initial_reaction': 0.3,
            'reflex_response': 0.25,
            'adjustment_behavior': 0.2,
            'comfort_indicators': 0.15,
            'recovery_response': 0.1
        }
        
        return sum(score * weights[metric] for metric, score in surprise_indicators.items())
    except Exception:
        return 0.0
def _analyze_temperature_comfort(self, face_data: Dict) -> float:
    """Analyze comfort level with food temperature"""
    try:
        comfort_indicators = {
            'facial_ease': self._analyze_temp_facial_ease(face_data),
            'consumption_pace': self._analyze_temp_pace(face_data),
            'handling_confidence': self._analyze_temp_handling(face_data),
            'discomfort_signs': self._detect_temp_discomfort(face_data),
            'adaptation_signs': self._analyze_temp_adaptation(face_data)
        }
        
        weights = {
            'facial_ease': 0.25,
            'consumption_pace': 0.25,
            'handling_confidence': 0.2,
            'discomfort_signs': 0.15,
            'adaptation_signs': 0.15
        }
        
        return sum(score * weights[metric] for metric, score in comfort_indicators.items())
    except Exception:
        return 0.0

def _analyze_temperature_adjustment(self, face_data: Dict) -> float:
    """Analyze adjustment to food temperature"""
    try:
        adjustment_metrics = {
            'behavioral_changes': self._analyze_temp_behavior_changes(face_data),
            'adaptation_speed': self._analyze_temp_adaptation_speed(face_data),
            'coping_strategies': self._analyze_temp_coping(face_data),
            'comfort_progression': self._analyze_comfort_progression(face_data),
            'technique_modification': self._analyze_technique_changes(face_data)
        }
        
        weights = {
            'behavioral_changes': 0.25,
            'adaptation_speed': 0.25,
            'coping_strategies': 0.2,
            'comfort_progression': 0.15,
            'technique_modification': 0.15
        }
        
        return sum(score * weights[metric] for metric, score in adjustment_metrics.items())
    except Exception:
        return 0.0

def _detect_protective_response(self, face_data: Dict) -> float:
    """Detect protective responses to temperature"""
    try:
        protective_indicators = {
            'recoil_response': self._analyze_recoil(face_data),
            'cautious_behavior': self._analyze_caution(face_data),
            'defensive_movements': self._analyze_defensive_moves(face_data),
            'anticipatory_actions': self._analyze_anticipation(face_data),
            'recovery_behavior': self._analyze_protective_recovery(face_data)
        }
        
        weights = {
            'recoil_response': 0.3,
            'cautious_behavior': 0.25,
            'defensive_movements': 0.2,
            'anticipatory_actions': 0.15,
            'recovery_behavior': 0.1
        }
        
        return sum(score * weights[metric] for metric, score in protective_indicators.items())
    except Exception:
        return 0.0

def _analyze_temp_facial_ease(self, face_data: Dict) -> float:
    """Analyze faciacial ease with temperature"""
    try:
        ease_indicators = {
            'muscle_relaxation': self._analyze_muscle_state(face_data),
            'expression_comfort': self._analyze_expression_comfort(face_data),
            'natural_movements': self._analyze_movement_naturalness(face_data),
            'tension_absence': self._analyze_facial_tension(face_data),
            'enjoyment_signs': self._analyze_enjoyment_indicators(face_data)
        }
        
        return sum(score for score in ease_indicators.values()) / len(ease_indicators)
    except Exception:
        return 0.0

def _analyze_temp_behavior_changes(self, face_data: Dict) -> float:
    """Analyze behavioral changes due to temperature"""
    try:
        behavior_metrics = {
            'movement_adaptation': self._analyze_movement_adaptation(face_data),
            'handling_changes': self._analyze_handling_changes(face_data),
            'pace_adjudjustment': self._analyze_pace_adjustment(face_data),
            'technique_variation': self._analyze_technique_variation(face_data),
            'comfort_seeking': self._analyze_comfort_seeking(face_data)
        }
        
        return sum(score for score in behavior_metrics.values()) / len(behavior_metrics)
    except Exception:
        return 0.0

def _analyze_recoil(self, face_data: Dict) -> float:
    """Analyze recoil response to temperature"""
    try:
        recoil_indicators = {
            'head_movement': self._analyze_head_recoil(face_data),
            'facial_retraction': self._analyze_facial_retraction(face_data),
            'eye_response': self._analyze_eye_recoil(face_data),
            'mouth_response': self._analyze_mouth_recoil(face_data),
            'body_response': self._analyze_body_recoil(face_data)
        }
        
        weights = {
            'head_movement': 0.25,
            'facial_retraction': 0.25,
            'eye_response': 0.2,
            'mouth_response': 0.15,
            'body_response': 0.15
        }
        
        return sum(score * weights[metric] for metric, score in recoil_indicators.items())
    except Exception:
        return 0.0

def _analyze_caution(self, face_data: Dict) -> float:
    """Analyze cautious behavior with temperature"""
    try:
        caution_indicators = {
            'approach_hesitation': self._analyze_approach_hesitation(face_data),
            'testing_behavior': self._analyze_testing_behavior(face_data),
            'protective_positioning': self._analyze_protective_positioning(face_data),
            'careful_manipulation': self._analyze_careful_manipulation(face_data),
            'risk_assessment': self._analyze_risk_assessment(face_data)
        }
        
        weights = {
            'approach_hesitation': 0.25,
            'testing_behavior': 0.25,
            'protective_positioning': 0.2,
            'careful_manipulation': 0.15,
            'risk_assessment': 0.15
        }
        
        return sum(score * weights[metric] for metric, score in caution_indicators.items())
    except Exception:
        return 0.0

def _analyze_defensive_moves(self, face_data: Dict) -> float:
    """Analyze defensive movements related to temperature"""
    try:
        defensive_indicators = {
            'withdrawal_motion': self._analyze_withdrawal(face_data),
            'protective_gestures': self._analyze_protective_gestures(face_data),
            'distancing_behavior': self._analyze_distancing(face_data),
            'guarding_actions': self._analyze_guarding(face_data),
            'avoidance_patterns': self._analyze_avoidance(face_data)
        }
        
        weights = {
            'withdrawal_motion': 0.3,
            'protective_gestures': 0.25,
            'distancing_behavior': 0.2,
            'guarding_actions': 0.15,
            'avoidance_patterns': 0.1
        }
        
        return sum(score * weights[metric] for metric, score in defensive_indicators.items())
    except Exception:
        return 0.0

def _analyze_anticipation(self, face_data: Dict) -> float:
    """Analyze anticipatory actions related to temperature"""
    try:
        anticipation_indicators = {
            'preparatory_gestures': self._analyze_preparation(face_data),
            'cautious_approach': self._analyze_approach_method(face_data),
            'predictive_movements': self._analyze_predictive_moves(face_data),
            'readiness_signals': self._analyze_readiness(face_data),
            'calculated_timing': self._analyze_timing(face_data)
        }
        
        weights = {
            'preparatory_gestures': 0.3,
            'cautious_approach': 0.25,
            'predictive_movements': 0.2,
            'readiness_signals': 0.15,
            'calculated_timing': 0.1
        }
        
        return sum(score * weights[metric] for metric, score in anticipation_indicators.items())
    except Exception:
        return 0.0

def _analyze_temp_facial_ease(self, face_data: Dict) -> float:
    """Analyze faciacial ease with temperature"""
    try:
        ease_indicators = {
            'muscle_relaxation': self._analyze_muscle_state(face_data),
            'expression_comfort': self._analyze_expression_comfort(face_data),
            'natural_movements': self._analyze_movement_naturalness(face_data),
            'tension_absence': self._analyze_facial_tension(face_data),
            'enjoyment_signs': self._analyze_enjoyment_indicators(face_data)
        }
        
        return sum(score for score in ease_indicators.values()) / len(ease_indicators)
    except Exception:
        return 0.0

def _analyze_temp_behavior_changes(self, face_data: Dict) -> float:
    """Analyze behavioral changes due to temperature"""
    try:
        behavior_metrics = {
            'movement_adaptation': self._analyze_movement_adaptation(face_data),
            'handling_changes': self._analyze_handling_changes(face_data),
            'pace_adjudjustment': self._analyze_pace_adjustment(face_data),
            'technique_variation': self._analyze_technique_variation(face_data),
            'comfort_seeking': self._analyze_comfort_seeking(face_data)
        }
        
        return sum(score for score in behavior_metrics.values()) / len(behavior_metrics)
    except Exception:
        return 0.0

# Flavor Analysis Helper Methods
def _analyze_flavor_persistence(self, video_frames: List[Dict]) -> float:
    """Analyze persistence of flavor after consumption"""
    try:
        persistence_indicators = {
            'duration': self._measure_flavor_duration(video_frames),
            'intensity_retention': self._measure_intensity_retention(video_frames),
            'flavor_evolution': self._track_flavor_changes(video_frames),
            'recurring_notes': self._detect_recurring_flavors(video_frames),
            'persistence_pattern': self._analyze_persistence_pattern(video_frames)
        }
        
        weights = {
            'duration': 0.3,
            'intensity_retention': 0.25,
            'flavor_evolution': 0.2,
            'recurring_no_notes': 0.15,
            'persistence_pattern': 0.1
        }
        
        return sum(score * weights[metric] for metric, score in persistence_indicators.items())
    except Exception:
        return 0.0

def _analyze_finish_complexity(self, video_frames: List[Dict]) -> float:
    """Analyze complexity of flavor finish"""
    try:
        complexity_indicators = {
            'layer_count': self._count_flavor_layers(video_frames),
            'flavor_interactions': self._analyze_flavor_interactions(video_frames),
            'complexity_development': self._track_complexity_development(video_frames),
            'finish_balance': self._analyze_finish_balance(video_frames),
            'complexity_duration': self._measure_complexity_duration(video_frames)
        }
        
        weights = {
            'layer_count': 0.25,
            'flavor_interactions': 0.25,
            'complexity_development': 0.2,
            'finish_balance': 0.15,
            'complexity_duration': 0.15
        }
        
        return sum(score * weights[metric] for metric, score in complexity_indicators.items())
    except Exception:
        return 0.0

def _analyze_palate_cleansing(self, video_frames: List[Dict]) -> float:
    """Analyze palate cleansing process"""
    try:
        cleansing_metrics = {
            'reset_effectiveness': self._measure_palate_reset(video_frames),
            'cleansing_time': self._measure_cleansing_time(video_frames),
            'residual_clearance': self._analyze_residual_clearance(video_frames),
            'sensitivity_recovery': self._measure_sensitivity_recovery(video_frames),
            'cleansing_completeness': self._analyze_cleansing_completeness(video_frames)
        }
        
        weights = {
            'reset_effectiveness': 0.3,
            'cleansing_time': 0.2,
            'residual_clearance': 0.2,
            'sensitivity_recovery': 0.15,
            'cleansing_completeness': 0.15
        }
        
        return sum(score * weights[metric] for metric, score in cleansing_metrics.items())
    except Exception:
        return 0.0

# Pause Analysis Helper Methods
def _count_pauses(self, video_frames: List[Dict]) -> Dict[str, int]:
    """Count and categorize pauses during consumption"""
    try:
        pause_counts = {
            'reflection_pauses': 0,
            'appreciation_pauses': 0,
            'recovery_pauses': 0,
            'palate_cleansing_pauses': 0,
            'social_interaction_pauses': 0
        }
        
        current_pause = None
        for i, frame in enumerate(video_frames):
            if self._is_pause_moment(frame):
                if not current_pause:
                    current_pause = self._categorize_pause(frame)
            elif current_pause:
                pause_counts[current_pause] += 1
                current_pause = None
        
        return pause_counts
    except Exception:
        return {key: 0 for key in ['reflection_pauses', 'appreciation_pauses',
                                  'recovery_pauses', 'palate_cleansing_pauses',
                                  'social_interaction_pauses']}

def _analyze_pause_duration(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze duration of di different types of pauses"""
    try:
        pause_durations = {
            'average_duration': self._calculate_average_pause_duration(video_frames),
            'pause_variation': self._calculate_pause_variation(video_frames),
            'pause_distribution': self._analyze_pause_distribution(video_frames),
            'type_specificfic_duration': self._analyze_pause_types_duration(video_frames),
            'context_impact': self._analyze_context_impact_on_duration(video_frames)
        }
        
        return pause_durations
    except Exception:
        return {key: 0.0 for key in ['average_duration', 'pause_variation',
                                    'pause_distribution', 'type_specific_duration',
                                    'context_impact']}

def _analyze_pause_timing(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze timing of pauses during consumption"""
    try:
        timing_metrics = {
            'interval_consistency': self._analyze_pause_intervals(video_frames),
            'timing_appropriateness': self._analyze_pause_appropriateness(video_frames),
            'sequence_patterns': self._analyze_pause_sequences(video_frames),
            'contextual_timing': self._analyze_contextual_timing(video_frames),
            'rhythm_impact': self._analyze_pause_rhythm_impact(video_frames)
        }
        
        return timing_metrics
    except Exception:
        return {key: 0.0 for key in ['interval_consistency', 'timing_appropriateness',
                                    'sequence_patterns', 'contextual_timing',
                                    'rhythm_impact']}

# Facial Analysis Helper Methods
def _detect_smile(self, face_data: Dict) -> float:
    """Detect and analyze smile characteristics"""
    try:
        smile_characteristics = {
            'smile_intensity': face_data.get('Smile', {}).get('Confidence', 0.0),
            'smile_genuineness': self._analyze_smile_genuineness(face_data),
            'smile_duration': self._analyze_smile_duration(face_data),
            'smile_context': self._analyze_smile_context(face_data),
            'emotional_congruence': self._analyze_emotional_congruence(face_data)
        }
        
        weights = {
            'smile_intensity': 0.3,
            'smile_genuineness': 0.25,
            'smile_duration': 0.2,
            'smile_context': 0.15,
            'emotional_congruence': 0.1
        }
        
        return sum(score * weights[metric] for metric, score in smile_characteristics.items())
    except Exception:
        return 0.0

def _analyze_eye_pleasure(self, face_data: Dict) -> float:
    """Analyze eye expressions indicating pleasure"""
    try:
        eye_indicators = {
            'eye_crinkle': self._detect_eye_crinkle(face_data),
            'pupil_dilation': self._analyze_pupil_dilation(face_data),
            'gaze_focus': self._analyze_gaze_focus(face_data),
            'blink_pattern': self._analyze_blink_pattern(face_data),
            'eye_brightness': self._analyze_eye_brightness(face_data)
        }
        
        weights = {
            'eye_crinkle': 0.3,
            'pupil_dilation': 0.25,
            'gaze_focus': 0.2,
            'blink_pattern': 0.15,
            'eye_brightness': 0.1
        }
        
        return sum(score * weights[metric] for metric, score in eye_indicators.items())
    except Exception:
        return 0.0

def _analyze_facial_relaxation(self, face_data: Dict) -> float:
    """Analyze facial muscle relaxation"""
    try:
        relaxation_indicators = {
            'muscle_tension': self._analyze_muscle_tension(face_data),
            'expression_ease': self._analyze_expression_ease(face_data),
            'forehead_relaxation': self._analyze_forehead_relaxation(face_data),
            'jaw_relaxation': self._analyze_jaw_relaxation(face_data),
            'overall_comfort': self._analyze_facial_comfort(face_data)
        }
        
        weights = {
            'muscle_tension': 0.25,
            'expression_ease': 0.25,
            'forehead_relaxation': 0.2,
            'jaw_relaxation': 0.15,
            'overall_comfort': 0.15
        }
        
        return sum(score * weights[metric] for metric, score in relaxation_indicators.items())
    except Exception:
        return 0.0

def _detect_temperature_surprise(self, face_data: Dict) -> float:
    """Detect surprise reactions to temperature"""
    try:
        surprise_indicators = {
            'initial_reaction': self._analyze_initial_temp_reaction(face_data),
            'reflex_response': self._analyze_temp_reflex(face_data),
            'adjustment_behavior': self._analyze_temp_adjustment(face_data),
            'comfort_indicators': self._analyze_temp_comfort_signs(face_data),
            'recovery_response': self._analyze_temp_recovery(face_data)
        }
        
        weights = {
            'initial_reaction': 0.3,
            'reflex_response': 0.25,
            'adjustment_behavior': 0.2,
            'comfort_indicators': 0.15,
            'recovery_response': 0.1
        }
        
        return sum(score * weights[metric] for metric, score in surprise_indicators.items())
    except Exception:
        return 0.0

def analyze_documentation_behavior(self, video_frames: List[Dict]) -> Dict[str, Any]:
    """Analyze documentation behavior and techniques"""
    return {
        'photo_technique': {
            'angle_selection': self._analyze_angle_selection(video_frames),
            'lighting_optimization': self._analyze_lighting_optimization(video_frames),
            'timing_consideration': self._analyze_timing_consideration(video_frames)
        },
        'video_documentation': {
            'movement_capture': self._analyze_movement_capture(video_frames),
            'sound_recording': self._analyze_sound_recording(video_frames),
            'narrative_elements': self._analyze_narrative_elements(video_frames)
        }
    }

def _analyze_angle_selection(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze photography angle selection"""
    try:
        angle_metrics = {
            'overhead_composition': self._analyze_overhead_shots(video_frames),
            'side_profile_capture': self._analyze_side_profile_shots(video_frames),
            '45_degree_perspective': self._analyze_45_degree_shots(video_frames),
            'detail_close_up': self._analyze_detail_shots(video_frames),
            'context_inclusion': self._analyze_context_shots(video_frames)
        }
        
        return angle_metrics
    except Exception as e:
        print(f"Error in angle selection analysis: {str(e)}")
        return {key: 0.0 for key in ['overhead_composition', 'side_profile_capture',
                                    '45_degree_perspective', 'detail_close_up',
                                    'context_inclusion']}

def _analyze_lighting_optimization(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze lighting optimization techniques"""
    try:
        lighting_metrics = {
            'natural_light_utilization': self._analyze_natural_light_usage(video_frames),
            'shadow_management': self._analyze_shadow_control(video_frames),
            'reflection_control': self._analyze_reflection_management(video_frames),
            'highlight_preservation': self._analyze_highlight_control(video_frames),
            'color_accuracy_maintenance': self._analyze_color_accuracy(video_frames)
        }
        
        return lighting_metrics
    except Exception as e:
        print(f"Error in lighting optimization analysis: {str(e)}")
        return {key: 0.0 for key in ['natural_light_utilization', 'shadow_management',
                                    'reflection_control', 'highlight_preservation',
                                    'color_accuracy_maintenance']}

def _analyze_timing_consideration(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze timing considerations in documentation"""
    try:
        timing_metrics = {
            'steam_capture': self._analyze_steam_capture(video_frames),
            'action_shot_timing': self._analyze_action_timing(video_frames),
            'melting_moment_documentation': self._analyze_melting_capture(video_frames),
            'pouring_sequence_recording': self._analyze_pouring_sequence(video_frames),
            'reaction_shot_coordination': self._analyze_reaction_timing(video_frames)
        }
        
        return timing_metrics
    except Exception as e:
        print(f"Error in timing consideration analysis: {str(e)}")
        return {key: 0.0 for key in ['steam_capture', 'action_shot_timing',
                                    'melting_moment_documentation', 'pouring_sequence_recording',
                                    'reaction_shot_coordination']}

def _analyze_movement_capture(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze movement capture techniques"""
    try:
        movement_metrics = {
            'smooth_panning': self._analyze_panning_smoothness(video_frames),
            'focus_pulling': self._analyze_focus_pulls(video_frames),
            'reveal_sequence': self._analyze_reveal_techniques(video_frames),
            'action_tracking': self._analyze_action_tracking(video_frames),
            'stability_maintenance': self._analyze_camera_stability(video_frames)
        }
        
        return movement_metrics
    except Exception as e:
        print(f"Error in movement capture analysis: {str(e)}")
        return {key: 0.0 for key in ['smooth_panning', 'focus_pulling',
                                    'reveal_sequence', 'action_tracking',
                                    'stability_maintenance']}

def _analyze_sound_recording(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze sound recording quality and techniques"""
    try:
        sound_metrics = {
            'reaction_audio_capture': self._analyze_reaction_audio(video_frames),
            'ambient_sound_balance': self._analyze_ambient_sound(video_frames),
            'commentary_clarity': self._analyze_commentary_quality(video_frames),
            'background_noise_management': self._analyze_noise_management(video_frames),
            'eating_sound_control': self._analyze_eating_sounds(video_frames)
        }
        
        return sound_metrics
    except Exception as e:
        print(f"Error in sound recording analysis: {str(e)}")
        return {key: 0.0 for key in ['reaction_audio_capture', 'ambient_sound_balance',
                                    'commentary_clarity', 'background_noise_management',
                                    'eating_sound_control']}

def _analyze_narrative_elements(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze narrative elements in documentation"""
    try:
        narrative_metrics = {
            'story_progression': self._analyze_story_flow(video_frames),
            'detail_emphasis': self._analyze_detail_highlighting(video_frames),
            'reaction_inclusion': self._analyze_reaction_integration(video_frames),
            'context_establishment': self._analyze_context_setting(video_frames),
            'conclusion_satisfaction': self._analyze_conclusion_effectiveness(video_frames)
        }
        
        return narrative_metrics
    except Exception as e:
        print(f"Error in narrative elements analysis: {str(e)}")
        return {key: 0.0 for key in ['story_progression', 'detail_emphasis',
                                    'reaction_inclusion', 'context_establishment',
                                    'conclusion_satisfaction']}
# Angle Selection Helper Methods
def _analyze_overhead_shots(self, video_frames: List[Dict]) -> float:
    """Analyze overhead shot composition"""
    try:
        overhead_metrics = {
            'angle_accuracy': self._check_overhead_angle(video_frames),
            'composition_balance': self._analyze_overhead_composition(video_frames),
            'subject_placement': self._analyze_overhead_subject_placement(video_frames),
            'frame_coverage': self._analyze_overhead_coverage(video_frames),
            'perspective_distortion': self._check_perspective_distortion(video_frames)
        }
        
        weights = {
            'angle_accuracy': 0.3,
            'composition_balance': 0.25,
            'subject_placement': 0.2,
            'frame_coverage': 0.15,
            'perspective_distortion': 0.1
        }
        
        return sum(score * weights[metric] for metric, score in overhead_metrics.items())
    except Exception:
        return 0.0

def _analyze_side_profile_shots(self, video_frames: List[Dict]) -> float:
    """Analyze side profile shot composition"""
    try:
        profile_metrics = {
            'angle_correctness': self._check_profile_angle(video_frames),
            'subject_alignment': self._analyze_profile_alignment(video_frames),
            'depth_representation': self._analyze_profile_depth(video_frames),
            'background_separation': self._analyze_profile_background(video_frames),
            'detail_clarity': self._analyze_profile_detail(video_frames)
        }
        
        weights = {
            'angle_correctness': 0.3,
            'subject_alignment': 0.25,
            'depth_representation': 0.2,
            'background_separation': 0.15,
            'detail_clarity': 0.1
        }
        
        return sum(score * weights[metric] for metric, score in profile_metrics.items())
    except Exception:
        return 0.0

def _analyze_45_degree_shots(self, video_frames: List[Dict]) -> float:
    """Analyze 45-degree angle shots"""
    try:
        angle_metrics = {
            'angle_precision': self._check_45_degree_angle(video_frames),
            'composition_quality': self._analyze_45_degree_composition(video_frames),
            'depth_perception': self._analyze_45_degree_depth(video_frames),
            'subject_presentation': self._analyze_45_degree_presentation(video_frames),
            'visual_interest': self._analyze_45_degree_interest(video_frames)
        }
        
        weights = {
            'angle_precision': 0.3,
            'composition_quality': 0.25,
            'depth_perception': 0.2,
            'subject_presentation': 0.15,
            'visual_interest': 0.1
        }
        
        return sum(score * weights[metric] for metric, score in angle_metrics.items())
    except Exception:
        return 0.0

# Lighting Optimization Helper Methods
def _analyze_natural_light_usage(self, video_frames: List[Dict]) -> float:
    """Analyze natural light utilization"""
    try:
        light_metrics = {
            'light_quality': self._assess_natural_light_quality(video_frames),
            'direction_optimization': self._assess_light_direction(video_frames),
            'diffusion_effectiveness': self._assess_light_diffusion(video_frames),
            'color_temperature': self._assess_light_temperature(video_frames),
            'shadow_softness': self._assess_shadow_softness(video_frames)
        }
        
        weights = {
            'light_quality': 0.3,
            'direction_optimization': 0.25,
            'diffusion_effectiveness': 0.2,
            'color_temperature': 0.15,
            'shadow_softness': 0.1
        }
        
        return sum(score * weights[metric] for metric, score in light_metrics.items())
    except Exception:
        return 0.0

def _analyze_shadow_control(self, video_frames: List[Dict]) -> float:
    """Analyze shadow management techniques"""
    try:
        shadow_metrics = {
            'shadow_density': self._analyze_shadow_density(video_frames),
            'edge_definition': self._analyze_shadow_edges(video_frames),
            'detail_preservation': self._analyze_shadow_details(video_frames),
            'balance_control': self._analyze_shadow_balance(video_frames),
            'intentional_use': self._analyze_shadow_purpose(video_frames)
        }
        
        weights = {
            'shadow_density': 0.3,
            'edge_definition': 0.25,
            'detail_preservation': 0.2,
            'balance_control': 0.15,
            'intentional_use': 0.1
        }
        
        return sum(score * weights[metric] for metric, score in shadow_metrics.items())
    except Exception:
        return 0.0

def _analyze_reflection_management(self, video_frames: List[Dict]) -> float:
    """Analyze reflection control techniques"""
    try:
        reflection_metrics = {
            'surface_reflections': self._analyze_surface_reflections(video_frames),
            'glare_control': self._analyze_glare_management(video_frames),
            'highlight_balance': self._analyze_highlight_balance(video_frames),
            'texture_enhancement': self._analyze_texture_reflections(video_frames),
            'intentional_effects': self._analyze_reflection_effects(video_frames)
        }
        
        weights = {
            'surface_reflections': 0.3,
            'glare_control': 0.25,
            'highlight_balance': 0.2,
            'texture_enhancement': 0.15,
            'intentional_effects': 0.1
        }
        
        return sum(score * weights[metric] for metric, score in reflection_metrics.items())
    except Exception:
        return 0.0

def _analyze_highlight_control(self, video_frames: List[Dict]) -> float:
    """Analyze highlight preservation techniques"""
    try:
        highlight_metrics = {
            'highlight_retention': self._analyze_highlight_retention(video_frames),
            'detail_preservation': self._analyze_highlight_detail(video_frames),
            'exposure_balance': self._analyze_highlight_exposure(video_frames),
            'specular_control': self._analyze_specular_highlights(video_frames),
            'dynamic_range': self._analyze_highlight_range(video_frames)
        }
        
        weights = {
            'highlight_retention': 0.3,
            'detail_preservation': 0.25,
            'exposure_balance': 0.2,
            'specular_control': 0.15,
            'dynamic_range': 0.1
        }
        
        return sum(score * weights[metric] for metric, score in highlight_metrics.items())
    except Exception:
        return 0.0

def _analyze_color_accuracy(self, video_frames: List[Dict]) -> float:
    """Analyze color accuracy maintenance"""
    try:
        color_metrics = {
            'white_balance': self._analyze_white_balance(video_frames),
            'color_fidelity': self._analyze_color_fidelity(video_frames),
            'saturation_control': self._analyze_saturation(video_frames),
            'tonal_accuracy': self._analyze_tonal_accuracy(video_frames),
            'consistency': self._analyze_color_consistency(video_frames)
        }
        
        weights = {
            'white_balance': 0.3,
            'color_fidelity': 0.25,
            'saturation_control': 0.2,
            'tonal_accuracy': 0.15,
            'consistency': 0.1
        }
        
        return sum(score * weights[metric] for metric, score in color_metrics.items())
    except Exception:
        return 0.0

# Documentation Behavior Analysis Helper Methods
def analyze_documentation_behavior(self, video_frames: List[Dict]) -> Dict[str, Any]:
    """Analyze documentation behavior and techniques"""
    return {
        'photo_technique': {
            'angle_selection': self._analyze_angle_selection(video_frames),
            'lighting_optimization': self._analyze_lighting_optimization(video_frames),
            'timing_consideration': self._analyze_timing_consideration(video_frames)
        },
        'video_documenmentation': {
            'movement_capture': self._analyze_movement_capture(video_frames),
            'sound_recording': self._analyze_sound_recording(video_frames),
            'narrative_elements': self._analyze_narrative_elements(video_frames)
        }
    }

def _analyalyze_angle_selection(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze photography angle selection"""
    try:
        angle_metrics = {
            'overhead_composition': self._analyze_overhead_shots(video_frames),
            'side_profile_capture': self._analyze_side_profile_shots(video_frames),
            '45_degree_perspective': self._analyze_45_degree_shots(video_frames),
            'detail_close_up': self._analyze_detail_shots(video_frames),
            'context_inclusion': self._analyze_context_shots(video_frames)
        }
        
        return angle_metrics
    except Exception as e:
        print(f"Error in angle selection analysis: {str(e)}")
        return {key: 0.0 for key in ['overhead_composition', 'side_profile_capture',
                                    '45_degree_perspective', 'detail_close_up',
                                    'context_inclusion']}

def _analyze_lighting_optimization(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze lighting optimization techniques"""
    try:
        lighting_metrics = {
            'natural_light_utilization': self._analyze_natural_light_usage(video_frames),
            'shadow_management': self._analyze_shadow_control(video_frames),
            'reflection_control': self._analyze_reflection_management(video_frames),
            'highighlight_preservation': self._analyze_highlight_control(video_frames),
            'color_accuracy_maintenance': self._analyze_color_accuracy(video_frames)
        }
        
        return lighting_metrics
    except Exception as e:
        print(f"Error in lighting optimization on analysis: {str(e)}")
        return {key: 0.0 for key in ['natural_light_utilization', 'shadow_management',
                                    'reflection_control', 'highlight_preservation',
                                    'color_accuracy_maintenance']}

def _analyze_timing_consideration(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze timing considerations in documentation"""
    try:
        timing_metrics = {
            'steam_capture': self._analyze_steam_capture(video_frames),
            'action_shot_timing': self._analyze_action_timing(video_frames),
            'melting_moment_documentation': self._analyze_melting_capture(video_frames),
            'pouring_sequence_recording': self._analyze_pouring_sequence(video_frames),
            'reaction_shot_coordination': self._analyze_reaction_timing(video_frames)
        }
        
        return timing_metrics
    except Exception as e:
        print(f"Error in timing consideration analysis: {str(e)}")
        return {key: 0.0 for key in ['steam_capture', 'action_shot_timing',
                                    'melting_moment_documentation', 'pouring_sequence_recording',
                                    'reaction_shot_coordination']}

def _analyze_movement_capture(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze movement capture techniques"""
    try:
        movement_metrics = {
            'smooth_panning': self._analyze_panning_smoothness(video_frames),
            'focus_pulling': self._analyze_focus_pulls(video_frames),
            'reveal_sequence': self._analyze_reveal_techniques(video_frames),
            'action_tracking': self._analyze_action_tracking(video_frames),
            'stability_maintenance': self._analyze_camera_stability(video_frames)
        }
        
        return movement_metrics
    except Exception as e:
        print(f"Error in movement capture analysis: {str(e)}")
        return {key: 0.0 for key in ['smooth_panning', 'focus_pulling',
                                    'reveal_sequence', 'action_tracking',
                                    'stability_maintenance']}

def _analyze_sound_recording(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze sound recording quality and techniques"""
    try:
        sound_metrics = {
            'reaction_audio_capture': self._analyze_reaction_audio(video_frames),
            'ambient_sound_balance': self._analyze_ambient_sound(video_frames),
            'commentary_clarity': self._analyze_commentary_quality(video_frames),
            'background_noise_management': self._analyze_noise_management(video_frames),
            'eating_sound_control': self._analyze_eating_sounds(video_frames)
        }
        
        return sound_metrics
    except Exception as e:
        print(f"Error in sound recording analysis: {str(e)}")
        return {key: 0.0 for key in ['reaction_audio_capture', 'ambient_sound_balance',
                                    'commentary_clarity', 'background_noise_management',
                                    'eating_sound_control']}

def _analyze_narrative_elements(self, video_frames: List[Dict]) -> Dict[str, float]:
    """Analyze narrative elements in documentation"""
    try:
        narrative_metrics = {
            'story_progression': self._analyze_story_flow(video_frames),
            'detail_emphasis': self._analyze_detail_highlighting(video_frames),
            'reaction_inclusion': self._analyze_reaction_integration(video_frames),
            'context_establishment': self._analyze_context_setting(video_frames),
            'conclusion_satisfaction': self._analyze_conclusion_effectiveness(video_frames)
        }
        
        return narrative_metrics
    except Exception as e:
        print(f"Error in narrative elements analysis: {str(e)}")
        return {key: 0.0 for key in ['story_progression', 'detail_emphasis',
                                    'reaction_inclusion', 'context_establishment',
                                    'conclusion_satisfaction']}
# Timing Consideration Helper Methods
def _analyze_steam_capture(self, video_frames: List[Dict]) -> float:
    """Analyze steam capture techniques"""
    try:
        steam_metrics = {
            'steam_visibility': self._analyze_steam_visibility(video_frames),
            'lighting_effect': self._analyze_steam_lighting(video_frames),
            'timing_precision': self._analyze_steam_timing(video_frames),
            'composition_impact': self._analyze_steam_composition(video_frames),
            'background_contrast': self._analyze_steam_contrast(video_frames)
        }
        
        weights = {
            'steam_visibility': 0.3,
            'lighting_effect': 0.25,
            'timing_precision': 0.2,
            'composition_impact': 0.15,
            'background_contrast': 0.1
        }
        
        return sum(score * weights[metric] for metric, score in steam_metrics.items())
    except Exception:
        return 0.0

def _analyze_action_timing(self, video_frames: List[Dict]) -> float:
    """Analyze timing of action shots"""
    try:
        timing_metrics = {
            'peak_moment_capture': self._analyze_peak_moments(video_frames),
            'anticipation_timing': self._analyze_anticipation_timing(video_frames),
            'follow_through': self._analyze_follow_through(video_frames),
            'sequence_flow': self._analyze_sequence_flow(video_frames),
            'moment_selection': self._analyze_moment_selection(video_frames)
        }
        
        weights = {
            'peak_moment_capture': 0.3,
            'anticipation_timing': 0.25,
            'follow_through': 0.2,
            'sequence_flow': 0.15,
            'moment_selection': 0.1
        }
        
        return sum(score * weights[metric] for metric, score in timing_metrics.items())
    except Exception:
        return 0.0

def _analyze_melting_capture(self, video_frames: List[Dict]) -> float:
    """Analyze melting moment documentation"""
    try:
        melting_metrics = {
            'transition_capture': self._analyze_melting_transition(video_frames),
            'timing_accuracy': self._analyze_melting_timing(video_frames),
            'visual_appeal': self._analyze_melting_visuals(video_frames),
            'sequence_completion': self._analyze_melting_sequence(video_frames),
            'detail_preservation': self._analyze_melting_details(video_frames)
        }
        
        weights = {
            'transition_capture': 0.3,
            'timing_accuracy': 0.25,
            'visual_appeal': 0.2,
            'sequence_completion': 0.15,
            'detail_preservation': 0.1
        }
        
        return sum(score * weights[metric] for metric, score in melting_metrics.items())
    except Exception:
        return 0.0

# Movement Capture Helper Methods
def _analyze_panning_smoothnessess(self, video_frames: List[Dict]) -> float:
    """Analyze smoothness of camera panning"""
    try:
        panning_metrics = {
            'motion_smoothness': self._analyze_motion_smoothness(video_frames),
            'speed_consistency': self._analyze_pan_speed(video_frames),
            'direction_stability': self._analyze_pan_direction(video_frames),
            'subject_tracking': self._analyze_subject_tracking(video_frames),
            'frame_composition': self._analyze_pan_composition(video_frames)
        }
        
        weights = {
            'motion_smoothness': 0.3,
            'speed_consistency': 0.25,
            'direction_stability': 0.2,
            'subject_tracking': 0.15,
            'frame_composition': 0.1
        }
        
        return sum(score * weights[metric] for metric, score in panning_metrics.items())
    except Exception:
        return 0.0

def _analyze_focus_pulls(self, video_frames: List[Dict]) -> float:
    """Analyze focus pulling techniques"""
    try:
        focus_metrics = {
            'focus_transition': self._analyze_focus_transition(video_frames),
            'timing_precision': self._analyze_focus_timing(video_frames),
            'subject_clarity': self._analyze_subject_clarity(video_frames),
            'depth_effect': self._analyze_depth_effect(video_frames),
            'artistic_impact': self._analyze_focus_impact(video_frames)
        }
        
        weights = {
            'focus_transition': 0.3,
            'timing_precision': 0.25,
            'subject_clarity': 0.2,
            'depth_effect': 0.15,
            'artistic_impact': 0.1
        }
        
        return sum(score * weights[metric] for metric, score in focus_metrics.items())
    except Exception:
        return 0.0

# Sound Recording Helper Methods
def _analyze_reaction_audio(self, video_frames: List[Dict]) -> float:
    """Analyze audio capture of reactions"""
    try:
        audio_metrics = {
            'voice_clarity': self._analyze_voice_clarity(video_frames),
            'reaction_timing': self._analyze_audio_timing(video_frames),
            'emotional_capture': self._analyze_emotional_audio(video_frames),
            'background_isolation': self._analyze_audio_isolation(video_frames),
            'volume_balance': self._analyze_audio_balance(video_frames)
        }
        
        weights = {
            'voice_clarity': 0.3,
            'reaction_timing': 0.25,
            'emotional_capture': 0.2,
            'background_isolation': 0.15,
            'volume_balance': 0.1
        }
        
        return sum(score * weights[metric] for metric, score in audio_metrics.items())
    except Exception:
        return 0.0

def _analyze_ambient_sound(self, video_frames: List[Dict]) -> float:
    """Analyze ambient sound balance"""
    try:
        ambient_metrics = {
            'background_level': self._analyze_background_audio(video_frames),
            'atmosphere_enhancement': self._analyze_atmosphere_audio(video_frames),
            'distraction_control': self._analyze_audio_distractions(video_frames),
            'spatial_awareness': self._analyze_audio_space(video_frames),
            'ambiance_contribution': self._analyze_ambiance_audio(video_frames)
        }
        
        weights = {
            'background_level': 0.3,
            'atmosphere_enhancement': 0.25,
            'distraction_control': 0.2,
            'spatial_awareness': 0.15,
            'ambiance_contribution': 0.1
        }
        
        return sum(score * weights[metric] for metric, score in ambient_metrics.items())
    except Exception:
        return 0.0

def _analyze_noise_management(self, video_frames: List[Dict]) -> float:
    """Analyze background noise management"""
    try:
        noise_metrics = {
            'noise_reduction': self._analyze_noise_reduction(video_frames),
            'signal_clarity': self._analyze_signal_clarity(video_frames),
            'interference_control': self._analyze_interference(video_frames),
            'audio_isolation': self._analyze_sound_isolation(video_frames),
            'consistency_maintenance': self._analyze_audio_consistency(video_frames)
        }
        
        weights = {
            'noise_reduction': 0.3,
            'signal_clarity': 0.25,
            'interference_control': 0.2,
            'audio_isolation': 0.15,
            'consistency_maintenance': 0.1
        }
        
        return sum(score * weights[metric] for metric, score in noise_metrics.items())
    except Exception:
        return 0.0
def _analyze_story_flow(self, video_frames: List[Dict]) -> float:
    """Analyze story progression and flow"""
    try:
        flow_metrics = {
            'sequence_logic': self._analyze_sequence_logic(video_frames),
            'pacing_structure': self._analyze_narrative_pacing(video_frames),
            'transition_smoothness': self._analyze_story_transitions(video_frames),
            'continuity_maintenance': self._analyze_story_continuity(video_frames),
            'engagement_arc': self._analyze_engagement_progression(video_frames)
        }
        
        weights = {
            'sequence_logic': 0.3,
            'pacing_structure': 0.25,
            'transition_smoothness': 0.2,
            'continuity_maintenance': 0.15,
            'engagement_arc': 0.1
        }
        
        return sum(score * weights[metric] for metric, score in flow_metrics.items())
    except Exception:
        return 0.0

def _analyze_detail_highlighting(self, video_frames: List[Dict]) -> float:
    """Analyze emphasis on important details"""
    try:
        detail_metrics = {
            'focus_precision': self._analyze_detail_focus(video_frames),
            'emphasis_technique': self._analyze_emphasis_methods(video_frames),
            'visualual_hierarchy': self._analyze_detail_hierarchy(video_frames),
            'context_balance': self._analyze_detail_context(video_frames),
            'attention_guidance': self._analyze_attention_direction(video_frames)
        }
        
        weights = {
            'focus_precision': 0.3,
            'emphasis_technique': 0.25,
            'visualual_hierarchy': 0.2,
            'context_balance': 0.15,
            'attention_guidance': 0.1
        }
        
        return sum(score * weights[metric] for metric, score in detail_metrics.items())
    except Exception:
        return 0.0

def _analyze_reaction_integration(self, video_frames: List[Dict]) -> float:
    """Analyze integration of reactions in narrative"""
    try:
        reaction_metrics = {
            'timing_effectiveness': self._analyze_reaction_timing(video_frames),
            'emotional_impact': self._analyze_emotional_impact(video_frames),
            'authenticity_preservation': self._analyze_reaction_authenticity(video_frames),
            'context_relevance': self._analyze_reaction_context(video_frames),
            'narrative_contribution': self._analyze_reaction_contribution(video_frames)
        }
        
        weights = {
            'timing_effectiveness': 0.3,
            'emotional_impact': 0.25,
            'authenticity_preservation': 0.2,
            'context_relevance': 0.15,
            'narrative_contribution': 0.1
        }
        
        return sum(score * weights[metric] for metric, score in reaction_metrics.items())
    except Exception:
        return 0.0

def _analyze_context_setting(self, video_frames: List[Dict]) -> float:
    """Analyze establishment of context"""
    try:
        context_metrics = {
            'scene_establishment': self._analyze_scene_setting(video_frames),
            'environmental_context': self._analyze_environment_context(video_frames),
            'temporal_context': self._analyze_time_context(video_frames),
            'cultural_context': self._analyze_cultural_context(video_frames),
            'situational_clarity': self._analyze_situation_clarity(video_frames)
        }
        
        weights = {
            'scene_establishment': 0.3,
            'environmental_context': 0.25,
            'temporal_context': 0.2,
            'cultural_context': 0.15,
            'situational_clarity': 0.1
        }
        
        return sum(score * weights[metric] for metric, score in context_metrics.items())
    except Exception:
        return 0.0

def _analyze_conclusion_effectiveness(self, video_frames: List[Dict]) -> float:
    """Analyze effectiveness of narrative conclusion"""
    try:
        conclusion_metrics = {
            'resolution_clarity': self._analyze_resolution_clarity(video_frames),
            'emotional_satisfaction': self._analyze_emotional_closure(video_frames),
            'message_delivery': self._analyze_final_message(video_frames),
            'memorability_impact': self._analyze_memorable_elements(video_frames),
            'narrative_completion': self._analyze_story_completion(video_frames)
        }
        
        weights = {
            'resolution_clarity': 0.3,
            'emotional_satisfaction': 0.25,
            'message_delivery': 0.2,
            'memorability_impact': 0.15,
            'narrative_completion': 0.1
        }
        
        return sum(score * weights[metric] for metric, score in conclusion_metrics.items())
    except Exception:
        return 0.0

def _analyze_sequence_logic(self, video_frames: List[Dict]) -> float:
    """Analyze logical progression of sequence"""
    try:
        logic_metrics = {
            'event_order': self._analyze_event_order(video_frames),
            'cause_effect': self._analyze_cause_effect(video_frames),
            'story_coherence': self._analyze_story_coherence(video_frames),
            'progression_clarity': self._analyze_progression_clarity(video_frames),
            'logical_flow': self._analyze_logical_connections(video_frames)
        }
        
        weights = {
            'event_order': 0.3,
            'cause_effect': 0.25,
            'story_coherence': 0.2,
            'progression_clarity': 0.15,
            'logical_flow': 0.1
        }
        
        return sum(score * weights[metric] for metric, score in logic_metrics.items())
    except Exception:
        return 0.0

# Event and Story Analysis Helper Methods
def _analyze_event_order(self, video_frames: List[Dict]) -> float:
    """Analyze ordering of events in documentation"""
    try:
        order_metrics = {
            'chronological_flow': self._check_chronological_sequence(video_frames),
            'event_transitions': self._analyze_event_transitions(video_frames),
            'sequence_clarity': self._analyze_sequence_clarity(video_frames),
            'temporal_logic': self._analyze_temporal_logic(video_frames),
            'narrative_structure': self._analyze_narrative_structure(video_frames)
        }
        
        weights = {
            'chronological_flow': 0.3,
            'event_transitions': 0.25,
            'sequence_clarity': 0.2,
            'temporal_logic': 0.15,
            'narrative_structure': 0.1
        }
        
        return sum(score * weights[metric] for metric, score in order_metrics.items())
    except Exception:
        return 0.0

def _analyze_cause_effect(self, video_frames: List[Dict]) -> float:
    """Analyze cause and effect relationships"""
    try:
        relationship_metrics = {
            'action_consequence': self._analyze_action_consequences(video_frames),
            'logical_connections': self._analyze_causal_connections(video_frames),
            'event_impact': self._analyze_event_impact(video_frames),
            'relationship_clarity': self._analyze_relationship_clarity(video_frames),
            'narrative_coherence': self._analyze_narrative_coherence(video_frames)
        }
        
        weights = {
            'action_consequence': 0.3,
            'logical_connections': 0.25,
            'event_impact': 0.2,
            'relationship_clarity': 0.15,
            'narrative_coherence': 0.1
        }
        
        return sum(score * weights[metric] for metric, score in relationship_metrics.items())
    except Exception:
        return 0.0

def _analyze_story_coherence(self, video_frames: List[Dict]) -> float:
    """Analyze overall story coherence"""
    try:
        coherence_metrics = {
            'narrative_flow': self._analyze_narrative_flow(video_frames),
            'theme_consistency': self._analyze_theme_consistency(video_frames),
            'message_clarity': self._analyze_message_clarity(video_frames),
            'story_unity': self._analyze_story_unity(video_frames),
            'plot_development': self._analyze_plot_development(video_frames)
        }
           
        weights = {
            'narrative_flow': 0.3,
            'theme_consistency': 0.25,
            'message_clarity': 0.2,
            'story_unity': 0.15,
            'plot_development': 0.1
        }
        
        return sum(score * weights[metric] for metric, score in coherence_metrics.items())
    except Exception:
        return 0.0

def _analyze_progression_clarity(self, video_frames: List[Dict]) -> float:
    """Analyze clarity of story progression"""
    try:
        clarity_metrics = {
            'progression_logic': self._analyze_progression_logic(video_frames),
            'milestone_clarity': self._analyze_milestone_clarity(video_frames),
            'development_flow': self._analyze_development_flow(video_frames),
            'transition_effectiveness': self._analyze_transition_effectiveness(video_frames),
            'story_direction': self._analyze_story_direction(video_frames)
        }
        
        weights = {
            'progression_logic': 0.3,
            'milestone_clarity': 0.25,
            'development_flow': 0.2,
            'transition_effectiveness': 0.15,
            'story_direction': 0.1
        }
        
        return sum(score * weights[metric] for metric, score in clarity_metrics.items())
    except Exception:
        return 0.0

# Resolution and Closure Helper Methods
def _analyze_resolution_clarity(self, video_frames: List[Dict]) -> float:
    """Analyze clarity of story resolution"""
    try:
        resolution_metrics = {
            'conclusion_clarity': self._analyze_conclusion_clarity(video_frames),
            'resolution_completeness': self._analyze_resolution_completeness(video_frames),
            'ending_impact': self._analyze_ending_impact(video_frames),
            'closure_effectiveness': self._analyze_closure_effectiveness(video_frames),
            'resolution_satisfaction': self._analyze_resolution_satisfaction(video_frames)
        }
        
        weights = {
            'conclusion_clarity': 0.3,
            'resolution_completeness': 0.25,
            'ending_impact': 0.2,
            'closure_effectiveness': 0.15,
            'resolution_satisfaction': 0.1
        }
        
        return sum(score * weights[metric] for metric, score in resolution_metrics.items())
    except Exception:
        return 0.0

def _analyze_emotional_closure(self, video_frames: List[Dict]) -> float:
    """Analyze emotional aspects of story closure"""
    try:
        emotional_metrics = {
            'emotional_resolution': self._analyze_emotional_resolution(video_frames),
            'audience_connection': self._analyze_audience_connection(video_frames),
            'emotional_impact': self._analyze_final_emotional_impact(video_frames),
            'satisfaction_level': self._analyze_satisfaction_level(video_frames),
            'emotional_resonance': self._analyze_emotional_resonance(video_frames)
        }
        
        weights = {
            'emotional_resolution': 0.3,
            'audience_connection': 0.25,
            'emotional_impact': 0.2,
            'satisfaction_level': 0.15,
            'emotional_resonance': 0.1
        }
        
        return sum(score * weights[metric] for metric, score in emotional_metrics.items())
    except Exception:
        return 0.0

def _analyze_final_message(self, video_frames: List[Dict]) -> float:
    """Analyze effectiveness of final message delivery"""
    try:
        message_metrics = {
            'message_clarity': self._analyze_message_clarity(video_frames),
            'key_point_emphasis': self._analyze_key_points(video_frames),
            'takeaway_effectiveness': self._analyze_takeaways(video_frames),
            'message_impact': self._analyze_message_impact(video_frames),
            'call_to_action': self._analyze_call_to_action(video_frames)
        }
        
        weights = {
            'message_clarity': 0.3,
            'key_point_emphasis': 0.25,
            'takeaway_effectiveness': 0.2,
            'message_impact': 0.15,
            'call_to_action': 0.1
        }
        
        return sum(score * weights[metric] for metric, score in message_metrics.items())
    except Exception:
        return 0.0

def _analyze_memorable_elements(self, video_frames: List[Dict]) -> float:
    """Analyze memorable aspects of the conclusion"""
    try:
        memorable_metrics = {
            'key_moment_impact': self._analyze_key_moments(video_frames),
            'visual_memorability': self._analyze_visual_memorability(video_frames),
            'emotional_resonance': self._analyze_emotional_memorability(video_frames),
            'uniqueness_factor': self._analyze_uniqueness(video_frames),
            'lasting_impression': self._analyze_lasting_impression(video_frames)
        }
        
        weights = {
            'key_moment_impact': 0.3,
            'visual_memorability': 0.25,
            'emotional_resonance': 0.2,
            'uniqueness_factor': 0.15,
            'lasting_impression': 0.1
        }
        
        return sum(score * weights[metric] for metric, score in memorable_metrics.items())
    except Exception:
        return 0.0
def _analyze_story_completion(self, video_frames: List[Dict]) -> float:
    """Analyze completeness of story"""
    try:
        completion_metrics = {
            'narrative_closure': self._check_narrative_closure(video_frames),
            'plot_resolution': self._check_plot_resolution(video_frames),
            'loose_ends': self._check_loosoose_ends(video_frames),
            'story_arc_completion': self._check_story_arc(video_frames),
            'audience_satisfaction': self._check_satisfaction_indicators(video_frames)
        }
        
        weights = {
            'narrative_closure': 0.3,
            'plot_resolution': 0.25,
            'loose_ends': 0.2,
            'story_arc_completion': 0.15,
            'audience_satisfaction': 0.1
        }
        
        return sum(score * weights[metric] for metric, score in completion_metrics.items())
    except Exception:
        return 0.0

def _check_narrative_closure(self, video_frames: List[Dict]) -> float:
    """Check for proper narrative closure"""
    try:
        closure_indicators = {
            'story_resolution': self._evaluate_resolution(video_frames),
            'theme_conclusion': self._evaluate_theme_closure(video_frames),
            'character_arc_completion': self._evaluate_character_arcs(video_frames),
            'message_delivery': self._evaluate_final_message(video_frames),
            'ending_effectiveness': self._evaluate_ending(video_frames)
        }
        
        return sum(score for score in closure_indicators.values()) / len(closure_indicators)
    except Exception:
        return 0.0

def _check_chronological_sequence(self, video_frames: List[Dict]) -> float:
    """Check chronological sequence of events"""
    try:
        sequence_metrics = {
            'time_flow': self._analyze_time_flow(video_frames),
            'event_order': self._check_event_order(video_frames),
            'sequence_logic': self._check_sequence_logic(video_frames),
            'temporal_consistency': self._check_temporal_consistency(video_frames),
            'timeline_clarity': self._check_timeline_clarity(video_frames)
        }
        
        return sum(score for score in sequence_metrics.values()) / len(sequence_metrics)
    except Exception:
           return 0.0

def _check_satisfaction_indicators(self, video_frames: List[Dict]) -> float:
    """Check for audience satisfaction indicators"""
    try:
        satisfaction_metrics = {
            'emotional_response': self._analyze_emotional_response(video_frames),
            'engagement_level': self._analyze_engagement_level(video_frames),
            'viewer_reaction': self._analyze_viewer_reaction(video_frames),
            'attention_maintenance': self._analyze_attention_maintenance(video_frames),
            'interest_level': self._analyze_interest_level(video_frames)
        }
        
        return sum(score for score in satisfaction_metrics.values()) / len(satisfaction_metrics)
    except Exception:
        return 0.0

def _check_plot_resolution(self, video_frames: List[Dict]) -> float:
    """Check for proper plot resolution"""
    try:
        resolution_indicators = {
            'conflict_resolution': self._analyze_conflict_resolution(video_frames),
            'story_conclusion': self._analyze_story_conclusion(video_frames),
            'plot_threads': self._analyze_plot_threads(video_frames),
            'resolution_quality': self._analyze_resolution_quality(video_frames),
            'narrative_completion': self._analyze_narrative_completion(video_frames)
        }
        
        return sum(score for score in resolution_indicators.values()) / len(resolution_indicators)
    except Exception:
        return 0.0

def _check_loose_ends(self, video_frames: List[Dict]) -> float:
    """Check for unresolved story elements"""
    try:
        loose_end_metrics = {
            'plot_completion': self._check_plot_completion(video_frames),
            'question_resolution': self._check_question_resolution(video_frames),
            'story_consistency': self._check_story_consistency(video_frames),
            'character_resolution': self._check_character_resolution(video_frames),
            'theme_resolution': self._check_theme_resolution(video_frames)
        }
        
        return sum(score for score in loose_end_metrics.values()) / len(loose_end_metrics)
    except Exception:
        return 0.0

def _check_story_arc(self, video_frames: List[Dict]) -> float:
    """Check completion of story arc"""
    try:
        arc_metrics = {
            'setup_completion': self._check_setup_completion(video_frames),
            'conflict_development': self._check_conflict_development(video_frames),
            'climax_effectiveness': self._check_climax_effectiveness(video_frames),
            'resolution_fulfillment': self._check_resolution_fulfillment(video_frames),
            'arc_coherence': self._check_arc_coherence(video_frames)
        }
        
        return sum(score for score in arc_metrics.values()) / len(arc_metrics)
    except Exception:
        return 0.0

def _analyze_time_flow(self, video_frames: List[Dict]) -> float:
    """Analyze flow of time in the narrative"""
    try:
        time_metrics = {
            'temporal_progression': self._check_temporal_progression(video_frames),
            'pacing_consistency': self._check_pacing_consistency(video_frames),
            'time_transitions': self._check_time_transitions(video_frames),
            'sequence_clarity': self._check_sequence_clarity(video_frames),
            'time_management': self._check_time_management(video_frames)
        }
        
        return sum(score for score in time_metrics.values()) / len(time_metrics)
    except Exception:
        return 0.0
class ContentAnalysisIntegrator:
    def __init__(self):
        self.rekognition_client = boto3.client('rekognition')
        self.content_analyzer = ContentAnalyzer()
        self.human_presentation_detector = HumanPresentationDetector()
        self.food_analyzer = FoodContentAnalyzer()
        self.documentation_analyzer = DocumentationBehaviorAnalyzer()

    def integrate_analysis(self, video_path: str, metrics_data: Dict) -> Dict[str, Any]:
        """Integrate Rekognition analysis with detailed features and metrics"""
        try:
            # 1. Get Rekognition analysis
            rekognition_results = self._get_rekognition_analysis(video_path)
            
            # 2. Extract our detailed features
            detailed_features = self._extract_detailed_features(video_path)
            
            # 3. Merge with engagement metrics
            merged_results = self._merge_with_metrics(
                rekognition_results,
                detailed_features,
                metrics_data
            )
            
            # 4. Calculate correlations and insights
            analysis_results = self._analyze_correlations(merged_results)
            
            return analysis_results
            
        except Exception as e:
            logger.error(f"Error in analysis integration: {str(e)}")
            return {}

    def _get_rekognition_analysis(self, video_path: str) -> Dict[str, Any]:
        """Get analysis results from Rekognition"""
        try:
            with open(video_path, 'rb') as video:
                response = self.rekognition_client.start_face_detection(
                    Video={'Bytes': video.read()},
                    FaceAttributes=['ALL']
                )
            
            job_id = response['JobId']
            result = self.rekognition_client.get_face_detection(JobId=job_id)
            
            return {
                'faces': result.get('Faces', []),
                'labels': self._get_labels(video_path),
                'moderation': self._get_moderation_labels(video_path),
                'text': self._get_text_detection(video_path)
            }
        except Exception as e:
            logger.error(f"Error in Rekognition analysis: {str(e)}")
            return {}

    def _extract_detailed_features(self, video_path: str) -> Dict[str, Any]:
        """Extract detailed features using our analyzers"""
        return {
            'human_presentation': self.human_presentation_detector.analyze_face_expression(video_path),
            'food_content': self.food_analyzer.analyze_food_content(video_path),
            'documentation': self.documentation_analyzer.analyze_documentation_behavior(video_path)
        }

    def _merge_with_metrics(self, rekognition_results: Dict, 
                          detailed_features: Dict,
                          metrics_data: Dict) -> Dict[str, Any]:
        """Merge all analysis results with engagement metrics"""
        merged_data = {
            'rekognition_analysis': rekognition_results,
            'detailed_features': detailed_features,
            'engagement_metrics': metrics_data
        }
        
        # Create combined feature vectors
        merged_data['feature_vectors'] = self._create_feature_vectors(
               rekognition_results,
            detailed_features
        )
        
        return merged_data

    def _analyze_ze_correlations(self, merged_results: Dict) -> Dict[str, Any]:
        """Analyze correlations between features and metrics"""
        correlations = {}
        
        for metric in ['likes', 'comments', 'shares', 'saves', 'reach']:
            if metric in merged_results['engagement_metrics']:
                metric_correlations = self._calculate_feature_correlations(
                    merged_results['feature_vectors'],
                    merged_results['engagement_metrics'][metric]
                )
                correlations[metric] = metric_correlations
        
        return {
            'raw_data': merged_results,
            'correlations': correlations,
            'insights': self._generate_insights(correlations)
        }
def _analyze_story_completion(self, video_frames: List[Dict]) -> float:
    """Analyze completeness of story"""
    try:
        completion_metrics = {
            'narrative_closure': self._check_narrative_closure(video_frames),
            'plot_resolution': self._check_plot_resolution(video_frames),
            'loose_ends': self._check_loose_ends(video_frames),
            'story_arc_completion': self._check_story_arc(video_frames),
            'audience_satisfaction': self._check_satisfaction_indicators(video_frames)
        }
        
        weights = {
            'narrative_closure': 0.3,
            'plot_resolution': 0.25,
            'loose_ends': 0.2,
            'story_arc_completion': 0.15,
            'audience_satisfaction': 0.1
        }
        
        return sum(score * weights[metric] for metric, score in completion_metrics.items())
    except Exception:
        return 0.0

def _check_narrative_closure(self, video_frames: List[Dict]) -> float:
    """Check for proper narrative closure"""
    try:
        closure_indicators = {
            'story_resolution': self._evaluate_resolution(video_frames),
            'theme_conclusion': self._evaluate_theme_closure(video_frames),
            'character_arc_completion': self._evaluate_character_arcs(video_frames),
            'message_delivery': self._evaluate_final_message(video_frames),
            'ending_effectiveness': self._evaluate_ending(video_frames)
        }
        
        return sum(score for score in closure_indicators.values()) / len(closure_indicators)
    except Exception:
        return 0.0

def _check_chronological_sequence(self, video_frames: List[Dict]) -> float:
    """Check chronological sequence of events"""
    try:
        sequence_metrics = {
            'time_flow': self._analyze_time_flow(video_frames),
            'event_order': self._check_event_order(video_frames),
            'sequence_logic': self._check_sequence_logic(video_frames),
            'temporal_consistency': self._check_temporal_consistency(video_frames),
            'timeline_clarity': self._check_timeline_clarity(video_frames)
        }
        
        return sum(score for score in sequence_metrics.values()) / len(sequence_metrics)
    except Exception:
        return 0.0

def _check_satisfaction_indicators(self, video_frames: List[Dict]) -> float:
    """Check for audience satisfaction indicators"""
    try:
        satisfaction_metrics = {
            'emotional_response': self._analyze_emotional_response(video_frames),
            'engagement_level': self._analyze_engagement_level(video_frames),
            'viewer_reaction': self._analyze_viewer_reaction(video_frames),
            'attention_maintenance': self._analyze_attention_maintenance(video_frames),
            'interest_level': self._analyze_interest_level(video_frames)
        }
        
        return sum(score for score in satisfaction_metrics.values()) / len(satisfaction_metrics)
    except Exception:
        return 0.0

def _check_plot_resolution(self, video_frames: List[Dict]) -> float:
    """Check for proper plot resolution"""
    try:
        resolution_indicators = {
            'conflict_resolution': self._analyze_conflict_resolution(video_frames),
            'story_conclusion': self._analyze_story_conclusion(video_frames),
            'plot_threads': self._analyze_plot_threads(video_frames),
            'resolution_quality': self._analyze_resolution_quality(video_frames),
            'narrative_completion': self._analyze_narrative_completion(video_frames)
        }
        
        return sum(score for score in resolution_indicators.values()) / len(resolution_indicators)
    except Exception:
        return 0.0

def _check_loose_ends(self, video_frames: List[Dict]) -> float:
    """Check for unresolved story elements"""
    try:
        loose_end_metrics = {
            'plot_completion': self._check_plot_completion(video_frames),
            'question_resolution': self._check_question_resolution(video_frames),
            'story_consistency': self._check_story_consistency(video_frames),
            'character_resolution': self._check_character_resolution(video_frames),
            'theme_resolution': self._check_theme_resolution(video_frames)
        }
        
        return sum(score for score in loose_end_metrics.values()) / len(loose_end_metrics)
    except Exception:
        return 0.0

def _check_story_arc(self, video_frames: List[Dict]) -> float:
    """Check completion of story arc"""
    try:
        arc_metrics = {
            'setup_completion': self._check_setup_completion(video_frames),
            'conflict_development': self._check_conflict_development(video_frames),
            'climax_effectiveness': self._check_climax_effectiveness(video_frames),
            'resolution_fulfillment': self._check_resolution_fulfillment(video_frames),
            'arc_coherence': self._check_arc_coherence(video_frames)
        }
        
        return sum(score for score in arc_metrics.values()) / len(arc_metrics)
    except Exception:
        return 0.0

def _analyze_time_flow(self, video_frames: List[Dict]) -> float:
    """Analyze flow of time in the narrative"""
    try:
        time_metrics = {
            'temporal_progression': self._check_temporal_progression(video_frames),
            'pacing_consistency': self._check_pacing_consistency(video_frames),
            'time_transitions': self._check_time_transitions(video_frames),
            'sequence_clarity': self._check_sequence_clarity(video_frames),
            'time_management': self._check_time_management(video_frames)
        }
        
        return sum(score for score in time_metrics.values()) / len(time_metrics)
    except Exception:
        return 0.0
